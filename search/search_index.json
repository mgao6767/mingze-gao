{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":true},"docs":[{"location":"","text":"<p></p>  <ul> <li>I\u2019m Mingze Gao, aka Adrian, Ph.D. in Finance.</li> <li>I now work as a Postdoctoral Research Fellow at the University of Sydney on bank monitoring and risk disclosure with A/Prof. Buhui Qiu and A/Prof. Eliza Wu in an Australian Research Council (ARC) Discovery Project (DP).</li> <li>I will be on the job market in mid 2023.</li> <li>My CV, Google Scholar and SSRN Profile.   </li> </ul>  <p>Contact</p> <ul> <li>Email: mingze.gao@sydney.edu.au</li> <li>Room 516The Codrington Building (H69)   The University of Sydney NSW 2006</li> </ul>","title":"A random walk away finance"},{"location":"apps/","text":"<p>The apps, programs and other tools I developed.</p>","title":"Apps"},{"location":"apps/#research","text":"<ul> <li>frds - a Python framework to compute a collection of major academic measures used in the finance literature.</li> <li>specurve - a Stata command used to perform Specification Curve Analysis and generate the Specification Curve plot.</li> <li>mktstructure - a command-line tool to download Refinitiv Tick History data and compute some market microstructure measures.</li> </ul>","title":"Research"},{"location":"apps/#other","text":"<ul> <li>LeGao - a web application used to make LEGO mosaics.</li> <li>Option Pricing Explained - a web application to provide an interactive pricing process for European options.</li> </ul>","title":"Other"},{"location":"cv/","text":"","title":"CV"},{"location":"cv/#curriculum-vitae","text":"","title":"Curriculum Vitae"},{"location":"cv/#education","text":"<p>PhD, Finance, University of Sydney (2017 - 2021)</p> <ul> <li>Thesis: \"Three Essays in Corporate Finance and Banking\"</li> <li>Supervisors: Dr. Buhui Qiu and Dr. Henry Leung</li> </ul> <p>Master of IT, University of New South Wales (2022 - present)</p> <p>Bachelor of Commerce (Hon), Finance, University of Sydney (2016)</p> <ul> <li>Thesis: \"Liquidity, Adverse Selection, and Information Asymmetry around Corporate Earnings Announcements\"</li> <li>Supervisor: Dr. Joakim Westerholm</li> </ul> <p>Bachelor of Commerce, Econometrics &amp; Finance, University of Sydney (2013 - 2015)</p>","title":"\ud83c\udf93 Education"},{"location":"cv/#publications","text":"","title":"\ud83d\udcc4 Publications"},{"location":"cv/#journal-article","text":"<ul> <li>Gao, M., Leung, H. and Qiu, B. (2021). Organization Capital and Executive Performance Incentives, Journal of Banking &amp; Finance, 123, 106017.</li> </ul>","title":"Journal Article"},{"location":"cv/#conference-paper","text":"<ul> <li>Organization Capital and Executive Performance Incentives, with Henry Leung and Buhui Qiu.<ul> <li>The 2020 FMA Annual Meeting (Virtual), October 2020.</li> <li>The 3rd Global PhD Colloquium, April 2019, Fordham University, New York -- Outstanding PhD Student Paper Award</li> <li>The 32nd Annual PhD Conference in Economics and Business, Australian National University, October 2019, Canberra, Australia.</li> </ul> </li> </ul>","title":"Conference Paper"},{"location":"cv/#grants-awards","text":"<ul> <li>The Paulette Isabel Jones PhD Completion Scholarship, University of Sydney, 2020.</li> <li>AFA Ph.D. Student Travel Grant Award for the 2020 Annual Meeting in San Diego, California from January 3-5, 2020.</li> <li>Outstanding PhD Student Paper Award at the 3rd Global PhD Colloquium in Fordham University, New York, April 2019.</li> <li>The University of Sydney Honours Scholarship, University of Sydney, 2016.</li> </ul>","title":"\ud83c\udfc6 Grants &amp; Awards"},{"location":"cv/#academic-experience","text":"<ul> <li>Postdoctoral Research Fellow, University of Sydney, 2021 - present.</li> <li>Visiting Scholar:<ul> <li>University of Hong Kong, 2019 (cancelled).</li> </ul> </li> <li>Member:<ul> <li>The Business Financing and Banking Research Group at the University of Sydney.</li> </ul> </li> <li>Discussant:<ul> <li>The 2020 FMA Annual Meeting (Virtual), October 2020.</li> </ul> </li> </ul>","title":"\ud83c\udfdb Academic Experience"},{"location":"cv/#teaching-experience","text":"Time Course Institution     Semester 1, 2022 FINC5090 Finance in the Global Economy FINC6021 Corporate Valuation University of Sydney   Semester 2, 2021 FINC2012 Corporate Finance II University of Sydney   Semester 1, 2021 FINC2011 Corporate Finance I FINC6013 International Business Finance University of Sydney   Semester 2, 2020 FINC6001 Intermediate Corporate Finance FINC6010 Derivative Securities University of Sydney   Semester 1, 2020 FINC2012 Corporate Finance II University of Sydney   Semester 2, 2019 FINC2012 Corporate Finance II FINC6010 Derivative Securities University of Sydney   Semester 1, 2019 FINC2011 Corporate Finance I FINC2012 Corporate Finance II University of Sydney   Semester 2, 2018 FINC2011 Corporate Finance I FINC2012 Corporate Finance II University of Sydney   Semester 1, 2018 FINC2012 Corporate Finance II University of Sydney   Semester 2, 2017 FINC2012 Corporate Finance II FINC3013 Mergers &amp; Acquisitions University of Sydney   Semester 1, 2017 FINC2012 Corporate Finance II FINC3011 International Financial Management University of Sydney   Semester 2, 2016 FINC2012 Corporate Finance II University of Sydney","title":"\ud83d\udc68\ud83c\udffb\u200d\ud83c\udfeb Teaching Experience"},{"location":"cv/#professional-experience","text":"<p>Quantitative Consultant - Infinitas Asset Management Ltd (2017 - 2020)</p> <p>Infinitas Asset Management (ABN 78 129 953 724 / AFSL 326087) is a boutique financial adviser and investment manager with a wide client base, including high and ultra-high net worth individuals, family offices, SMSFs, foundations and not-for-profit groups.</p> <p>Co-Founder, Director - Transcendental Capital Pty Ltd (2018 - Present)</p> <p>Transcendental Capital (ABN 28 624 272 199) is a proprietary investment company established in 2018 by three finance PhD students from the University of Sydney Business School. The company leverages frontier academic insights in trading and investments across different assets and markets to achieve superior long-term risk-adjusted performance.</p>","title":"\ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbc Professional Experience"},{"location":"timetable/","text":"<p>Below is the schedule of my tutorials, workshops and consultations for the current and past semesters since 2018.</p>","title":"Timetable for Teaching and Consultation"},{"location":"timetable/#2022-semester-1","text":"<p>All tutorials are temporarily delivered online due to COVID-19 until further notice. Please check Canvas for the Zoom link for each tutorial.</p>","title":"2022 Semester 1"},{"location":"timetable/#finc5090-finance-in-the-global-economy","text":"Type Time Schedule     Tutorial Thursday 1pm Week 2 to 13   Tutorial Thursday 2pm Week 2 to 13   Tutorial Thursday 3pm Week 2 to 13","title":"FINC5090 Finance in the Global Economy"},{"location":"timetable/#finc6021-corporate-valuation","text":"Type Time Schedule     Tutorial Tuesday 1pm Week 2 to 13   Tutorial Tuesday 2pm Week 2 to 13","title":"FINC6021 Corporate Valuation"},{"location":"timetable/#2021-semester-2","text":"<p>All tutorials are temporarily delivered online due to COVID-19 until further notice. Please check Canvas for the Zoom link for each tutorial.</p>","title":"2021 Semester 2"},{"location":"timetable/#finc2012-corporate-finance-ii","text":"Type Time Schedule     Tutorial Monday 10am Week 2 to 13   Tutorial Monday 11am Week 2 to 13   Tutorial Monday 12pm Week 2 to 13   Tutorial Monday 4pm Week 2 to 13","title":"FINC2012 Corporate Finance II"},{"location":"timetable/#2021-semester-1","text":"<p>All tutorials are delivered online due to COVID-19. Please check Canvas for the Zoom link for each tutorial.</p>","title":"2021 Semester 1"},{"location":"timetable/#finc2011-corporate-finance-i","text":"Type Time Schedule     Tutorial Monday 9am Week 2 to 13   Tutorial Monday 11am Week 2 to 13   Tutorial Monday 5pm Week 2 to 13   Tutorial Monday 6pm Week 2 to 13","title":"FINC2011 Corporate Finance I"},{"location":"timetable/#finc6013-international-business-finance","text":"Type Time Schedule     Tutorial Thursday 9am Week 2 to 13   Tutorial Thursday 10am Week 2 to 13   Tutorial Thursday 1pm Week 2 to 13   Tutorial Thursday 2pm Week 2 to 13","title":"FINC6013 International Business Finance"},{"location":"timetable/#2020-semester-2","text":"<p>All tutorials are delivered online due to COVID-19. Please check Canvas for the Zoom link for each tutorial.</p>","title":"2020 Semester 2"},{"location":"timetable/#finc6001-intermediate-corporate-finance","text":"Type Time Schedule     Tutorial Wednesday 5pm Week 1 to 12   Tutorial Wednesday 6pm Week 1 to 12   Tutorial Wednesday 7pm Week 1 to 12   Tutorial Thursday 7pm Week 1 to 12","title":"FINC6001 Intermediate Corporate Finance"},{"location":"timetable/#finc6010-derivatives-securities","text":"Type Time Schedule     Tutorial Monday 9am Week 1 to 12   Tutorial Monday 7pm Week 1 to 12   Tutorial Thursday 11am Week 1 to 12   Tutorial Thursday 2pm Week 1 to 12","title":"FINC6010 Derivatives Securities"},{"location":"timetable/#2020-semester-1","text":"","title":"2020 Semester 1"},{"location":"timetable/#finc2012-corporate-finance-ii_1","text":"<p>All changed to Zoom online sessions due to COVID-19.</p>   Tutorial Wednesday 10:00 Eastern Avenue Seminar Room 312 Week 2 to 13   Tutorial Wednesday 11:00 Institute Tutorial Room 386 Week 2 to 13   Tutorial Wednesday 12:00 Storie Dixson Room N334 Week 2 to 3   Tutorial Wednesday 12:00 Eastern Avenue Seminar Room 121 Week 4 to 13   Tutorial Wednesday 13:00 Storie Dixson Room N333 Week 2 to 13   Tutorial Thursday 13:00 Storie Dixson Room N334 Week 2 to 13","title":"FINC2012 Corporate Finance II"},{"location":"timetable/#2019-semester-2","text":"","title":"2019 Semester 2"},{"location":"timetable/#finc6010-derivative-securities","text":"Workshop Friday 9:00 PNR Lecture Theatre 2   Workshop Friday 10:00 PNR Lecture Theatre 2","title":"FINC6010 Derivative Securities"},{"location":"timetable/#finc2012-corporate-finance-ii_2","text":"Tutorial Monday 14:00 ABS Seminar Room 2250   Tutorial Monday 15:00 ABS Seminar Room 2250   Tutorial Wednesday 12:00 ABS Seminar Room 2070   Tutorial Wednesday 13:00 ABS Collaborative Learning Studio 3190   Tutorial Wednesday 14:00 Carslaw Learning Studio 353","title":"FINC2012 Corporate Finance II"},{"location":"timetable/#2019-semester-1","text":"","title":"2019 Semester 1"},{"location":"timetable/#finc2011-corporate-finance-i_1","text":"Tutorial Monday 9:00 Storie Dixson Room N333 Week 2 to 7, 9 to 13   Tutorial Monday 10:00 Storie Dixson Room N333 Week 2 to 7, 9 to 13   Tutorial Monday 11:00 Storie Dixson Room N333 Week 2 to 7, 9 to 13","title":"FINC2011 Corporate Finance I"},{"location":"timetable/#finc2012-corporate-finance-ii_3","text":"Tutorial Tuesday 13:00 ABS Seminar Room 3280 Week 4 to 7, 9 to 13   Tutorial Tuesday 14:00 ABS Seminar Room 3280 Week 2 to 7, 9 to 13   Tutorial Tuesday 15:00 ABS Seminar Room 3280 Week 3 to 7, 9 to 13   Tutorial Wednesday 13:00 Mackie Seminar Room 107 Week 3 to 7, 9 to 13   Tutorial Wednesday 14:00 Edward Ford Tutorial Room 136 Week 3 to 7, 9 to 13   Tutorial Wednesday 15:00 ABS Case Study Room 1060 Week 4 to 7, 9 to 13   Tutorial Thursday 13:00 Storie Disxon Room N334v Week 2   Tutorial Thursday 14:00 Madsen Tutorial Room 318 Week 2   Tutorial Thursday 15:00 Madsen Tutorial Room 315 Week 2","title":"FINC2012 Corporate Finance II"},{"location":"timetable/#2018-semester-2","text":"","title":"2018 Semester 2"},{"location":"timetable/#finc2011-corporate-finance-i_2","text":"Tutorial Monday 9:00 ABS Seminar Room 3240   Tutorial Monday 10:00 ABS Seminar Room 3240   Tutorial Monday 11:00 ABS Seminar Room 3240   Tutorial Monday 12:00 ABS Seminar Room 3240   Consultation TBA","title":"FINC2011 Corporate Finance I"},{"location":"timetable/#finc2012-corporate-finance-ii_4","text":"Tutorial Tuesday 16:00 Carslaw Tutorial Room 274   Tutorial Tuesday 17:00 Carslaw Tutorial Room 274   Tutorial Wednesday 12:00 Merewether Seminar Room 1 (Rm 154)   Tutorial Wednesday 13:00 Merewether Seminar Room 1 (Rm 154)   Tutorial Wednesday 14:00 Merewether Seminar Room 1 (Rm 154)   Tutorial (new) Wednesday 15:00 Merewether Seminar Room 1 (Rm 154)   Tutorial (new) Wednesday 16:00 Merewether Seminar Room 1 (Rm 154)   Tutorial (new) Wednesday 17:00 Merewether Seminar Room 3 (Rm 156)   Consultation Thursday 11:00-12:00 Economics &amp; Business Building H69 Rm 215","title":"FINC2012 Corporate Finance II"},{"location":"timetable/#2018-semester-1","text":"","title":"2018 Semester 1"},{"location":"timetable/#finc2012-corporate-finance-ii_5","text":"Tutorial Monday 10:00 ABS Seminar Room 2010   Tutorial Monday 11:00 ABS Seminar Room 2010   Tutorial Monday 12:00 ABS Seminar Room 2010   Tutorial Tuesday 10:00 ABS Seminar Room 3280   Tutorial Tuesday 11:00 ABS Seminar Room 3280   Tutorial Thursday 14:00 Merewether Seminar Room 398   Consultation Thursday 15:00 Economics &amp; Business Building (H69) Room 215   <p>Happy studying!</p>","title":"FINC2012 Corporate Finance II"},{"location":"data/","text":"<p>Note</p> <p>The old Research Data Services has been deprecated and will be  merged into frds in a future release.</p>  <p>frds is a Python framework I authored that aims to provide the simplest way to compute a collection of major academic measures used in the finance literature, one-click with a Graphical User Interface (GUI). It features:</p> <ul> <li>consolidation of major data sources such as WRDS, Federal Reserve Board,   Refinitiv, Thomson Reuters, Bloomberg and more.</li> <li>one-liner to compute major measures used in literature.</li> </ul> <p></p>","title":"FRDS - Financial Research Data Services"},{"location":"data/#installation-configuration","text":"<p>frds requires Python3.8 or higher. To install using <code>pip</code>:</p> <p><pre><code>$ pip install frds\n</code></pre> After installation, a folder <code>frds</code> will be created under your user's home directory, which contains a <code>data</code> folder, a <code>result</code> folder and a default configuration file <code>config.ini</code>:</p> <pre><code>[Paths]\nbase_dir: ~/frds\ndata_dir: ${base_dir}/data\nresult_dir: ${base_dir}/result\n\n[Login]\nwrds_username: \nwrds_password: \n</code></pre> <p>You need to enter your WRDS username and password under the login section.</p>","title":"Installation &amp; Configuration"},{"location":"data/#usage","text":"<p>To start estimating various measures, run <code>frds</code> as a module:</p> <pre><code>$ python -m frds.gui.run\n</code></pre> <p>Alternatively, run without GUI:</p> <pre><code>$ python -m frds.run\n</code></pre> <p>The output data will be saved as STATA <code>.dta</code> file in the <code>result</code> folder.</p>","title":"Usage"},{"location":"data/#example-output","text":"<p>Below is an example output for <code>tangibility</code>, defined as the Property, Plant and Equipment (Net) scaled by Assets (Total), estimated for all firms in the Compustat Fundamental Annual. The result dataset is saved in <code>/result/Tangibility.dta</code>.</p> <p></p>","title":"Example Output"},{"location":"data/#supported-measures","text":"Measure Description Datasets Used     tangibility Property, Plant and Equipment (Net) scaled by Assets (Total) wrds.comp.funda   roa Income Before Extraordinary Items scaled by Assets (Total) wrds.comp.funda   roe Income Before Extraordinary Items scaled by Common Equity (Total) wrds.comp.funda   book leverage (Long-term Debt + Debt in Current Liabilities) / (Long-term Debt + Debt in Current Liabilities + Common Equity) wrds.comp.funda   capital expenditure Capital Expenditures scaled by Assets (Total) wrds.comp.funda   market to book Market Value of Common Equity to Book Common Equity wrds.comp.funda   accounting restatement Number of various accounting restatements during the fiscal year wrds.comp.funda, wrds.audit.auditnonreli","title":"Supported Measures"},{"location":"data/rds/","text":"<p>Warning</p> <p>This research data service has been deprecated and will be merged into  frds - financial research data services.</p>  <p>A database of market microstructure measures.</p> <p>Proof-of-concept joint work with Prof. Joakim Westerholm and Dr. Henry Leung.</p> <p>Our objective is to construct and maintain a database of market micriostructure measures based on high-frequency tick history sourced from Refinitiv Thomson Reuters Tick History.</p>","title":"RDS - deprecated"},{"location":"data/rds/#example-usage","text":"<p>I provide an easy-to-use SQL interface for researchers to retrieve the data. Below is an example usage in SAS.</p> <p>Currently, you'll need to be inside the USYD's network to use RDS. You can either use a PC inside the Business School or use VPN.</p> <pre><code>/* Assign lib reference to connect to the server. */\nlibname rds mysql \n    server=\"asgard.econ.usyd.edu.au\" database=rds \n    user=actualusername password=actualpassword;\n</code></pre> <p>If you enconter this error:</p>  <p>ERROR: The SAS/ACCESS Interface to MYSQL cannot be loaded. The libmysql code appendage could not be loaded.</p>  <p>Solution is here http://support.sas.com/kb/19/250.html.</p> <p>Let's now use RDS to get a collection of measures estimated.</p> <p><pre><code>/* Some example usage. */\n/* More measures and complete documentaion to come. */\nproc sql;\n/* Retrieve the full table of estimated measures. */\ncreate table measures as select * from rds.measures \n    order by local_date asc, RIC asc;\n\n/* Retrieve all Bid-Ask Spread estimates. */\ncreate table baspread as select * from rds.bidaskspread;\n\n/* Retrieve all Effective Spread estimates for RIC=AAL.OQ. */\ncreate table espread as select * from rds.effectivespread \n    where RIC='AAL.OQ';\n\n/* Retrieve all Realized Spread estimates from 2019-01-01 to 2019-01-15. */\ncreate table rspread as select * from rds.realizedspread \n    where local_date between \"01Jan2019\"d and \"15Jan2019\"d;\n\n/* Retrieve all LinSangerBooth1995 estimates of adverse selection. */\ncreate table lsb1995 as select * from rds.measures\n    where measure=\"LinSangerBooth1995\";\n\nquit;\n</code></pre> Let's try plot a timeseries to prove it works.</p> <pre><code>/* Simple timeseries plot */\ntitle \"Timeseries Plot of Realized Spread For USB.N and AIG.N\";\nproc sgplot data=measures;\n    where measure=\"RealizedSpread\" &amp; (RIC=\"USB.N\" | RIC=\"AIG.N\");\n    series x=local_date y=estimate /markers group=RIC;\n    refline 0/axis=y;\nrun;\n</code></pre> <p>The output is:</p> <p></p>","title":"Example Usage"},{"location":"data/rds/#technology-stack","text":"<p>I wrote this system in Python and C/C++. A workstation of an 8-core 16-thread CPU, 64GB RAM and m.2. SSDs is used.</p> <p></p>  <p>Behind the scene, the program classifies trade directions using Lee and Ready (1991) algorithm on the fly, and estimates several measures for each security and each day.</p>  <p>Results are stored in a MySQL database inside the university network, but may be stored and served at AWS in the future.</p>","title":"Technology Stack"},{"location":"data/rds/#development-plan","text":"<p>We plan to continue the development of this project and:</p> <ul> <li>cover more measures, securities and extend the data period;</li> <li>provide an easy-to-use web interface apart from the SQL interface;</li> <li>provide a REST API for more efficient and professional usage.</li> </ul>","title":"Development Plan"},{"location":"data/rds/#disclaimer","text":"<p>This project may contain errors. Users are recommended to double check the data quality before usage. We hold no responsibility for any damage and/or loss incurred as a result of using any data provided on this site. We may provide the source code for selected measures and encourage users to check it for correctness and accuracy.</p> <p>If there is any bug and/or error, please contact me at mingze.gao@sydney.edu.au.</p>","title":"Disclaimer"},{"location":"data/rds/#contact","text":"<ul> <li>Mingze Gao: mingze.gao@sydney.edu.au.</li> <li>Henry Leung: henry.leung@sydney.edu.au.</li> <li>Joakim Westerholm: joakim.westerholm@sydney.edu.au.</li> </ul>","title":"Contact"},{"location":"measures/accounting_restatement/","text":"","title":"Accounting Restatement"},{"location":"measures/accounting_restatement/#definition","text":"<p>Restating previous financial statements is a signal worth attention and investigation. This measure counts the number of restatements in the past fiscal year (configurable to arbitrary number of years) due to various reasons based on the WRDS AuditAnalytics Non-Reliance Restatement dataset.</p> <p>Specifically, the following variables from <code>WRDS.AUDIT.AUDITNONRELI</code> are used:</p>    Variable Description     <code>company_fkey</code> EDGAR CIK   <code>file_date</code> Filing date   <code>res_notif_key</code> Restatement notification key   <code>res_accounting</code> Restatement due to accounting issues   <code>res_adverse</code> Restatement had an adverse impact   <code>res_fraud</code> Restatement related to fraud   <code>res_cler_err</code> Restatement due to clerical errors   <code>res_sec_invest</code> Restatement followed by SEC investigation    <p>The dataset is merged with <code>WRDS.COMP.FUNDA</code> on <code>CIK</code>, grouped by <code>gvkey</code> and <code>datadate</code>. Then the frequency of each restatement type is counted.</p>","title":"Definition"},{"location":"measures/asset_tangibility/","text":"","title":"Asset Tangibility"},{"location":"measures/asset_tangibility/#definition","text":"<p>Property, Plant and Equipment (Net) scaled by Assets (Total).</p>   \\text{Asset Tangibility}_{i,t}=\\frac{PPENT_{i,t}}{AT_{i,t}}    <p>where PPENTPPENT and ATAT are from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/bhc_dividend_to_assets/","text":"","title":"BHC Dividend / Assets"},{"location":"measures/bhc_dividend_to_assets/#definition","text":"<p>Cash dividends on common stock <code>BHCK4460</code> scaled by total assets <code>BHCK2170</code>.</p>   \\text{Dividend/Assets}_{i,t}=\\frac{\\text{BHCK4460}_{i,t}}{\\text{BHCK2170}_{i,t}}    <p>where <code>BHCK4460</code> and <code>BHCK2170</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_dividend_to_assets/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_fx_exposure/","text":"","title":"BHC FX Exposure"},{"location":"measures/bhc_fx_exposure/#definition","text":"<p>Fee and interest income from loans in foreign offices <code>BHCK4059</code> scaled by total interest income <code>BHCK4107</code>.</p>   \\text{FX Exposure}_{i,t}=\\frac{\\text{BHCK4059}_{i,t}}{\\text{BHCK4107}_{i,t}}    <p>where <code>BHCK4059</code> and <code>BHCK4107</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_fx_exposure/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_gross_fx_hedging/","text":"","title":"BHC Gross Foreign Exchange Rate Hedging"},{"location":"measures/bhc_gross_fx_hedging/#definition","text":"<p>Total gross notional amount of foreign exchange rate derivatives held for purposes other than trading <code>BHCK8726</code> over total assets <code>BHCK2170</code>; for the period 1995 to 2000, contracts not marked to market <code>BHCK8730</code> are added.</p> <p>If t \\in [1995, 2000]:</p>   \\text{Gross FX Hedging}_{i,t}=\\frac{\\text{BHCK8726}_{i,t}+\\text{BHCK8730}_{i,t}}{\\text{BHCK2170}_{i,t}}   \\text{Gross FX Hedging}_{i,t}=\\frac{\\text{BHCK8726}_{i,t}+\\text{BHCK8730}_{i,t}}{\\text{BHCK2170}_{i,t}}   <p>Otherwise:</p>   \\text{Gross FX Hedging}_{i,t}=\\frac{\\text{BHCK8726}_{i,t}}{\\text{BHCK2170}_{i,t}}   \\text{Gross FX Hedging}_{i,t}=\\frac{\\text{BHCK8726}_{i,t}}{\\text{BHCK2170}_{i,t}}   <p>where <code>BHCK8726</code>, <code>BHCK8730</code> and <code>BHCK4107</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_gross_fx_hedging/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_gross_ir_hedging/","text":"","title":"BHC Gross Interest Rate Hedging"},{"location":"measures/bhc_gross_ir_hedging/#definition","text":"<p>Total gross notional amount of interest rate derivatives held for purposes other than trading <code>BHCK8725</code> over total assets <code>BHCK2170</code>; for the period 1995 to 2000, contracts not marked to market <code>BHCK8729</code> are added.</p> <p>If t \\in [1995, 2000]:</p>   \\text{Gross IR Hedging}_{i,t}=\\frac{\\text{BHCK8725}_{i,t}+\\text{BHCK8729}_{i,t}}{\\text{BHCK2170}_{i,t}}   \\text{Gross IR Hedging}_{i,t}=\\frac{\\text{BHCK8725}_{i,t}+\\text{BHCK8729}_{i,t}}{\\text{BHCK2170}_{i,t}}   <p>Otherwise:</p>   \\text{Gross IR Hedging}_{i,t}=\\frac{\\text{BHCK8725}_{i,t}}{\\text{BHCK2170}_{i,t}}   \\text{Gross IR Hedging}_{i,t}=\\frac{\\text{BHCK8725}_{i,t}}{\\text{BHCK2170}_{i,t}}   <p>where <code>BHCK8725</code>, <code>BHCK8729</code> and <code>BHCK4107</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_gross_ir_hedging/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_loan_growth/","text":"","title":"BHC Loan Growth"},{"location":"measures/bhc_loan_growth/#definition","text":"<p>The natural logarithm of a bank's total loans in the current quarter, divided by its total loans in the previous quarter.2 Here the total loans are measures by <code>BHCK2122</code> in FR Y-9C reports. This item represents the proportion of a bank's total loans that do not exclude the allowance for loans and lease losses.</p>   \\text{LoanGrowth}_{i,t} = \\ln \\left( \\frac{\\text{BHCK2122}_{i,t}}{\\text{BHCK2122}_{i,t-1}} \\right)    <p>where <code>BHCK2122</code> is from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>  <p>Warning</p> <p>A caveat is that if \\text{BHCK2122}_{i,t}\\text{BHCK2122}_{i,t} is 0, then this measure would be invalid as \\ln(0)\\ln(0) is undefined. In this case, I replace it with -1 which means the total loans is reduced by 100% to 0.</p>  <p>An alternative measure of loan growth, the percentage change in the total loans, is not affected by this issue.</p>   \\text{LoanGrowthPct}_{i,t} = \\left( \\frac{\\text{BHCK2122}_{i,t}}{\\text{BHCK2122}_{i,t-1}} -1\\right) \\times 100   \\text{LoanGrowthPct}_{i,t} = \\left( \\frac{\\text{BHCK2122}_{i,t}}{\\text{BHCK2122}_{i,t-1}} -1\\right) \\times 100   <p>Their correlation is over 99%.</p>","title":"Definition"},{"location":"measures/bhc_loan_growth/#equivalent-stata-code","text":"<pre><code>use \"~/frds/result/BankHoldingCompany LoanGrowth.dta\", clear\ngen qtr = qofd(RSSD9999)\nformat qtr %tq\nxtset RSSD9001 qtr, quarterly\ngen BHCLoanGrowthPct = (BHCK2122 / L.BHCK2122 - 1) * 100\n</code></pre>","title":"Equivalent Stata Code"},{"location":"measures/bhc_loan_growth/#reference","text":"<ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> <li> <p>This definition is from Zheng (2020 JBF).\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_maturity_gap/","text":"","title":"BHC Maturity Gap"},{"location":"measures/bhc_maturity_gap/#definition","text":"","title":"Definition"},{"location":"measures/bhc_maturity_gap/#maturity-gap","text":"<p>Earning assets that are repriceable or mature within one year <code>BHCK3197</code> minus interest-bearing deposits that mature or reprice within one year <code>BHCK3296</code> minus long-term debt that reprices or matures within one year <code>BHCK3298 + BHCK3409</code> minus variable rate preferred stock <code>BHCK3408</code> minus other borrowed money with a maturity of one year or less <code>BHCK2332</code> minus commercial paper <code>BHCK2309</code> minus federal funds and repo liabilities <code>BHDMB993 + BHCKB995</code>, normalized by total assets <code>BHCK2170</code>, where all variables are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Maturity Gap"},{"location":"measures/bhc_maturity_gap/#narrow-maturity-gap","text":"<p>Narrow maturity gap is defined similarly to Maturity Gap, except that it does not subtract interest-bearing deposits that mature or reprice within one year <code>BHCK3296</code>.</p>","title":"Narrow Maturity Gap"},{"location":"measures/bhc_maturity_gap/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_netincome_to_assets/","text":"","title":"BHC Net Income / Assets"},{"location":"measures/bhc_netincome_to_assets/#definition","text":"<p>Net income <code>BHCK4340</code> scaled by total assets <code>BHCK2170</code>.</p>   \\text{NetIncome/Assets}_{i,t}=\\frac{\\text{BHCK4340}_{i,t}}{\\text{BHCK2170}_{i,t}}    <p>where <code>BHCK4340</code> and <code>BHCK2170</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_netincome_to_assets/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_regcap_to_assets/","text":"","title":"BHC Regulatory Capital / Assets"},{"location":"measures/bhc_regcap_to_assets/#definition","text":"<p>Total qualifying capital allowable under the risk-based capital guidelines <code>BHCK3792</code> normalized by risk-weighted assets <code>BHCKA223</code>.</p>   \\text{RegCap/Assets}_{i,t}=\\frac{\\text{BHCK3792}_{i,t}}{\\text{BHCKA223}_{i,t}}    <p>where <code>BHCK3792</code> and <code>BHCKA223</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_regcap_to_assets/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/bhc_size/","text":"","title":"BHC Size"},{"location":"measures/bhc_size/#definition","text":"<p>Natural logarithm of total assets <code>BHCK2170</code>.</p>   \\text{Size}_{i,t} = \\ln \\left( \\text{BHCK2170}_{i,t} \\right)    <p>where <code>BHCK2170</code> is from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Definition"},{"location":"measures/bhc_tier1cap_to_assets/","text":"","title":"BHC Tier1 Capital / Assets"},{"location":"measures/bhc_tier1cap_to_assets/#definition","text":"<p>Tier 1 capital allowable under the risk-based capital guidelines <code>BHCK8274</code> normalized by risk-weighted assets <code>BHCKA223</code>.</p>   \\text{Tier1Cap/Assets}_{i,t}=\\frac{\\text{BHCK8274}_{i,t}}{\\text{BHCKA223}_{i,t}}    <p>where <code>BHCK8274</code> and <code>BHCKA223</code> are from the Bank Holding Company data by the Federal Reserve Bank of Chicago.1 </p>","title":"Definition"},{"location":"measures/bhc_tier1cap_to_assets/#reference","text":"<p>Rampini, Viswanathan and Vuillemey (2020 JF).</p>   <ol> <li> <p>https://www.chicagofed.org/banking/financial-institution-reports/bhc-data.\u00a0\u21a9</p> </li> </ol>","title":"Reference"},{"location":"measures/board_independence/","text":"","title":"Board Independence"},{"location":"measures/board_independence/#definition","text":"","title":"Definition"},{"location":"measures/board_independence/#boardsize","text":"<p>Number of directors, where directors are identified as those whose <code>seniority</code> in the BoardEx <code>na_wrds_org_composition</code> table is either \"Executiver Director\" or \"Supervisory Director\".</p>","title":"<code>BoardSize</code>"},{"location":"measures/board_independence/#independentmembers","text":"<p>Number of independent members on board, identified as those directors whose <code>rolename</code> in BoardEx <code>na_wrds_org_composition</code> table contains \"independent\".</p>","title":"<code>IndependentMembers</code>"},{"location":"measures/board_independence/#boardindependence","text":"<p>Ratio of independent board members to board size.</p>   \\text{BoardIndependence}_{i,t}=\\frac{\\text{IndependentMembers}_{i,t}}{\\text{BoardSize}_{i,t}}","title":"<code>BoardIndependence</code>"},{"location":"measures/book_leverage/","text":"","title":"Book Leverage"},{"location":"measures/book_leverage/#definition","text":"<p>The book leverage is defined as the amount of debts scaled by the firm's total debts plus common equity.</p>   \\text{Book Leverage}_{i,t} = \\frac{DLTT_{i,t}+DLC_{i,t}}{DLTT_{i,t}+DLC_{i,t}+CEQ_{i,t}}    <p>where DLTTDLTT is the long-term debt, DLCDLC is the debt in current liabilities, and CEQCEQ is the common equity, all from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p> <p>If CEQCEQ is missing, the book leverage is treated as missing.</p>","title":"Definition"},{"location":"measures/capital_expenditure/","text":"","title":"Capital Expenditure"},{"location":"measures/capital_expenditure/#definition","text":"<p>The capital expenditures scaled by total assets.</p>   \\text{Capital Expenditure}_{i,t} = \\frac{CAPX_{i,t}}{AT_{i,t}}    <p>where CAPXCAPX and ATAT are from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/credit_rating/","text":"","title":"Credit Rating"},{"location":"measures/credit_rating/#definition","text":"<p>S&amp;P Credit Ratings retrieved from the WRDS Capital IQ database. Specifically, it merges <code>wrds.ciq.erating</code> and <code>wrds.ciq.gvkey</code> by Company ID and if there are multiple ratings issued in a day for the same entity, the last rating data is used. However, there are a few cases where the same entity receive different ratings the same day, and the reporting time data is insufficient to discern the most recent one.</p> <p>For ease of empirical analysis, the S&amp;P ratings are transformed into conventional numerical scores from 1 to 22, where 1 represents a AAA rating and 22 reflects a D rating. Specifically, the mapping is:</p> <ul> <li>\"AAA\": 1,</li> <li>\"AA+\": 2,</li> <li>\"AA\": 3,</li> <li>\"AA-\": 4,</li> <li>\"A+\": 5,</li> <li>\"A\": 6,</li> <li>\"A-\": 7,</li> <li>\"BBB+\": 8,</li> <li>\"BBB\": 9,</li> <li>\"BBB-\": 10,</li> <li>\"BB+\": 11,</li> <li>\"BB\": 12,</li> <li>\"BB-\": 13,</li> <li>\"B+\": 14,</li> <li>\"B\": 15,</li> <li>\"B-\": 16,</li> <li>\"CCC+\": 17,</li> <li>\"CCC\": 18,</li> <li>\"CCC-\": 19,</li> <li>\"CC\": 20,</li> <li>\"C\": 21,</li> <li>\"D\": 22.</li> </ul>","title":"Definition"},{"location":"measures/credit_rating/#equivalent-sas-code","text":"<pre><code>proc sql;\ncreate table credt_rating as \nselect distinct company_id, rdate, rating, rtype, b.gvkey \nfrom ciq.wrds_erating as a inner join ciq.wrds_gvkey as b\non a.company_id=b.companyid and \n    (a.rdate &lt;= b.enddate or b.enddate=.E) and (a.rdate &gt;= b.startdate or b.startdate=.B)\n    and not missing(b.gvkey) and rtype=\"Local Currency LT\"\ngroup by gvkey, rdate having rtime=max(rtime)\norder by company_id, rdate; \nquit;\n</code></pre>","title":"Equivalent SAS Code"},{"location":"measures/executive_ownership/","text":"","title":"Executive Ownership"},{"location":"measures/executive_ownership/#definition","text":"<p>Executive ownership measures the proportion of the firm's common equity owned by its executives.</p>","title":"Definition"},{"location":"measures/executive_ownership/#execsharepct","text":"<p>Executive-year level, executive share ownership:</p>   \\text{ExecSharePct}_{e,t} = \\frac{SHROWN\\_TOT_{e,t}}{CSHO_{i,t}}    <p>where SHROWN\\_TOTSHROWN\\_TOT is the shares owned by the executive (as reported) from Compustat Execucomp Annual Compensation <code>EXECCOMP.ANNCOMP</code>, and CSHOCSHO is the common shares outstanding from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"<code>ExecSharePct</code>"},{"location":"measures/executive_ownership/#execsharepctexclopt","text":"<p>Executive-year level, executive share ownership excluding options:</p>   \\text{ExecSharePctExclOpt}_{e,t} = \\frac{SHROWN\\_EXCL\\_OPTS_{e,t}}{CSHO_{i,t}}   \\text{ExecSharePctExclOpt}_{e,t} = \\frac{SHROWN\\_EXCL\\_OPTS_{e,t}}{CSHO_{i,t}}   <p>where SHROWN\\_EXCL\\_OPTSSHROWN\\_EXCL\\_OPTS is the shares owned by the executive, excluding options, from Compustat Execucomp Annual Compensation <code>EXECCOMP.ANNCOMP</code>, and CSHOCSHO is the common shares outstanding from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"<code>ExecSharePctExclOpt</code>"},{"location":"measures/executive_ownership/#execoptpct","text":"<p>Executive-year level, executive share ownership based on shares acquired on option exercise:</p>   \\text{ExecOptPct}_{e,t} = \\frac{OPT\\_EXER\\_NUM_{e,t}}{CSHO_{i,t}}   \\text{ExecOptPct}_{e,t} = \\frac{OPT\\_EXER\\_NUM_{e,t}}{CSHO_{i,t}}   <p>where OPT\\_EXER\\_NUMOPT\\_EXER\\_NUM is the number of shares acquired on option exercise by the executive from Compustat Execucomp Annual Compensation <code>EXECCOMP.ANNCOMP</code>, and CSHOCSHO is the common shares outstanding from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"<code>ExecOptPct</code>"},{"location":"measures/executive_ownership/#execsharevestpct","text":"<p>Executive-year level, executive share ownership based on shared acquired on vesting:</p>   \\text{ExecShareVestPct}_{e,t} = \\frac{SHRS\\_VEST\\_NUM_{e,t}}{CSHO_{i,t}}   \\text{ExecShareVestPct}_{e,t} = \\frac{SHRS\\_VEST\\_NUM_{e,t}}{CSHO_{i,t}}   <p>where SHRS\\_VEST\\_NUMSHRS\\_VEST\\_NUM is the number of shares acquired on vesting by the executive from Compustat Execucomp Annual Compensation <code>EXECCOMP.ANNCOMP</code>, and CSHOCSHO is the common shares outstanding from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"<code>ExecShareVestPct</code>"},{"location":"measures/executive_ownership/#execincentivepct","text":"<p>Executive-year level, value realized on option exercise and vesting scaled by total compensation:</p>   \\text{ExecIncentivePct}_{e,t} = \\frac{OPT\\_EXER\\_VAL_{e,t} + SHRS\\_VEST\\_NUM_{e,t}}{TDC1_{e,t}}   \\text{ExecIncentivePct}_{e,t} = \\frac{OPT\\_EXER\\_VAL_{e,t} + SHRS\\_VEST\\_NUM_{e,t}}{TDC1_{e,t}}   <p>where OPT\\_EXER\\_VALOPT\\_EXER\\_VAL is the value realized on vesting and TDC1TDC1 is the executive's total compensation, both from Compustat Execucomp Annual Compensation <code>EXECCOMP.ANNCOMP</code>. </p>","title":"<code>ExecIncentivePct</code>"},{"location":"measures/firm_size/","text":"","title":"Firm Size"},{"location":"measures/firm_size/#definition","text":"<p>The natural logarithm of total assets.</p>   \\text{Firm Size}_{i,t} = \\ln \\left( AT_{i,t} \\right)    <p>where ATAT is from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/kyleslambda/","text":"<p>A measure of market impact cost from Kyle (1985), which can be interpreted as the cost of demanding a certain amount of liquidity over a given time period.</p>","title":"Kyle's Lambda"},{"location":"measures/kyleslambda/#definition","text":"<p>Following Hasbrouck (2009) and Goyenko, Holden, Trzcinka (2009), Kyle's Lambda for a given stock i and day tt, is calculated as the slope coefficient \\lambda_{i,t}\\lambda_{i,t} in the regression:</p>   ret_{i,t,n}= \\delta_{i,t} + \\lambda_{i,t} S_{i,t,n}+\\epsilon_{i,t,n}   ret_{i,t,n}= \\delta_{i,t} + \\lambda_{i,t} S_{i,t,n}+\\epsilon_{i,t,n}   <p>where for the nnth five-minute period on date tt and stock ii, ret_{i,t,n}ret_{i,t,n} is the stock return and S_{i,t,n}S_{i,t,n} is the sum of the signed square-root dollar volume, that is,</p>   S_{i,t,n}=\\sum_k{sign}(dvol_{i,t,n,k}) \\sqrt{dvol_{i,t,n,k}}   S_{i,t,n}=\\sum_k{sign}(dvol_{i,t,n,k}) \\sqrt{dvol_{i,t,n,k}}","title":"Definition"},{"location":"measures/kyleslambda/#source-code","text":"<p> </p> <p>This example Python code is not optimized for speed and serves only demonstration purpose. It may contain errors.</p> <p>It returns \\lambda \\times 10^6\\lambda \\times 10^6</p> <pre><code># KylesLambda.py\nimport numpy as np\n\nname = 'KylesLambda'\ndescription = \"\"\"\nA measure of market impact cost from Kyle (1985), \nwhich can be interpreted as the cost of demanding a certain amount of liquidity over a given time period.\nResult is Lambda*1E6.\n\"\"\"\nvars_needed = ['Price', 'Volume', 'Direction']\n\n\ndef estimate(data):\n    price = data['Price'].to_numpy()\n    volume = data['Volume'].to_numpy()\n    direction = data['Direction'].to_numpy()\n    sqrt_dollar_volume = np.sqrt(np.multiply(price, volume))\n    signed_sqrt_dollar_volume = np.abs(\n        np.multiply(direction, sqrt_dollar_volume))\n    # Find the total signed sqrt dollar volume and return per 5 min.\n    timestamps = np.array(data.index, dtype='datetime64')\n    last_ts, last_price = timestamps[0], price[0]\n    bracket_ssdv = 0\n    bracket = last_ts + np.timedelta64(5, 'm')\n    rets, ssdvs, = [], []\n    for idx, ts in enumerate(timestamps):\n        if ts &lt;= bracket:\n            bracket_ssdv += signed_sqrt_dollar_volume[idx]\n        else:\n            ret = np.log(price[idx-1]/last_price)\n            if not np.isnan(ret) and not np.isnan(bracket_ssdv):\n                rets.append(ret)\n                ssdvs.append(bracket_ssdv)\n            # Reset bracket\n            bracket = ts + np.timedelta64(5, 'm')\n            last_price = price[idx]\n            bracket_ssdv = signed_sqrt_dollar_volume[idx]\n    # Perform regression.\n    x = np.vstack([np.ones(len(ssdvs)), np.array(ssdvs)]).T\n    try:\n        coef, _, _, _ = np.linalg.lstsq(x, np.array(rets), rcond=None)\n    except np.linalg.LinAlgError:\n        return None\n    else:\n        return None if np.isnan(coef[1]) else coef[1]*1E6\n</code></pre>","title":"Source Code"},{"location":"measures/lomackinlay1988/","text":"<p>A simple test for the random walk hypothesis of prices and efficient market.</p>","title":"Variance Ratio Test - Lo and MacKinlay (1988)"},{"location":"measures/lomackinlay1988/#definition","text":"<p>Let's assume:</p> <ul> <li>a price series \\{p_t\\}=\\{p_0,p_1,p_2,...,p_T\\} and</li> <li>a log return series \\{x_t\\}=\\{x_1, x_2, ..., x_T\\}\\{x_t\\}=\\{x_1, x_2, ..., x_T\\} where x_t=\\ln \\frac{p_t}{p_{t-1}}x_t=\\ln \\frac{p_t}{p_{t-1}}</li> </ul>","title":"Definition"},{"location":"measures/lomackinlay1988/#variance-ratio-vr","text":"<p>The variance ratio of kk-period return is defined as:</p>  \\textit{V}(k)=\\frac{\\textit{Var}(x_t+x_{t-1}+...+x_{t-k+1})/k}{\\textit{Var}(x_t)} \\textit{V}(k)=\\frac{\\textit{Var}(x_t+x_{t-1}+...+x_{t-k+1})/k}{\\textit{Var}(x_t)}  <p>The estimator of \\textit{V}(k)\\textit{V}(k) proposed in Lo and MacKinlay (1988) is</p>  \\textit{VR}(k)=\\frac{\\hat\\sigma^2(k)}{\\hat\\sigma^2(1)} \\textit{VR}(k)=\\frac{\\hat\\sigma^2(k)}{\\hat\\sigma^2(1)}  <p>where \\hat\\sigma^2(1)\\hat\\sigma^2(1) is the unbiased estimator of the one-period return variance, using the one-period returns \\{x_t\\}\\{x_t\\}, and is defined as</p>  \\hat\\sigma^2(1)=\\frac{1}{T-1} \\sum_{t-1}^T (x_t - \\hat\\mu)^2 \\hat\\sigma^2(1)=\\frac{1}{T-1} \\sum_{t-1}^T (x_t - \\hat\\mu)^2  <p>and \\hat\\sigma^2(k)\\hat\\sigma^2(k) is the estimator of kk-period return variance using kk-period returns. Lo and MacKinlay (1988) defined it, due to limited sample size and the desire to improve the power of the test, as</p>  \\hat\\sigma^2(k)=\\frac{1}{m} \\sum_{t-1}^T \\left(\\ln\\frac{P_t}{P_{t-k}} - k\\hat\\mu \\right)^2 \\hat\\sigma^2(k)=\\frac{1}{m} \\sum_{t-1}^T \\left(\\ln\\frac{P_t}{P_{t-k}} - k\\hat\\mu \\right)^2  <p>where m=k(T-k+1)(1-k/T)m=k(T-k+1)(1-k/T) is chosen such that \\hat\\sigma^2(k)\\hat\\sigma^2(k) is an unbiased estimator of the kk-period return variance when \\sigma^2_t\\sigma^2_t is constant over time.</p>","title":"Variance Ratio (VR)"},{"location":"measures/lomackinlay1988/#variance-ratio-test-statistics","text":"<p>Lo and MacKinlay (1988) proposed that under the null hypothesis of V(k)=1V(k)=1, the test statistic is given by</p>  Z(k)=\\frac{\\textit{VR}(k)-1}{\\sqrt{\\phi(k)}} Z(k)=\\frac{\\textit{VR}(k)-1}{\\sqrt{\\phi(k)}}  <p>which follows the standard normal distribution asymptotically.</p>","title":"Variance Ratio Test Statistics"},{"location":"measures/lomackinlay1988/#homoscedasticity","text":"<p>Under the assumption of homoscedasticity, the asymptotic variance \\phi\\phi is given by </p>  \\phi(k)=\\frac{2(2k-1)(k-1)}{3kT} \\phi(k)=\\frac{2(2k-1)(k-1)}{3kT}","title":"Homoscedasticity"},{"location":"measures/lomackinlay1988/#heteroscedasticity","text":"<p>Under the assumption of heteroscedasticity, the asymptotic variance \\phi\\phi is given by </p>  \\phi(k)=\\sum_{j=1}^{k-1} \\left[\\frac{2(k-j)}{k} \\right]^2\\delta(j) \\phi(k)=\\sum_{j=1}^{k-1} \\left[\\frac{2(k-j)}{k} \\right]^2\\delta(j)   \\delta(j)=\\frac{\\sum_{t=j+1}^T (x_t - \\hat\\mu)^2(x_{t-j} - \\hat\\mu)^2}{\\left[\\sum_{t=1}^T (x_t - \\hat\\mu)^2\\right]^2} \\delta(j)=\\frac{\\sum_{t=j+1}^T (x_t - \\hat\\mu)^2(x_{t-j} - \\hat\\mu)^2}{\\left[\\sum_{t=1}^T (x_t - \\hat\\mu)^2\\right]^2}   <p>Erratum</p> <p>Note that there's a missing TT in the numerator of \\delta(j)\\delta(j) above. It is actually missing the 1988 RFS paper and the 1998 JE'mtric paper, but has been corrected in the 1990 RFS Issue 1: https://doi.org/10.1093/rfs/3.1.ii. The corrected version reads:</p>  \\delta(j)=\\frac{T\\sum_{t=j+1}^T (x_t - \\hat\\mu)^2(x_{t-j} - \\hat\\mu)^2}{\\left[\\sum_{t=1}^T (x_t - \\hat\\mu)^2\\right]^2} \\delta(j)=\\frac{T\\sum_{t=j+1}^T (x_t - \\hat\\mu)^2(x_{t-j} - \\hat\\mu)^2}{\\left[\\sum_{t=1}^T (x_t - \\hat\\mu)^2\\right]^2}  <p>To correct it in the example code below, change the highlighted line 51 to: <pre><code>delta_arr = T * b_arr / np.square(np.sum(sqr_demeaned_x))\n</code></pre></p> <p>I thank Simon Jurkatis for letting me know about the erratum.</p>","title":"Heteroscedasticity"},{"location":"measures/lomackinlay1988/#source-code","text":"<p> </p> <p>This example Python code has been optimized for speed but serves only demonstration purpose. It may contain errors.</p> <pre><code># LoMacKinlay.py\nimport numpy as np\nfrom numba import jit\n\nname = 'LoMacKinlay1988'\ndescription = 'Variance ratio and test statistics as in Lo and MacKinlay (1988)'\nvars_needed = ['Price']\n\n\n@jit(nopython=True, nogil=True, cache=True)\ndef _estimate(log_prices, k, const_arr):\n    # Log returns = [x2, x3, x4, ..., xT], where x(i)=ln[p(i)/p(i-1)]\n    rets = np.diff(log_prices)\n    # T is the length of return series\n    T = len(rets)\n    # mu is the mean log return\n    mu = np.mean(rets)\n    # sqr_demeaned_x is the array of squared demeaned log returns\n    sqr_demeaned_x = np.square(rets - mu)\n    # Var(1)\n    # Didn't use np.var(rets, ddof=1) because\n    # sqr_demeaned_x is calculated already and will be used many times.\n    var_1 = np.sum(sqr_demeaned_x) / (T-1)\n    # Var(k)\n    # Variance of log returns where x(i) = ln[p(i)/p(i-k)]\n    # Before np.roll() - array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    # After np.roll(,shift=2) - array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    # Discard the first k elements.\n    rets_k = (log_prices - np.roll(log_prices, k))[k:]\n    m = k * (T - k + 1) * (1 - k / T)\n    var_k = 1/m * np.sum(np.square(rets_k - k * mu))\n\n    # Variance Ratio\n    vr = var_k / var_1\n\n    # a_arr is an array of { (2*(k-j)/k)^2 } for j=1,2,...,k-1, fixed for a given k:\n    #   When k=5, a_arr = array([2.56, 1.44, 0.64, 0.16]).\n    #   When k=8, a_arr = array([3.0625, 2.25, 1.5625, 1., 0.5625, 0.25, 0.0625])\n    # Without JIT it's defined as:\n    #   a_arr = np.square(np.arange(k-1, 0, step=-1, dtype=np.int) * 2 / k)\n    # But np.array creation is not allowed in nopython mode.\n    # So const_arr=np.arange(k-1, 0, step=-1, dtype=np.int) is created outside.\n    a_arr = np.square(const_arr * 2 / k)\n\n    # b_arr is part of the delta_arr.\n    b_arr = np.empty(k-1, dtype=np.float64)\n    for j in range(1, k):\n        b_arr[j-1] = np.sum((sqr_demeaned_x *\n                             np.roll(sqr_demeaned_x, j))[j+1:])\n\n    delta_arr = b_arr / np.square(np.sum(sqr_demeaned_x))\n\n    # Both arrarys are of length (k-1)\n    assert len(delta_arr) == len(a_arr) == k-1\n\n    phi1 = 2 * (2*k - 1) * (k-1) / (3*k*T)\n    phi2 = np.sum(a_arr * delta_arr)\n\n    # VR test statistics under two assumptions\n    vr_stat_homoscedasticity = (vr - 1) / np.sqrt(phi1)\n    vr_stat_heteroscedasticity = (vr - 1) / np.sqrt(phi2)\n\n    return vr, vr_stat_homoscedasticity, vr_stat_heteroscedasticity\n\n\ndef estimate(data):\n    \"A fast estimation of Variance Ratio test statistics as in Lo and MacKinlay (1988)\"\n    # Prices array = [p1, p2, p3, p4, ..., pT]\n    prices = data['Price'].to_numpy(dtype=np.float64)\n    result = []\n    # Estimate many lags.\n    for k in [2, 4, 6, 8, 10, 15, 20, 30, 40, 50, 100, 200, 500, 1000]:\n        # Compute a constant array as np.array creation is not allowed in nopython mode.\n        const_arr = np.arange(k-1, 0, step=-1, dtype=np.int)\n        vr, stat1, stat2 = _estimate(np.log(prices), k, const_arr)\n        result.append({\n            f'Variance Ratio (k={k})': vr,\n            f'Variance Ratio Test Statistic (k={k}) Homoscedasticity Assumption': stat1,\n            f'Variance Ratio Test Statistic (k={k}) Heteroscedasticity Assumption': stat2\n        })\n    return result\n</code></pre> <p>As an example, let's create 1 million prices from random walk and estimate the variance ratio and two test statistics at various lags.</p> <pre><code>if __name__ == \"__main__\":\n\n    import pandas as pd\n    from pprint import pprint\n    np.random.seed(1)\n    # Generate random steps with mean=0 and standard deviation=1\n    steps = np.random.normal(0, 1, size=1000000)\n    # Set first element to 0 so that the first price will be the starting stock price\n    steps[0] = 0\n    # Simulate stock prices, P with a large starting price\n    P = 10000 + np.cumsum(steps)\n    # Test\n    data = pd.DataFrame(P, columns=['Price'])\n    result = estimate(data)\n    pprint(result)\n</code></pre> <p>In just a few seconds, the output is:</p> <pre><code>[{'Variance Ratio (k=2)': 1.0003293867428105,\n  'Variance Ratio Test Statistic (k=2) Heteroscedasticity Assumption': 0.3290463403922243,\n  'Variance Ratio Test Statistic (k=2) Homoscedasticity Assumption': 0.32938657811705435},\n {'Variance Ratio (k=4)': 1.0007984480057006,\n  'Variance Ratio Test Statistic (k=4) Heteroscedasticity Assumption': 0.4259533413884602,\n  'Variance Ratio Test Statistic (k=4) Homoscedasticity Assumption': 0.4267881978178301},\n {'Variance Ratio (k=6)': 0.9999130202975425,\n  'Variance Ratio Test Statistic (k=6) Heteroscedasticity Assumption': -0.035117568315004344,\n  'Variance Ratio Test Statistic (k=6) Homoscedasticity Assumption': -0.03518500446785826},\n {'Variance Ratio (k=8)': 1.0001094011344318,\n  'Variance Ratio Test Statistic (k=8) Heteroscedasticity Assumption': 0.036922688136577515,\n  'Variance Ratio Test Statistic (k=8) Homoscedasticity Assumption': 0.03698431520269611},\n {'Variance Ratio (k=10)': 1.000702410129927,\n  'Variance Ratio Test Statistic (k=10) Heteroscedasticity Assumption': 0.20772743120012313,\n  'Variance Ratio Test Statistic (k=10) Homoscedasticity Assumption': 0.20803582207641647},\n {'Variance Ratio (k=15)': 1.0022173139633856,\n  'Variance Ratio Test Statistic (k=15) Heteroscedasticity Assumption': 0.5213067838911684,\n  'Variance Ratio Test Statistic (k=15) Homoscedasticity Assumption': 0.5219816274021579},\n {'Variance Ratio (k=20)': 1.0038048661705044,\n  'Variance Ratio Test Statistic (k=20) Heteroscedasticity Assumption': 0.7646395131154204,\n  'Variance Ratio Test Statistic (k=20) Homoscedasticity Assumption': 0.7655801985571125},\n {'Variance Ratio (k=30)': 1.0054447472916035,\n  'Variance Ratio Test Statistic (k=30) Heteroscedasticity Assumption': 0.8819250061384853,\n  'Variance Ratio Test Statistic (k=30) Homoscedasticity Assumption': 0.8829960534692654},\n {'Variance Ratio (k=40)': 1.0073830253022766,\n  'Variance Ratio Test Statistic (k=40) Heteroscedasticity Assumption': 1.0290213306735625,\n  'Variance Ratio Test Statistic (k=40) Homoscedasticity Assumption': 1.0303005120740392},\n {'Variance Ratio (k=50)': 1.0086502431826903,\n  'Variance Ratio Test Statistic (k=50) Heteroscedasticity Assumption': 1.0741837462564026,\n  'Variance Ratio Test Statistic (k=50) Homoscedasticity Assumption': 1.0755809312730416},\n {'Variance Ratio (k=100)': 1.0153961901671604,\n  'Variance Ratio Test Statistic (k=100) Heteroscedasticity Assumption': 1.3415119471043384,\n  'Variance Ratio Test Statistic (k=100) Homoscedasticity Assumption': 1.3434284573260773},\n {'Variance Ratio (k=200)': 1.0157046541161026,\n  'Variance Ratio Test Statistic (k=200) Heteroscedasticity Assumption': 0.9639233626580027,\n  'Variance Ratio Test Statistic (k=200) Homoscedasticity Assumption': 0.9653299929052963},\n {'Variance Ratio (k=500)': 1.0182166207668526,\n  'Variance Ratio Test Statistic (k=500) Heteroscedasticity Assumption': 0.7055681216511915,\n  'Variance Ratio Test Statistic (k=500) Homoscedasticity Assumption': 0.7065863036900429},\n {'Variance Ratio (k=1000)': 1.0187822241562863,\n  'Variance Ratio Test Statistic (k=1000) Heteroscedasticity Assumption': 0.5140698821944161,\n  'Variance Ratio Test Statistic (k=1000) Homoscedasticity Assumption': 0.5147582201029065}]\n</code></pre> <p>It's easy to see that at all lags tested, we cannot reject the null hypothesis that this price series follows a random walk.</p> <p>For comparison purpose, below is an implementation in pure Python. It is more readable but is significantly slower.</p> <pre><code>def estimate_python(data, k=5):\n    \"A slow pure python implementation\"\n    prices = data['Price'].to_numpy(dtype=np.float64)\n    log_prices = np.log(prices)\n    rets = np.diff(log_prices)\n    T = len(rets)\n    mu = np.mean(rets)\n    var_1 = np.var(rets, ddof=1, dtype=np.float64)\n    rets_k = (log_prices - np.roll(log_prices, k))[k:]\n    m = k * (T - k + 1) * (1 - k / T)\n    var_k = 1/m * np.sum(np.square(rets_k - k * mu))\n\n    # Variance Ratio\n    vr = var_k / var_1\n    # Phi1\n    phi1 = 2 * (2*k - 1) * (k-1) / (3*k*T)\n    # Phi2\n\n    def delta(j):\n        res = 0\n        for t in range(j+1, T+1):\n            t -= 1  # array index is t-1 for t-th element\n            res += np.square((rets[t]-mu)*(rets[t-j]-mu))\n        return res / ((T-1) * var_1)**2\n\n    phi2 = 0\n    for j in range(1, k):\n        phi2 += (2*(k-j)/k)**2 * delta(j)\n\n    return vr, (vr - 1) / np.sqrt(phi1), (vr - 1) / np.sqrt(phi2)\n</code></pre>","title":"Source Code"},{"location":"measures/market_to_book/","text":"","title":"Market to Book Ratio"},{"location":"measures/market_to_book/#definition","text":"<p>Market value of common equity scaled by the book value common equity.</p>   MTB_{i,t} = \\frac{PRCC\\_F_{i,t}\\times CSHO_{i,t}}{CEQ_{i,t}}    <p>where PRCC\\_FPRCC\\_F is the share price at fiscal year end, CSHOCSHO is the common shares outstanding, and CEQCEQ is common equity, all from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/measures/","text":"<p>My (growing) collection of various measures' definitions. Please see frds.io for how to compute all these measures in one click.</p>","title":"Measures"},{"location":"measures/measures/#firm-characteristics","text":"<ul> <li>Accounting Restatements<ul> <li>Number of various accounting restatements during the past (n) fiscal year.</li> <li>Source: <code>wrds.comp.funda</code>, <code>wrds.audit.auditnonreli</code>. </li> </ul> </li> <li>Asset Tangibility <ul> <li>Property, Plant and Equipment (Net) scaled by total assets.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>Board Independence<ul> <li>Board size and independence measured as the ratio of independent board members to board size.</li> <li>Source: <code>wrds.funda</code>, <code>wrds.boardex.na_wrds_company_profile</code>, <code>wrds.boardex.na_wrds_org_composition</code>.</li> </ul> </li> <li>Book Leverage<ul> <li>Amount of debts scaled by the firm's total debts plus common equity.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>Capital Expenditure<ul> <li>Capital expenditures scaled by total assets.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>Credit Rating<ul> <li>S&amp;P credit rating.</li> <li>Source: <code>wrds.ciq.erating</code>, <code>wrds.ciq.gvkey</code>.</li> </ul> </li> <li>Executive Ownership<ul> <li>Various measures of executive stock ownership.</li> <li>Source: <code>wrds.comp.funda</code>, <code>wrds.execcomp.anncomp</code>.</li> </ul> </li> <li>Firm Size<ul> <li>Natural logarithm of total assets.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>Market-to-Book Ratio<ul> <li>Market value of common equity to book value of common equity.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>ROA<ul> <li>Income before extraordinary items scaled by total assets.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>ROE<ul> <li>Income before extraordinary items scaled by common equity.</li> <li>Source: <code>wrds.comp.funda</code>.</li> </ul> </li> <li>Stock Delisting<ul> <li>Stocks delisted due to financial troubles or as a result of being merged.</li> <li>Source: <code>wrds.crsp.dse</code>.</li> </ul> </li> <li>Tobin's Q<ul> <li>Tobin's Q</li> <li>Source: <code>wrds.comp.funda</code>.</li> <li>Reference: Gompers, Ishii and Metrick (2003 QJE), and Kaplan and Zingales (1997 QJE).</li> </ul> </li> </ul>","title":"Firm Characteristics"},{"location":"measures/measures/#bank-holding-company-bhc-characteristics","text":"<ul> <li>BHC Size<ul> <li>Natural logarithm of total assets.</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> </ul> </li> <li>BHC Loan Growth<ul> <li>Natural logarithm of total loans in the current quarter divided by the total loans in the previous quarter.</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Referece: Zheng (2020 JBF).</li> </ul> </li> <li>BHC FX Exposure<ul> <li>Fee and interest income from loans in foreign offices (BHCK4059) scaled by total interest income (BHCK4107).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC NetIncome/Assets<ul> <li>Net income (BHCK4340) / total assets (BHCK2170).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC Dividend/Assets<ul> <li>Cash dividends on common stock (BHCK4460) / total assets (BHCK2170).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC RegulatoryCapital/Assets<ul> <li>Total qualifying capital allowable under the risk-based capital guidelines (BHCK3792) normalized by risk-weighted assets (BHCKA223).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC Tier1Capital/Assets<ul> <li>Tier 1 capital allowable under the risk-based capital guidelines (BHCK8274) normalized by risk-weighted assets (BHCKA223).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC Gross IR Hedging<ul> <li>Total gross notional amount of interest rate derivatives held for purposes other than trading (BHCK8725) over total assets (BHCK2170); for the period 1995 to 2000, contracts not marked to market (BHCK8729) are added.</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC Gross FX Hedging<ul> <li>Total gross notional amount of foreign exchange rate derivatives held for purposes other than trading (BHCK8726) over total assets (BHCK2170); for the period 1995 to 2000, contracts not marked to market (BHCK8730) are added.</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> <li>BHC Maturity Gap &amp; Narrow Maturity Gap<ul> <li>Maturity gap is defined as the earning assets that are repriceable or mature within one year (BHCK3197) minus interest-bearing deposits that mature or reprice within one year (BHCK3296) minus long-term debt that reprices or matures within one year (BHCK3298 + BHCK3409) minus variable rate preferred stock (BHCK3408) minus other borrowed money with a maturity of one year or less (BHCK2332) minus commercial paper (BHCK2309) minus federal funds and repo liabilities (BHDMB993 + BHCKB995), normalized by total assets.</li> <li>Narrow maturity gap does not subtract interest-bearing deposits that mature or reprice within one year (BHCK3296).</li> <li>Source: <code>frb_chicago.bhc.bhcf</code>.</li> <li>Reference: Rampini, Viswanathan and Vuillemey (2020 JF).</li> </ul> </li> </ul>","title":"Bank Holding Company (BHC) Characteristics"},{"location":"measures/roa/","text":"","title":"ROA"},{"location":"measures/roa/#definition","text":"<p>Income Before Extraordinary Items scaled by Assets (Total).</p>   ROA_{i,t} = \\frac{IB_{i,t}}{AT_{i,t}}    <p>where IBIB and ATAT are from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/roe/","text":"","title":"ROE"},{"location":"measures/roe/#definition","text":"<p>Income Before Extraordinary Items scaled by Common Equity (Total).</p>   ROE_{i,t} = \\frac{IB_{i,t}}{CEQ_{i,t}}    <p>where IBIB and CEQCEQ are from Compustat Fundamentals Annual <code>WRDS.COMP.FUNDA</code>.</p>","title":"Definition"},{"location":"measures/stock_delisting/","text":"","title":"Stock Delisting"},{"location":"measures/stock_delisting/#definition","text":"<p>This dataset contains the stocks delisted either due to financial troubles (delisting code <code>dlstcd</code> = 500-599) or as a result of being merged (delisting code = 200-299).</p>","title":"Definition"},{"location":"measures/tobin_q/","text":"","title":"Tobin's Q"},{"location":"measures/tobin_q/#definition","text":"<p>Tobin's Q is defined as the ratio between the market value of the firm over the replacement cost of its assets.</p>   \\text{TobinQ} = \\frac{\\text{Marekt value of the firm}}{\\text{Replacement cost of assets}}    <p>There're a number of ways to estimate Toin's Q empirically. Gompers, Ishii and Metrick (2003 QJE), following Kaplan and Zingales (1997 QJE), define Tobin's Q as:</p>  <p>The market value of assets divided by the book value of assets (Compustat item 6), where the market value of assets is computed as book value of assets plus the market value of common stock less the sum of the book value of common stock (Compustat item 60) and balance sheet deferred taxes (Compustat item 74). All book values for fiscal year t (from Compustat) are combined with the market value of common equity at the calendar end of year t.1</p>  <p>which gives:</p>   \\text{TobinQ}_{i,t} = \\frac{\\text{Total Assets}_{i,t} + \\text{Market Equity}_{i,t} - \\text{Book Equity}_{i,t}}{\\text{Total Assets}_{i,t}}   \\text{TobinQ}_{i,t} = \\frac{\\text{Total Assets}_{i,t} + \\text{Market Equity}_{i,t} - \\text{Book Equity}_{i,t}}{\\text{Total Assets}_{i,t}}   <p>where:</p> <ul> <li>\\text{Total Assets}\\text{Total Assets} is the book value total assets as reported</li> <li>\\text{Market Equity}=PRCC\\_C \\times CSHO\\text{Market Equity}=PRCC\\_C \\times CSHO</li> <li>\\text{Book Equity}=SEQ+TXDB+ITCB-PREF\\text{Book Equity}=SEQ+TXDB+ITCB-PREF and </li> <li>PREF=\\text{coalesce}(PSTKRV,PSTKL,PSTK)PREF=\\text{coalesce}(PSTKRV,PSTKL,PSTK)</li> </ul>","title":"Definition"},{"location":"measures/tobin_q/#variables","text":"Variable Description     <code>PRCC_C</code> Stock price at the calendar year end for a fair cross sectional comparison   <code>CSHO</code> Common shares outstanding   <code>SEQ</code> Shareholder equity   <code>TXDB</code> Deferred taxes   <code>ITCB</code> Investment Tax Credit   <code>PREF</code> Preferred Stock   <code>PSTKRV</code> Preferred stock - redemption value   <code>PSTKL</code> Preferred stock - liquidating value   <code>PSTK</code> Preferred stock - carrying value      <ol> <li> <p>See Appendix 2 of Gompers, Ishii and Metrick (2003 QJE).\u00a0\u21a9</p> </li> </ol>","title":"Variables"},{"location":"posts/100-bitcoins-forgone-for-science/","text":"<p>This post is just another piece of my serious nonsense. All of a sudden, I wanted to know how many Bitcoins I could have mined since 2012? This is because I\u2019ve known Bitcoin since its existence in 2009, but have never really put any effort in mining. Instead, I was fascinated by the idea of using distributed (volunteer) computing to solve scientific problems. For example, BOINC and related projects like World Community Grid are using the computing power donated from around the world to find effective treatments for cancer and HIV/AIDS, low-cost water filtration systems and new materials for capturing solar energy efficiently, etc. I was one of the many volunteers for a long time, even before the genesis block of Bitcoin.</p> <p>An interesting question is, what if I didn\u2019t donate my computers to volunteer computing, but used them in Bitcoin instead? How many Bitcoins I could have mined? To solve this question, I started from looking at my contribution history of the World Community Grid (it\u2019s awesome that the full history is available).</p> <p></p> <p>According to WCG\u2019s website, 7 WCG Point are equal to 1 BOINC credit, which represents 1/100 day of CPU time on a reference computer that does 1,000 MFLOPS based on the Whetstone benchmark.</p> <p>However, the defnition of BOINC credit has been changed to 1/200 day of CPU time since 2010, though on WCG\u2019s website it still says the total WCG Points divided by 700 gives the number of GigaFLOPs. I\u2019m going to stick to the WCG\u2019s website for now.</p> <p>Suppose I\u2019ve got one WCG Point today, then it means my computer has spent 1/700 day of CPU time, i.e. 123 seconds, at a computing rate of 1 GigaFLOP/second. So, if I can convert GigaFLOPs to Bitcoin hashrate, the problem will be quite easy.</p> <p>However, FLOPs cannot be converted to hashrate in a simple manner as Bitcoin hashes are about integer math, totally different from floating point operations. I\u2019m just going to use a very rough estimate that 1 hash results 12.7k FLOPs (source: BitcoinTalk thread, CoinDesk), so that</p>  <p>1 WCG Point implies mining at a speed of 78.7kH/s for 123 seconds. -- a very rough estimate</p>  <p>Then, if I received 1k Points a day, it might be safe to say I\u2019ve been mining for about 123k seconds at a speed of 78.7kH/s, which translates to an average daily hashrate of 112kH/s or 0.112MH/s.</p> <p>I did some math and found that it seems in June 2012 my hashrate was as high as 0.006% of the whole network, though one year later it\u2019s effectively 0%. lol.</p> <p></p> <p>Next step will be calculating how many Bitcoins I could have mined based on the hashrate history.</p> <p>Taking into account the average block time and the controlled supply of Bitcoin (table below), I plot the daily average number of blocks and Bitcoins generated in this period.</p>    Date reached Block Reward Era BTC/block End BTC % of Limit     2009-01-03 0 1 50.00 12.500%   2010-04-22 52500 1 50.00 25.000%   2011-01-28 105000 1 50.00 37.500%   2011-12-14 157500 1 50.00 50.000%   2012-11-28 210000 2 25.00 56.250%   2013-10-09 262500 2 25.00 62.500%   2014-08-11 315000 2 25.00 68.750%   2015-07-29 367500 2 25.00 75.000%   2016-07-09 420000 3 12.50 78.125%   2017-06-23 472500 3 12.50 81.250%   2018-05-29 525000 3 12.50 84.375%    <p></p> <p>Based my average hashrate and the historical network hashrate, the plot below shows how many Bitcoins I could have mined if I didn\u2019t denote my computers\u2019 computing power to the World Community Grid but to Bitcoin mining \u2013 14.8 Bitcoins!</p> <p></p> <p>Okay, problem solved.</p> <p>If I\u2019ve really mined these 14.8 Bitcoins, then I\u2019ll probably have a shot at becoming a millionaire, if again I could hold them and time the market perfectly. At Bitcoin\u2019s highest historical price in Australian dollar, 14.8 Bitcoins are roughly 380,505 dollars. Even if I follow the redefined BOINC credit, I still could have mined half of the 14.8 Bitcoins and potentially pocketed 190k dollars.</p> <p>I\u2019ve also participated in more than just World Community Grid, including some famous ones like SETI@Home and Einstein@Home. Below are two certificates of contributed computing power.</p> <p></p> <p></p> <p>So together I\u2019ve put in about 2.28 quintillion, or 2.28E18, FLOPs into these two projects.</p> <p>The funny thing is that I\u2019ve put in only 348 PetaFLOPs into World Community Grid during this entire period, or 0.348 quintillion FLOPs in total.</p> <p>Hence, if my donation of computing power to SETI@Home and Einstein@Home happened at similar time as to WCG, then potentially I could have mined at least 6 times more Bitcoins. Well, I couldn\u2019t imagine what my life would be if I\u2019ve mined 100 Bitcoins, which might be $2.5 million.</p> <p></p>","title":"100 Bitcoins Forgone for Science"},{"location":"posts/accumulator-option-pricing/","tags":["Option Pricing"],"text":"<p>An accumulator is a financial derivative that is sometimes known as \"I kill you later\". This post attempts to explain how it is structured and price it via Monte Carlo simulations in Python.</p>","title":"Accumulator option pricing"},{"location":"posts/accumulator-option-pricing/#1-overview-of-accumulator","tags":["Option Pricing"],"text":"<p>Like all derivatives, there are two parties invovled in an accumulator, the buyer and the seller, both agree on a strike price that is usually at a discount to the prevailing market price of the underlying security at the time of contract origination.</p> <ul> <li>The buyer has the obligation to buy certain amount of the underlying security at the predetermined strike price.</li> <li>The seller has the obligation to sell the specified amount of the underlying security at the strike price to the buyer.</li> </ul> <p>The accumulator is settled periodically throughout its term. At each settlement:</p> <ul> <li>If the market price of the underlying security is above the predetermined knock-out price, the contract is terminated.</li> <li>If the market price of the underlying security is between the knock-out price and the strike price, the buyer \"accumulates\" the underlying security at the strike price.</li> <li>If the market price of the underlying security is below the strike price, the buyer is obligated to buy the underlying security at the strike price at 2 (or more) times of the predetermined amount.</li> </ul>","title":"1. Overview of Accumulator"},{"location":"posts/accumulator-option-pricing/#2-an-example-6-month-accumulator","tags":["Option Pricing"],"text":"<p>Let's make up an example so as to illustrate how it works. </p>","title":"2. An Example 6-month Accumulator"},{"location":"posts/accumulator-option-pricing/#21-month-0","tags":["Option Pricing"],"text":"<p>Suppose that I bought an accumulator from Sherry the seller today, where the underlying security is Transcendental Capital's stock (TSC, hypothetical ticker), currently trading at $100. The strike price is $90 and the knock-out price is $105. The amount of stocks that I can buy is 1,000 in each settlement. The accumulator lasts for 6 months and settles monthly.</p>","title":"2.1. Month 0"},{"location":"posts/accumulator-option-pricing/#22-month-1","tags":["Option Pricing"],"text":"<p>At the end of month 1, the market price of TSC is $102, which is between the strike price ($90) and the knock-out price ($105). I can buy 1,000 shares from Sherry at the strike price of $90 each and make a profit of (\\$102-\\$90)\\times1000=\\$12,000.</p>","title":"2.2. Month 1"},{"location":"posts/accumulator-option-pricing/#23-month-2","tags":["Option Pricing"],"text":"<p>At the end of month 2, the market price of TSC is $95, which is between the strike price ($90) and the knock-out price ($105). I can buy 1,000 shares from Sherry at the strike price of $90 each and make a profit of (\\$95-\\$90)\\times1000=\\$5,000(\\$95-\\$90)\\times1000=\\$5,000.</p>","title":"2.3. Month 2"},{"location":"posts/accumulator-option-pricing/#24-month-3","tags":["Option Pricing"],"text":"<p>At the end of month 3, the market price of TSC is $85, which is below the strike price ($90). I have to buy 2,000 shares from Sherry at the strike price of $90, making a loss of (\\$90-\\$85)\\times2000=\\$10,000(\\$90-\\$85)\\times2000=\\$10,000.</p>","title":"2.4. Month 3"},{"location":"posts/accumulator-option-pricing/#25-month-4","tags":["Option Pricing"],"text":"<p>At the end of month 4, the market price of TSC is $88, which is below the strike price ($90). I have to buy 2,000 shares from Sherry at the strike price of $90, making a loss of (\\$90-\\$88)\\times2000=\\$4,000(\\$90-\\$88)\\times2000=\\$4,000.</p>","title":"2.5. Month 4"},{"location":"posts/accumulator-option-pricing/#26-month-5","tags":["Option Pricing"],"text":"<p>At the end of month 5, the market price of TSC is $106, which is above the knock-out price, so the contract is terminated immediately. I cannot make any profit from Sherry any longer.</p>","title":"2.6. Month 5"},{"location":"posts/accumulator-option-pricing/#3-some-observations","tags":["Option Pricing"],"text":"<p>In the example above:</p> <ul> <li> I've accumulated in total of 6,000 shares of TSC at a cost of $90 per share. This implies that an accumulator is like a forward contract and helps the buyer lock in the cost of acquiring the underlying security (why it's also known as share forward accumulator).</li> <li> As long as the share price is between the strike price and knock-out price, the buyer can accumulate shares at a discount.</li> <li> The 6-month accumulator terminates in 5 months because of the knock out and so the seller's loss is limited.</li> <li> When the share price is below the strike price, the buyer accumulates more shares but at a loss.</li> </ul> <p>All these taken together, we can find that the buyer has: 1. a limited upside potential because the potential gain is capped by the knock-out price and zero when knocked out, and 2. a disproportional (limited) downside in that any loss is amplified by the doubled amount of shares he or she has to purchase from the seller when share price is below the strike.</p> <p>But this is not the full story. Another hidden feature is that while the accumulator is terminated when the share price is above the knock-out price, the contract does not terminate when the buyer is at a loss untill it matures. So, even though the maximum losses of both the buyer and the seller are fixed, but they differ significantly and disproportionately.</p> <p>If so, why would anyone become interested in buying the contract? Potentially it's because the strike is set to be lower at market price, therefore at the beginning the buyers always feel like they are taking advantages. They may also think that once the price increases to above the knock-out level, which might be set to slightly higher than market price, the contract is terminated so they are free of any loss.</p> <p>However, the buyers often underestimate the probability of a price decline and how big the impact it will have on accumulator buyers. The \"I kill you later\" earns its name for a reason.</p>","title":"3. Some Observations"},{"location":"posts/accumulator-option-pricing/#4-some-math","tags":["Option Pricing"],"text":"<p>Let's make some notations.</p> <ul> <li>Strike price is KK</li> <li>Share price at time tt is S_tS_t</li> <li>Knock-out price is K^+K^+</li> <li>The amount of shares to buy is:</li> <li>AA when K&lt;S_t&lt;K^+K&lt;S_t&lt;K^+, and </li> <li>cAcA when S_t&gt;K^+S_t&gt;K^+, where c&gt;1c&gt;1</li> <li>There are NN settlements</li> </ul> <p>So at each settlement, the payoff matrix conditional on the contract not terminated in the previous settlement is:</p>    Share Price Buyer's Payoff Seller's Payoff     S_t&gt;K^+S_t&gt;K^+ 0 0   K\\le S_t\\le K^+K\\le S_t\\le K^+ A(S_t-K)\\ge0A(S_t-K)\\ge0 -A(S_t-K)\\le0-A(S_t-K)\\le0   S_t&lt;KS_t&lt;K c A(S_t-K)&lt;0c A(S_t-K)&lt;0 -cA(S_t-K)&gt;0-cA(S_t-K)&gt;0    <p>However, deriving a closed-end analytical solution is not easy since there are many settlements in the contract and the total payoff is path-dependent (the knock out). There is a conference paper in 2009 discussing the issue and the PDF version is available here.</p>","title":"4. Some Math ..."},{"location":"posts/accumulator-option-pricing/#5-a-simulation-approach","tags":["Option Pricing"],"text":"<p>I am to going to use Monte Carlo simulations to find out the distribution of buyer's payoff.</p>","title":"5. ... A Simulation Approach"},{"location":"posts/accumulator-option-pricing/#51-assumptions","tags":["Option Pricing"],"text":"<p>For simplicity, I'm going to make the following assumptions:</p> <ul> <li>the share price when the contract is signed S_t=\\$100S_t=\\$100.</li> <li>the strike price and the knock-out price are symmetric around $100.<ul> <li>the strike price K=100-kK=100-k</li> <li>the knock-out price K^+=100+kK^+=100+k</li> </ul> </li> <li>the contract lasts for a year with 12 settlements and each month end.</li> <li>the amount of shares to buy in each settlement:<ul> <li>A=1,000A=1,000 if the share price at settlement is between the strike price and the knock-out price. </li> <li>A=2,000A=2,000 if the share prices settlement is below the strike price. </li> </ul> </li> <li>the monthly stock returns follow a normal distribution with mean zero and a standard deviation of \\sigma\\sigma. </li> </ul> <p>Then, there are only two variables: kk and \\sigma\\sigma that I will need to vary!</p>","title":"5.1. Assumptions"},{"location":"posts/accumulator-option-pricing/#52-core-code","tags":["Option Pricing"],"text":"<p>The simulation code I write below leverages Numba to speed up the calculation.</p> <p>For 1 million simulations per pair of (k, \\sigma)(k, \\sigma), it takes about 2 seconds on my laptop with JIT and almost 1 minute without it.</p> <pre><code>import random\nimport numpy as np\nfrom collections import OrderedDict\nfrom numba import jitclass, int32, float32\n\n\n@jitclass(OrderedDict({\n    'times': int32,\n    'strike_price': float32,\n    'knock_out_price': float32,\n    'volatility': float32\n}))\nclass FastSimulation:\n\n    def __init__(self, times, strike_price, knock_out_price, volatility):\n        self.times = times\n        self.strike_price = strike_price\n        self.knock_out_price = knock_out_price\n        self.volatility = volatility\n\n    def run(self):\n        np.random.seed(1)\n        buyer_payoffs = []\n        for i in range(self.times):\n            # generate 12 monthly returns from a normal distribution\n            # written this way as size parameter is not supported by numba\n            returns = [np.random.normal(\n                loc=0, scale=self.volatility)/100 + 1 for _ in range(12)]\n            # convert returns to a price array\n            prices = np.asarray(returns).cumprod() * 100\n            payoff = 0\n            for price in prices:\n                # the accumulator is terminated immediately\n                if price &gt; self.knock_out_price:\n                    break\n                payoff += self.buyer_payoff(price)\n            buyer_payoffs.append(payoff)\n        return buyer_payoffs\n\n    def buyer_payoff(self, share_price):\n        \"Buyer payoff conditional on the accumulator not terminated\"\n        if share_price &gt; self.knock_out_price:\n            return 0\n        payoff = 1000 * (share_price - self.strike_price)\n        if self.strike_price &lt;= share_price &lt;= self.knock_out_price:\n            return payoff\n        else:\n            return payoff * 2\n</code></pre>","title":"5.2. Core Code"},{"location":"posts/accumulator-option-pricing/#53-results","tags":["Option Pricing"],"text":"<p>Numbers are boring. So here I put two plots showing the distribution of the buyer's payoffs.</p>","title":"5.3. Results"},{"location":"posts/accumulator-option-pricing/#531-k5k5-and-vin-15vin-15","tags":["Option Pricing"],"text":"<p></p>","title":"5.3.1. k=5k=5 and v\\in [1..5]v\\in [1..5]"},{"location":"posts/accumulator-option-pricing/#532-k10k10-and-vin-15vin-15","tags":["Option Pricing"],"text":"<p></p>","title":"5.3.2. k=10k=10 and v\\in [1..5]v\\in [1..5]"},{"location":"posts/accumulator-option-pricing/#6-discussion","tags":["Option Pricing"],"text":"<p>Apparently, the accumulator is a very interesting and sometimes evil derivative. From the plots above we can notice several things:</p> <ol> <li>When volatility is low, an accumulator gives the buyer an opportunity to accumulate shares at a discount and therefore positive payoffs.</li> <li>When volatility is high, the buyer's payoff distribution becomes increasingly negatively skewed.</li> <li>The maximum potential payoff is capped and decreasing in volatility but VaR is increasing in the volatility.</li> <li>......</li> </ol> <p>Hence, as a buyer of an accumulator, you win small with low volatility but lose big and huge with high volatility. I don't think any rational investor would like to take the long position. However, we do find exceptions, like CITIC Limited lost HK$15 billion in accumulators back in 2008.</p>","title":"6. Discussion"},{"location":"posts/adding-another-factor-to-principal-agent-model/","text":"<p>This post is adapted from the online appendix of my JBF paper \"organization capital and executive performance incentives\".</p>  <p>In a traditional principal-agent model, firm output is a function of the agent's effort and the principal observes only the output not agent's effort. The principal carefully designs the agent's compensation package, especially the sensitivity of the agent's pay to firm output, to maximize the firm value. Now, what if we add another factor to the relationship between firm output and agent's effort? How would the optimal pay sensitivity change?</p> <p>My earlier paper studied this issue by assuming such a factor, organization capital, that substitutes agent's effort in improving firm output. I find that if firm output is a function of two substituting factors (of which one is agent's effort), the optimal sensitivity of agent's pay to firm output can be both higher or lower, depending on the principal's choice.</p> <p>To yield this two-way prediction, let's see a simple extension to the standard principal-agent model following Holmstrom and Milgrom (1987), where the principal hires an agent (CEO) to run the firm. We added organization capital (OC) as an additional determinant of firm outcomes, but in fact we can assume any factor, e.g., intellectual property, IT infrastructure, etc., that either strengthens or weakens the relation between firm output and executive effort.</p> <p>The production function is given by V(a,o)=f(a,o)+\\varepsilon, where aa is the effort by the agent, oo is the firm\u2019s organization capital, and \\varepsilon\\varepsilon is random noise.</p> <ul> <li>f(a,o)f(a,o) is an increasing function in both aa and oo. The substitutability of OC and executive effort implies that \\frac{\\partial^2}{\\partial o \\partial a}f(a,o)&lt;0\\frac{\\partial^2}{\\partial o \\partial a}f(a,o)&lt;0, i.e., OC reduces the marginal effect of any agent action on firm outcomes.</li> <li>Without loss of generality, we assume f(a,o)=a+(1-a)of(a,o)=a+(1-a)o, where both aa and oo follow a continuous uniform distribution from 0 to 1.</li> </ul> <p>The agent is paid a wage c(V)c(V) and has reservation utility of \\underline U\\underline U. His objective function is given by E\\left[U\\right]=E\\left[u\\left(v\\left(c\\right)-g\\left(a\\right)\\right)\\right]E\\left[U\\right]=E\\left[u\\left(v\\left(c\\right)-g\\left(a\\right)\\right)\\right].</p> <ul> <li>The increasing and weakly convex function gg represents the cost of effort.</li> <li> <p>The function uu represents his utility function and vv represents his felicity function (i.e., his utility from cash), both increasing and weakly concave.</p> </li> <li> <p>The functions gg, uu and vv are all twice continuously differentiable.</p> </li> </ul> <p>The risk-neutral principal chooses the effort level aa and contract cc to maximize the expected firm value minus the wage paid to the agent, $$     \\max_{c(\\cdot),a} E\\left[V\\left(a,o\\right)-c\\left(V\\left(a,o\\right)\\right)\\right] $$ subject to the individual rationality or participation constraint (IR) and incentive compatibility constraint (IC) as follows: $$     E\\left[u\\left(v\\left(c\\left(V\\left(a,o\\right)\\right)\\right)-g\\left(a\\right)\\right)\\right] \\ge \\underline{U}     \\     a \\in \\arg\\max_{\\hat{a}}E\\left[u\\left(v\\left(c\\left(V\\left(\\hat{a},o\\right)\\right)\\right)-g\\left(\\hat{a}\\right)\\right)\\right] $$</p> <p>We first consider the case where the optimal effort is determined endogenously. Under the Holmstrom and Milgrom (1987) framework, the following assumptions are made:</p> <ol> <li>an exponential utility function of the agent, u(x)=-e^{-\\eta x}u(x)=-e^{-\\eta x}, where \\eta\\eta is the coefficient of constant absolute risk aversion,</li> <li>v(c)=cv(c)=c, such that cost of effort is pecuniary and can be viewed as a subtraction to cash pay, </li> <li>a normal noise, \\varepsilon \\sim N(0,\\sigma^2)\\varepsilon \\sim N(0,\\sigma^2), and </li> <li>that the agent chooses his effort continuously in a multiperiod model and the optimal contract is linear, i.e., c=\\phi+\\theta Vc=\\phi+\\theta V, where \\phi\\phi is the fixed wage and \\theta\\theta represents the proportion of firm outcomes shared with the agent via compensation (i.e., \\theta\\theta represents the agent\u2019s pay-for-performance sensitivity).</li> </ol> <p>Further, Holmstrom and Milgrom (1987) show that the problem is equivalent to a single-period static problem under these assumptions. For simplicity, we also assume a quadratic cost of effort, g(a)=\\frac{1}{2}ga^2g(a)=\\frac{1}{2}ga^2, so that the principal\u2019s optimization problem becomes:</p>       \\max_{\\phi,\\theta,a^*} E\\left[V-c\\right]       \\max_{\\phi,\\theta,a^*} E\\left[V-c\\right]   <p>subject to</p>       E\\left[-e^{-\\eta\\left(c-\\frac{1}{2}ga^{*2}\\right)}\\right] \\ge \\underline{U}     \\\\     a^* \\in \\arg\\max_{\\hat{a}} E\\left[-e^{-\\eta\\left(c-\\frac{1}{2}g\\hat{a}^2\\right)}\\right]       E\\left[-e^{-\\eta\\left(c-\\frac{1}{2}ga^{*2}\\right)}\\right] \\ge \\underline{U}     \\\\     a^* \\in \\arg\\max_{\\hat{a}} E\\left[-e^{-\\eta\\left(c-\\frac{1}{2}g\\hat{a}^2\\right)}\\right]   <p>Substituting in c=\\phi+\\theta Vc=\\phi+\\theta V and V(a,o)=f(a,o)+\\varepsilonV(a,o)=f(a,o)+\\varepsilon, maximizing the agent\u2019s (negative exponential) utility function is equivalent to maximizing \\phi+\\theta f(a,o)-\\frac{1}{2}ga^2-\\frac{1}{2}\\eta \\theta^2 \\sigma^2\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2-\\frac{1}{2}\\eta \\theta^2 \\sigma^2. </p> <p>Since f(a,o)=a+(1-a)of(a,o)=a+(1-a)o, the first-order condition of the agent\u2019s objective function with respect to a is given by a^*=\\theta(1-o)\u2044ga^*=\\theta(1-o)\u2044g, which implies his effort choice is decreasing in the cost of effort gg, decreasing in the firm\u2019s organization capital oo, and increasing in the pay-for-performance sensitivity \\theta\\theta.</p> <p>Moreover, his chosen effort is independent of the fixed wage \\phi\\phi, so that the principal can adjust the fixed pay to satisfy his participation constraint without affecting the incentives. Substituting a^*=\\theta(1-o)\u2044ga^*=\\theta(1-o)\u2044g into the principal\u2019s objective function and setting the participation constraint to bind, the optimal level of pay-for-performance sensitivity is given by:</p>       \\theta = \\frac{1}{1+\\eta g \\frac{\\sigma^2}{(1-o)^2}}       \\theta = \\frac{1}{1+\\eta g \\frac{\\sigma^2}{(1-o)^2}}    <p>Note</p> <p>This optimal level of pay-for-performance sensitivity is derived as follows. Substituting c=\\phi + \\theta Vc=\\phi + \\theta V and V(a,o)=f(a,o)+\\varepsilonV(a,o)=f(a,o)+\\varepsilon into the agent's objective function of E\\left[U\\right]=E\\left[u\\left(v(c)-g(a)\\right)\\right]E\\left[U\\right]=E\\left[u\\left(v(c)-g(a)\\right)\\right], where u(x)=-e^{-\\eta x}u(x)=-e^{-\\eta x}, v(c)=cv(c)=c, and \\varepsilon \\sim N(0,\\sigma^2)\\varepsilon \\sim N(0,\\sigma^2), we obtain:</p>       E\\left[U\\right] = E\\left[e^{-\\eta \\left(\\phi+\\theta f(a,o)+\\theta\\varepsilon-\\frac{1}{2}ga^2\\right)}\\right] \\\\     = -E\\left[e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2\\right)}\\right] \\times E\\left[e^{-\\eta \\theta \\varepsilon}\\right]\\\\     = -e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2\\right)} \\times e^{\\frac{\\eta^2 \\theta^2 \\sigma^2}{2}} \\\\     = -e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2-\\frac{1}{2}\\eta \\theta^2 \\sigma^2\\right)}       E\\left[U\\right] = E\\left[e^{-\\eta \\left(\\phi+\\theta f(a,o)+\\theta\\varepsilon-\\frac{1}{2}ga^2\\right)}\\right] \\\\     = -E\\left[e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2\\right)}\\right] \\times E\\left[e^{-\\eta \\theta \\varepsilon}\\right]\\\\     = -e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2\\right)} \\times e^{\\frac{\\eta^2 \\theta^2 \\sigma^2}{2}} \\\\     = -e^{-\\eta \\left(\\phi+\\theta f(a,o)-\\frac{1}{2}ga^2-\\frac{1}{2}\\eta \\theta^2 \\sigma^2\\right)}   <p>The first-order condition (FOC) of the agent with respect to aa is given by</p>       \\frac{\\partial}{\\partial a}\\left(\\phi+\\theta f(a^*,o)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right)=0       \\frac{\\partial}{\\partial a}\\left(\\phi+\\theta f(a^*,o)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right)=0   <p>Since we assume f(a,o)=a+(1-a)of(a,o)=a+(1-a)o, this yields the agent's FOC:</p>       a^*=\\theta(1-o)/g       a^*=\\theta(1-o)/g   <p>Setting the participation constraint to bind, we have</p>       E\\left[U\\right] = -e^{-\\eta\\left(\\phi+\\theta\\left(a^*+(1-a^*)o\\right)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right)} = \\underline{U}       E\\left[U\\right] = -e^{-\\eta\\left(\\phi+\\theta\\left(a^*+(1-a^*)o\\right)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right)} = \\underline{U}   <p>The above equation implies:</p>       \\phi + \\theta\\left(a^*+\\left(1-a^*\\right)o\\right)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2=w       \\phi + \\theta\\left(a^*+\\left(1-a^*\\right)o\\right)-\\frac{1}{2}ga^{*2}-\\frac{1}{2}\\eta\\theta^2\\sigma^2=w   <p>where w\\equiv -\\ln(-\\underline{U})/\\etaw\\equiv -\\ln(-\\underline{U})/\\eta is a constant determined by the agent's reservation utility and his coefficient of constant absolute risk aversion. Substituting in a^*=\\theta(1-o)/ga^*=\\theta(1-o)/g, we yield</p>       E\\left[c\\right] = \\phi + \\theta E\\left[V\\right] \\\\     = w+\\frac{\\theta^2(1-o)^2}{2g} +\\frac{1}{2}\\eta\\theta^2\\sigma^2       E\\left[c\\right] = \\phi + \\theta E\\left[V\\right] \\\\     = w+\\frac{\\theta^2(1-o)^2}{2g} +\\frac{1}{2}\\eta\\theta^2\\sigma^2   <p>Thus, by substituting a^*=\\theta(1-o)/ga^*=\\theta(1-o)/g into the principal\u2019s objective function E\\left[V-c\\right]E\\left[V-c\\right], we yield</p>       a^*+(1-a^*)o-\\left[w+\\frac{\\theta^2(1-o)^2}{2g}+\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right] \\\\     =\\frac{\\theta}{g}(1-o)^2+o-\\left[w+\\frac{\\theta^2(1-o)^2}{2g}+\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right]       a^*+(1-a^*)o-\\left[w+\\frac{\\theta^2(1-o)^2}{2g}+\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right] \\\\     =\\frac{\\theta}{g}(1-o)^2+o-\\left[w+\\frac{\\theta^2(1-o)^2}{2g}+\\frac{1}{2}\\eta\\theta^2\\sigma^2\\right]   <p>The principal's FOC with respect to \\theta\\theta yields:</p>       \\theta = \\frac{1}{1+\\eta g \\frac{\\sigma^2}{(1-o)^2}}       \\theta = \\frac{1}{1+\\eta g \\frac{\\sigma^2}{(1-o)^2}}    <p>Other things equal, we can see that the optimal pay-for-performance sensitivity \\theta\\theta is decreasing in the firm\u2019s organization capital oo. Specifically, this substitution effect is from the fact that OC reduces the marginal effect of executive effort on firm outcomes and thus reduces the optimal effort level endogenously.</p> <p>On the other hand, fixing a^*a^*, the required pay-for-performance sensitivity is \\theta=(a^* g)/(1-o)\\theta=(a^* g)/(1-o), which is increasing in organization capital oo. Thus, to elicit any given level of effort, the incentive compensation must be more high-powered (\"fixed target action\" as in Edmans and Gabaix (2016).</p> <p>The relation between OC and executive pay-for-performance sensitivity depends critically on the optimal level of effort the principal wants to implement:</p> <ul> <li>If the principal wants to implement a fixed target action (e.g., a=1a=1 in our case, or to induce full effort in general), the optimal \\theta\\theta is increasing in the firm\u2019s organization capital.</li> <li>If the principal trades off the benefits and costs of high effort, the optimal \\theta\\theta is decreasing in the firm\u2019s organization capital.</li> </ul> <p>Therefore, the model offers two empirical predictions. On the one hand, high OC firms may offer higher pay-for-performance sensitivity to induce executive effort. On the other hand, pay-for-performance sensitivity may be reduced in high OC firms as a result of efficiency gains from the substitution of OC for executive effort.</p> <p>Now, coming back at the question at the beginning, adding another factor to the principal-agent model may cause the optimal pay structure to change in either direction, even if such factor has a directional impact on the relation between firm output and agent's effort. In our case, such factor reduces the marginal effect of agent's effort on firm output. But one can easily find many other factors that may increase the marginal effect of agent's effort and yield similar predictions.</p> <p>Perhaps, what's also interesting is that, if we know the directional effect of a factor while observing both pay-for-performance sensitivity and the level of such factor, we may be able to infer whether the principal elicits full executive effort at all costs. Paired with firm performance, could this be some indicators of governance or board ability? Seems like some future research questions.</p>","title":"Adding Another Factor to Principal-Agent Model"},{"location":"posts/beta-unlevered-and-levered/","text":"<p>Beta is a measure of market risk.</p>","title":"Beta - Unlevered and Levered"},{"location":"posts/beta-unlevered-and-levered/#unlevered-firm-u","text":"<p>If a firm has no debt, it's all equity-financed and thus its equity's beta \\beta_{E} equals its asset's beta \\beta_{A}\\beta_{A}. This beta is also the unlevered beta, \\beta_{\\text{unlevered}}\\beta_{\\text{unlevered}}, since it's unaffected by leverage. The unlevered beta measures the market risk exposure of the firm's shareholders. Let's call this firm uu, Hence, we have:</p>  \\begin{equation} \\beta_{\\text{unlevered}}=\\beta_E^u=\\beta_A^u \\end{equation} \\begin{equation} \\beta_{\\text{unlevered}}=\\beta_E^u=\\beta_A^u \\end{equation}  <p>This equality says that in an unlevered firm, the unlevered beta equals its equity beta and its asset beta.</p>","title":"Unlevered Firm u"},{"location":"posts/beta-unlevered-and-levered/#levered-firm-l","text":"<p>If the same firm is partly financed by debt, let's call it firm ll. The asset of the levered firm ll is financed by both equity and debt, and hence the asset's market risk is from both equity and debt. The asset's beta is a weighted average of its equity beta and debt beta.</p>  \\begin{equation} \\beta_A^l = \\frac{E}{E+D(1-t)} \\beta_E^l + \\frac{D(1-t)}{E+D(1-t)} \\beta_D^l \\end{equation} \\begin{equation} \\beta_A^l = \\frac{E}{E+D(1-t)} \\beta_E^l + \\frac{D(1-t)}{E+D(1-t)} \\beta_D^l \\end{equation}   <p>\\beta_A^l\\beta_A^l measures the change in the return on a portfolio of all firm  ll's securities (debt and equity) for each additional one percent change in  the market return.</p>  <p>This part is not very hard to understand. The beta of a portfolio is the weighted average beta of its constituents. If you believe that debt beta is zero since the value of debt may not be affected by the equity market, then \\beta_D^l=0\\beta_D^l=0 and the equation (2) can be simplified to:</p>   \\begin{align} \\beta_A^l &amp;= \\frac{E}{E+D(1-t)} \\beta_E^l \\newline     &amp;= \\frac{1}{1+\\frac{D}{E}(1-t)} \\beta_E^l \\end{align}   \\begin{align} \\beta_A^l &amp;= \\frac{E}{E+D(1-t)} \\beta_E^l \\newline     &amp;= \\frac{1}{1+\\frac{D}{E}(1-t)} \\beta_E^l \\end{align}   <p>However, this firm's shareholders are now more exposed to the market risk than before, because leverage increases the variation in the payoff to shareholders. This means the equity's beta of this levered firm is higher than the equity's beta of the unlevered firm, i.e. \\beta_E^l&gt;\\beta_E^u\\beta_E^l&gt;\\beta_E^u.</p> <p>Note that, the levered beta \\beta_{\\text{levered}}\\beta_{\\text{levered}} that we talk about refers to \\beta_E^l\\beta_E^l, which is the equity beta of the levered firm ll.</p>","title":"Levered Firm l"},{"location":"posts/beta-unlevered-and-levered/#unlevered-vs-levered","text":"<p>On the other hand, firm uu and firm ll differ only in capital structure whilst both have the same asset. Let's say we have a portfolio of firm uu's asset and the other portfolio of firm ll's asset, then these two portfolios should have the same expected return and market risk exposure.2 This means the two portfolios have the same beta, implying:</p>  \\begin{equation}\\beta_A^u = \\beta_A^l \\end{equation} \\begin{equation}\\beta_A^u = \\beta_A^l \\end{equation}  <p>If we substitue in the definition of unlevered and levered beta (equation (1) and (4)):</p>   \\begin{equation} \\beta_{\\text{unlevered}} =  \\frac{1}{1+\\frac{D}{E}(1-t)} \\beta_{\\text{levered}} \\end{equation}   \\begin{equation} \\beta_{\\text{unlevered}} =  \\frac{1}{1+\\frac{D}{E}(1-t)} \\beta_{\\text{levered}} \\end{equation}   <p>or</p>   \\begin{equation} \\beta_{\\text{levered}} =  \\left( 1+\\frac{D}{E}(1-t) \\right) \\beta_{\\text{unlevered}} \\end{equation}   \\begin{equation} \\beta_{\\text{levered}} =  \\left( 1+\\frac{D}{E}(1-t) \\right) \\beta_{\\text{unlevered}} \\end{equation}   <p>This is the formula that we use to lever and unlever beta.1 </p>","title":"Unlevered vs Levered"},{"location":"posts/beta-unlevered-and-levered/#further-clarification","text":"<p>The equity beta of a firm with debts is levered. To remove the impact of leverage on shareholders' market risk exposure, we need to unlever this beta in order to get the unlevered beta. This unlevered beta is also called the asset beta.</p> <p>Note that the asset beta is a syncronym for unlevered beta. It is not, however, the asset's beta \\beta_A^l\\beta_A^l when the firm is leveraged as in equation (2) to (4). This convention is confusing indeed, so throughout this post, I'm using asset's beta to refer to the beta of a portfolio of all securities (debt and equity) of the levered firm.</p>","title":"Further Clarification"},{"location":"posts/beta-unlevered-and-levered/#notations","text":"<ul> <li>\\beta_E^u\\beta_E^u: the equity's beta of the unlevered firm</li> <li>\\beta_A^u\\beta_A^u: the asset's beta of the unlevered firm</li> <li>\\beta_E^l\\beta_E^l: the equity's beta of the levered firm</li> <li>\\beta_D^l\\beta_D^l: the debt's beta of the levered firm</li> <li>\\beta_A^l\\beta_A^l: the asset's beta of the levered firm</li> <li>DD: the size of the firm's debt</li> <li>EE: the size of the firm's equity</li> <li>tt: the tax rate</li> <li>\\beta_{\\text{unleverd}}\\beta_{\\text{unleverd}}: unlevered beta, the equity (asset) beta of the   unlevered version of the firm</li> <li>\\beta_{\\text{leverd}}\\beta_{\\text{leverd}}: levered beta, the equity beta of the levered   version of the firm</li> </ul>   <ol> <li> <p>This eq.(7) is also named Hamada Equation, where we assumed a zero debt beta. It draws on the Modigliani-Miller theorem on capital structure, and appeared in Prof. Robert Hamada's paper \"The Effect of the Firm's Capital Structure on the Systematic Risk of Common Stocks\" in the Journal of Finance in 1972.\u00a0\u21a9</p> </li> <li> <p>Modigliani-Miller theorem states that the capital structure should not affect a firm's value.\u00a0\u21a9</p> </li> </ol>","title":"Notations"},{"location":"posts/bitcoin-address-generator-in-obfuscated-python/","text":"<p>Never underestimate what programmers can do.</p> <p>The code below shows a fully-functioning Bitcoin address generator in obfuscated Python (2.5-2.7), which I saw in an interesting article posted in 2013.</p> <pre><code>_                   =r\"\"\"A(W/2,*M(3*G\n               *G*V(2*J%P),G,J,G)+((M((J-T\n            )*V((G-S)%P),S,T,G)if(S@(G,J))if(\n         W%2@(S,T)))if(W@(S,T);H=2**256;import&amp;h\n       ashlib&amp;as&amp;h,os,re,bi    nascii&amp;as&amp;k;J$:int(\n     k.b2a_hex(W),16);C$:C    (W/    58)+[W%58]if(W@\n    [];X=h.new(\"rip           em    d160\");Y$:h.sha25\n   6(W).digest();I$                 d=32:I(W/256,d-1)+\n  chr(W%256)if(d&gt;0@\"\";                  U$:J(k.a2b_base\n 64(W));f=J(os.urando       m(64))        %(H-U(\"AUVRIxl\nQt1/EQC2hcy/JvsA=\"))+      1;M$Q,R,G       :((W*W-Q-G)%P,\n(W*(G+2*Q-W*W)-R)%P)       ;P=H-2**       32-977;V$Q=P,L=\n1,O=0:V(Q%W,W,O-Q/W*                      L,L)if(W@O%P;S,\nT=A(f,U(\"eb5mfvncu6                    xVoGKVzocLBwKb/Nst\nzijZWfKBWxb4F5g=\"),      U(\"SDra         dyajxGVdpPv8DhEI\nqP0XtEimhVQZnEfQj/       sQ1Lg=\"),        0,0);F$:\"1\"+F(W\n [1:])if(W[:1           ]==\"\\0\"@\"\"        .join(map(B,C(\n  J(W))));K$:               F(W          +Y(Y(W))[:4]);\n   X.update(Y(\"\\4\"+                     I(S)+I(T)));B$\n    :re.sub(\"[0OIl    _]|            [^\\\\w]\",\"\",\"\".jo\n     in(map(chr,ra    nge    (123))))[W];print\"Addre\n       ss:\",K(\"\\0\"+X.dig    est())+\"\\nPrivkey:\",K(\n         \"\\x80\"+I(f))\"\"\";exec(reduce(lambda W,X:\n            W.replace(*X),zip(\" \\n&amp;$@\",[\"\",\"\",\n               \" \",\"=lambda W,\",\")else \"])\n                    ,\"A$G,J,S,T:\"+_))\n</code></pre> <p>I\u2019ve tested it on Python 2.7 on Ubuntu. Working like a charm.</p> <p></p>  <p>Warning</p> <p>Don't use this address! The private key is not private!</p>","title":"Bitcoin Address Generator in Obfuscated Python"},{"location":"posts/bloomberg-bquant/","text":"<p>Bloomberg is developing a new function in the Terminal, called BQuant, BQNT, under the Bloomberg Anywhere license. I happen to be able to test it thanks to a fund manager and find it could be a future way of using Bloomberg Terminal.","title":"Bloomberg BQuant (BQNT)"},{"location":"posts/bloomberg-bquant/#background","text":"<p>Bloomberg recently made JupyterLab available inside the Terminal and invited partners to test it out. This function is named BQuant, or BQNT&lt;GO&gt;, which is still under heavy development, but the idea is just great. Jupyter notebooks inside Bloomberg Terminal! Just before this news, I was helping a fund manager in writing some alert programs that do some analysis on equity market and then send email notifications, which didn\u2019t go well because first it is very easy to breach the data limit using Bloomberg API (blpapi) and second I wasn\u2019t very comfortable about the presentation of analysis results. I was using poor HTML code in emails and didn\u2019t find a convenient way to insert plots and figures. Besides, I was also writing some back testing code to evaluate potential trading strategies. But still there\u2019s a concern as I won\u2019t be working there full time and they probably won\u2019t have a permanent programmer, so if they want to alter parameters a little bit it\u2019ll be a problem.</p> <p>But things happen, with BQNT or more specifically the Jupyter notebook, I can make an interactive UI-based application without worrying about the data limit issue, as they also provide a new data retrieval interface, BQL, Bloomberg Query Language. In the past, pulling data through blpapi is basically retrieving data from the Terminal. But BQL, something like SQL, is to submit the query request to Bloomberg\u2019s server and get the data directly from server, which also enables basic calculations so as to further reduce the size of data being pulled out. Then, BQNT comes with pre-installed bqplot and some wrappers of libraries like ipwidgets, which makes visualization much easier and interactive. As BQNT is a customized JupyterLab, output cells can be maximized and code hided. The result is just like a single-page application.</p> <p></p> <p>The tearsheet above shows some basic features of BQNT, and of course there are more. There\u2019s a gallery in the Terminal with several demos showing what BQNT can make, including portfolio performance report, security filtering, trading strategy back test, etc., quite inspiring.</p> <p>With a quick play, I was able to write a multi-security back test of William %R based strategy with trailing stop. All input parameters can be varied using sliders, dropdowns, calendars and etc. There is also an autocomplete security selection widget to assist you in defining the universe. Plots and tables can be aligned nicely using HBox and VBox\u2026 So, I\u2019m impressed, really.</p> <p>I can foresee that in the future, users of Bloomberg Terminal can have BQNT powered applications tailored to their needs. For example, I want to know the stock volatility and price plot together with some commodity futures orderbook info. BQNT may give you the app. But of course, I\u2019ve only a rough guess and there could be many possibles and impossibles ahead of BQNT. I\u2019m a big fan, though.</p>","title":"Background"},{"location":"posts/bloomberg-bquant/#my-work","text":"","title":"My Work"},{"location":"posts/bloomberg-bquant/#bql-for-data-retrieval","text":"<p>We know there\u2019s a blpapi available already. Using this API one can pull data from a Terminal to Excel, Python, etc. But there is a limit on the frequency or total queries allowed in a certain period, which however isn\u2019t clear. As Bloomberg doesn\u2019t allow local storage of its data, if we need to retrieve a sizeable data too many times, there will be an issue.</p> <p>The good thing about BQNT is that it comes with a new query system \u2013 so called BQL. It allows simple calculations done on the server side so as to reduce the size of data transferred. And, people in Bloomberg said, by using BQL we are not very likely to face any data limit issue again. I haven\u2019t done much stress tests so I can\u2019t tell whether there is still a limit or not.</p>","title":"BQL for Data Retrieval"},{"location":"posts/bloomberg-bquant/#some-quick-examples","text":"<p>Get all component stocks of an index:</p> <pre><code>import bql\nbq = bql.Service()\nsecurities = bq.univ.members('AS31 Index')\n</code></pre> <p>Get OHLC data of all component stocks:</p> <pre><code>from bql.util import get_time_series\nstart_date = '2017-01-01'\nend_date = '2018-01-01'\ndata = get_time_series(securities, ['PX_LAST', 'PX_OPEN', 'PX_HIGH', 'PX_LOW'], start_date, end_date)\n</code></pre> <p>If I want to know the industry sector of these stock, all I need is:</p> <pre><code>req = bql.Request(securities, bq.data.industry_sector())\ndata_industry = bql.combined_df(bq.execute(req))\n</code></pre> <p>The returned data is a <code>pandas.DataFrame</code>, which is just awesome!</p>","title":"Some Quick Examples"},{"location":"posts/bloomberg-bquant/#customised-jupyterlab","text":"<p>Jupyter Notebook has always been a favourite environment in data science. No need to say much. A JupyterLab inside Bloomberg Terminal together with BQL, basically the core idea of BQNT, is no doubt fantastic. For quants who need to do a lot of testings on trading ideas, filtering of securities, etc., this integrated environment is absolutely a good place to sort everything out. Moreover, files in BQNT are synced under a BBA license, you can easily pick up your work from any Terminal. In our meeting today, the size of this free cloud storage is said to be about 250MB but may be upgraded.</p> <p>For fund managers or traders who want only a ready-to-use application, they can have some programmers to make one for them. The BQNT team kindly demonstrated a beta feature, where a \u2018consumer view\u2019 can be shared to others, which hides all Jupyter Notebook related parts and is really the final output alone \u2014 just like the Calculator on Windows.</p>","title":"Customised JupyterLab"},{"location":"posts/bloomberg-bquant/#the-r-backtesting-app","text":"<p>This App I wrote replicates BT&lt;GO&gt; in its back testing outputs, but comes with more flexibility such as trailing stop loss, which isn\u2019t available in BT&lt;GO&gt;. It serves as a demo of BQNT powered application, validating current beta.</p> <p>The objectives of the app are: </p> <ul> <li>to perform %R strategy on a single security as well as on all components of an index;</li> <li>to provide both quantitative and qualitative back testing results; </li> <li>be friendly to any user with zero programming knowledge.</li> </ul>","title":"The %R Backtesting App"},{"location":"posts/bloomberg-bquant/#main-ui","text":"<p>The main UI provides a short description of the trading strategy under back test, followed by a control panel where we can specify benchmark, underlying, time range, % parameters as well as trailing stop loss percentage. I also put a progress bar and status bar below for more immediate feedback.</p> <p></p>","title":"Main UI"},{"location":"posts/bloomberg-bquant/#outputs","text":"<p>If the underlying selected is a single security, e.g. CBA AU Equity, the simple back test output is something like below. An <code>InteractiveLinePlot</code> linked with a subplot to show equity evolution in selection; a <code>LinePlot</code> for the price series of the security with markers for enters and exits; and a <code>LinePlot</code> for the %R indicator.</p> <p></p> <p>If the underlying selected is an index, e.g. AS31 Index, the back test is performed on each individual component of the index and results are presented below. A <code>KDEPlot</code> shows the distribution of total return, max return and min return, followed by a <code>ToggleButtons</code> to show All, Positive only and Negative only. Equity Return by industry sector and the benchmark return are sorted and plotted below.</p> <p></p> <p>Then there is the detailed <code>DataGrid</code> for all calculated metrics of all securities and of each industry sectors, just like the output in BT&lt;GO&gt;. Results can be exported to a spreadsheet which will be conveniently stored in the BQNT platform, or the \u2018cloud\u2019 of size 250MB in total. A qualitative summary of this particular back test is provided in the end.</p> <p></p> <p>This App is by no means a finished work. I basically tried to mix in as many different things as possible. The end product should be one such that provides a condensed and conclusive opinion after each run, considering that its users may be those fund managers who do not want to get their hands dirty.</p>","title":"Outputs"},{"location":"posts/bloomberg-bquant/#other-thoughts","text":"<p>In my chat with Bloomberg BQNT team, I visioned BQNT powered apps may be the future way of using Bloomberg. For one, with more internal integration worked out, like the current one with PORT&lt;GO&gt;, surely users can use these UI-based apps to get jobs done. The good thing is that it can put everything you need together in one place, and only those you need. Once consumer view is rolled out, this will be more evident. They also are developing a scheduling module which will run Notebooks automatically, although at an additional cost.</p> <p>Another thing I suggested is a marketplace for those BQNT powered apps. Say, I\u2019ve developed a market analysis application on BQNT, maybe I can put it for sale on the marketplace so someone else won\u2019t need to reinvent the wheel. It can also foster a community around BQNT, if any. The only downside is that BQNT is accessible only under BBA licence, which isn\u2019t cheap. Individual programmers / quants may not be able to afford it, and those in big institutions may not have the time and right to build and sell apps on it. This kinda sucks.</p> <p>I can see the huge potential of BQNT, which if operates well can be the new way of using Bloomberg Terminal \u2014 the learning curve of Terminal is really too steep for many current and potential users, and they don\u2019t get very much out of it. But, if there are many ready-to-use UI-based applications for their customised needs, things definitely will be better. Unfortunately, BQNT is not open-source, and the access to it is very limited (BBA licence), I don\u2019t believe there will be an active community and hence a marketplace of a variety of apps.</p>","title":"Other Thoughts"},{"location":"posts/call-option-value-from-two-approaches/","text":"<p>Suppose today the stock price is S and in one year time, the stock price could be either S_1S_1 or S_2S_2. You hold an European call option on this stock with an exercise price of X=SX=S, where S_1&lt;X&lt;S_2S_1&lt;X&lt;S_2 for simplicity. So you'll exercise the call when the stock price turns out to be S_2S_2 and leave it unexercised if S_1S_1.</p>","title":"Call Option Value from Two Approaches"},{"location":"posts/call-option-value-from-two-approaches/#1-replicating-portfolio-approach","text":"Case 1 Case 2     Stock Price S_1S_1 S_2S_2   Option: 1 Call of cost cc     Exercise? No Yes   Payoff (to replicate) 0 S_2-XS_2-X   Stock: \\delta\\delta shares of cost \\delta S\\delta S     Payoff \\delta S_1\\delta S_1 \\delta S_2\\delta S_2   Borrowing PV(K)     Repay K K    <p>So we have:</p>   \\begin{equation} \\delta S_1-K=0 \\end{equation}   \\begin{equation} \\delta S_1-K=0 \\end{equation}     \\begin{equation} \\delta S_2 -K = S_2-X \\end{equation}   \\begin{equation} \\delta S_2 -K = S_2-X \\end{equation}   <p>Therefore, the call option value is given by the difference between the cost of \\delta\\delta units of shares and the amount of borrowing:</p>   \\begin{align}  c_{REP} &amp;= \\delta S - PV(K) \\newline   &amp;= \\delta S - Ke^{-r_f} \\newline   &amp;= \\delta S - \\delta S_1e^{-r_f} \\end{align}   \\begin{align}  c_{REP} &amp;= \\delta S - PV(K) \\newline   &amp;= \\delta S - Ke^{-r_f} \\newline   &amp;= \\delta S - \\delta S_1e^{-r_f} \\end{align}   <p>When \\delta\\delta is defined as \\frac{(S_2-X)-0}{S_2-S_1}\\frac{(S_2-X)-0}{S_2-S_1} as in the textbook (at introductory level),</p>   \\begin{equation}   c_{REP}= \\frac{S_2-X}{S_2-S_1}(S - S_1e^{-r_f}) \\end{equation}   \\begin{equation}   c_{REP}= \\frac{S_2-X}{S_2-S_1}(S - S_1e^{-r_f}) \\end{equation}","title":"1. Replicating Portfolio Approach"},{"location":"posts/call-option-value-from-two-approaches/#2-risk-neutral-approach","text":"<p>Without too much trouble, we can derive the call value using risk neutral approach as</p>   \\begin{align} c_{RN} &amp;= \\frac{p(S_2-X)+(1-p)\\times0}{e^{r_f}}\\newline &amp;= \\frac{p(S_2-X)+0}{e^{r_f}}\\newline  &amp;= p(S_2-X) e^{-r_f} \\end{align}   \\begin{align} c_{RN} &amp;= \\frac{p(S_2-X)+(1-p)\\times0}{e^{r_f}}\\newline &amp;= \\frac{p(S_2-X)+0}{e^{r_f}}\\newline  &amp;= p(S_2-X) e^{-r_f} \\end{align}   <p>We know that</p>   \\begin{equation} p\\times \\frac{S_2}{S} + (1-p)\\frac{S_1}{S} = e^{r_f} \\end{equation}   \\begin{equation} p\\times \\frac{S_2}{S} + (1-p)\\frac{S_1}{S} = e^{r_f} \\end{equation}   <p>so</p>   \\begin{align} p &amp;= \\frac{e^{r_f}-\\frac{S_1}{S}}{\\frac{S_2}{S}-\\frac{S_1}{S}}\\newline &amp;=\\frac{Se^{r_f}-S_1}{S_2-S_1} \\end{align}   \\begin{align} p &amp;= \\frac{e^{r_f}-\\frac{S_1}{S}}{\\frac{S_2}{S}-\\frac{S_1}{S}}\\newline &amp;=\\frac{Se^{r_f}-S_1}{S_2-S_1} \\end{align}   <p>Therefore,</p>   \\begin{align} c_{RN} &amp;= p(S_2-X) e^{r_f}\\newline &amp;=\\frac{Se^{r_f}-S_1}{S_2-S_1}(S_2-X) e^{-r_f}\\newline &amp;=\\frac{S-S_1e^{-r_f}}{S_2-S_1}(S_2-X) \\end{align}   \\begin{align} c_{RN} &amp;= p(S_2-X) e^{r_f}\\newline &amp;=\\frac{Se^{r_f}-S_1}{S_2-S_1}(S_2-X) e^{-r_f}\\newline &amp;=\\frac{S-S_1e^{-r_f}}{S_2-S_1}(S_2-X) \\end{align}","title":"2. Risk Neutral Approach"},{"location":"posts/call-option-value-from-two-approaches/#identical-result-from-the-two-methods","text":"<p>It's easy to find that</p>   c_{RN} = c_{REP}   c_{RN} = c_{REP}   <p>Hence, the call option value from replicating portfolio is the same as from risk neutral approach.</p>","title":"Identical Result from the Two Methods"},{"location":"posts/centrifuge-problem/","text":"<p>Question</p> <p>Given a centrifuge with n holes, can we balance it with kk (1\\le k \\le n1\\le k \\le n) identical test tubes?</p>  <p>This is a simple yet interesting problem, very well illustrated by Numberphile and discussed by Matt Baker's blog.</p> <p>The now proved solution is that:</p>  <p>Note</p> <p>You can balance kk identical test tubes, 1\\le k\\le n1\\le k\\le n, in an nn-hole centrifuge if and only if both kk and n-kn-k can be expressed as a sum of prime divisors of nn.</p>  Example: 18-hole centrifugeExample: 20-hole centrifuge   <p></p>   <p></p>    <p>Below is my attempt to programmatically answer the centrifuge problem.</p>","title":"Centrifuge Problem"},{"location":"posts/centrifuge-problem/#method-1-naive-dfs","text":"<p>The very first method literally follows the solution. For a given (n,k)(n,k) pair, check if kk and n-kn-k can be written as a linear combination of the prime divisors of nn (with non-negative coefficients).</p> <pre><code>def is_linear_combination(x: int, prime_numbers: list) -&gt; bool:\n    \"\"\"Check if `x` can be written as a linear combination of prime numbers, i.e.,\n\n    x = b1*p1 + b2*p2 + b3*p3 + ... + bn*pn\n\n    where pi represents a prime number in `prime_numbers`, bi is a non-negative integer.\n    \"\"\"\n    # very naive and not optimized\n    for n in prime_numbers:\n        # n divides x \n        if x % n:\n            return True\n        # n does not divides x, check if the difference between x and multiples of n can be\n        # a linear combination of other remaining prime numbers\n        for i in range(x//n):\n            if is_linear_combination(x - i*n, [p for p in prime_numbers if p!=n]):\n                return True\n    return False \n\ndef centrifuge_naive(n: int, k: int) -&gt; bool:\n    \"\"\"Check if a `n`-hole centrifuge can be balanced with `k` identical test tubes.\n\n    True if both `k` and `n-k` can be written as a linear combination of the prime divisors of `n`.\n    \"\"\"\n    prime_divisors = get_prime_divisors(n) # simple cached function, skipped\n    return is_linear_combination(k, prime_divisors) and is_linear_combination(n-k, prime_divisors)\n</code></pre>","title":"Method 1: Na\u00efve DFS"},{"location":"posts/centrifuge-problem/#some-optimizations","text":"<p>The above method works just fine, but very slow if we want to compute the total number of solutions, instead of just checking whether a particular kk works.</p> <p>There can be a few optimizations, for example, we can compute only the lower half of kks:</p> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef centrifuge_naive(n: int, k: int) -&gt; bool:\n    prime_divisors = get_prime_divisors(n) # cached\n    if k &gt; n//2:\n        return centrifuge(n, n-k)\n    return is_linear_combination(k, prime_divisors) and is_linear_combination(n-k, prime_divisors)\n</code></pre> <p>Further, if nn is a (large) prime number itself, we understand that all 1\\le k\\lt n1\\le k\\lt n will not work. Similarly, if nn is a power of prime number, we can bypass many values of kk too.</p> <pre><code>@lru_cache(maxsize=None)\ndef centrifuge_naive(n, k):\n    prime_divisors = get_prime_divisors(n)\n    # ...\n    # special case when n is power of prime\n    if len(prime_divisors) == 1:\n        p = prime_divisors[0]\n        return (k % p == 0) and ((n - k) % p == 0)\n    # ...\n</code></pre> <p>At certain point, we will realize that it would be faster to simply compute all possible kks instead of checking one by one whether a certain kk can balance the centrifuge. This leads us to the second approach, which I call \"bootstrap\".</p>","title":"Some Optimizations"},{"location":"posts/centrifuge-problem/#method-2-bootstrap","text":"<p>The bootstrap method is a variant of DFS, which essentially generates all possible kk for a given nn by exhausting the values from linear combinations of nn's prime divisors. The generated values should be between 2 and nn. Then we can tell if k'k' can balance the nn-hole centrifuge by checking whether k'k' and n-k'n-k' are in the generated values.</p> <pre><code>def bootstrap(x, n, numbers, result):\n    \"\"\"Compute all linear combinations of the given numbers smaller than n\"\"\"\n    for p in numbers:\n        if p+x &gt; n:\n            break\n        for i in range((n-x) // p):\n            p_ = x + p * i # p_ &lt;= n\n            if not result[p_]:\n                # x + p*i has not been tested, and is a linear combination of given numbers \n                result[p_] = True\n                # check whether we can add multiples of remaining numbers\n                bootstrap(p_, n, [n2 for n2 in numbers if n2 != p], result)\n\ndef centrifuge_bootstrap(n: int, k: int) -&gt; bool:\n    prime_divisors = get_prime_divisors(n) # cached, `prime_divisors` is sorted\n    # result[k] represents whether k is valid, k=0...n\n    result = [True] + [False] * (n-1) + [True]\n    bootstrap(0, n, prime_divisors, result) # TODO: bootstrap only once for a given `n`\n    return result[k] and result[n-k]\n</code></pre> <p>This method invests some time in pre-computing all possible linear combinations of the prime divisors of nn. If we are only interested to see a particular (n,k)(n,k) pair, we can break out when we have done <code>result[k]</code> and <code>result[n-k]</code> in <code>bootstrap()</code>.</p>","title":"Method 2: Bootstrap"},{"location":"posts/centrifuge-problem/#method-3-dynamic-programming","text":"<p>The last method uses dynamic programming. We can use f[k]f[k]=<code>True</code> to represent that kk is a linear combination of nn's prime divisors. A value ii is either itself a prime divisor of nn (and thus a linear combination of the prime divisors), or the sum of a nn's prime divisor pp and (i-p)(i-p). In the latter case, if (i-p)(i-p) is a linear combination of nn's prime divisors, so is p+(i-p)=ip+(i-p)=i. </p>  <p>Hint</p> <p>If (i-p)(i-p) is a linear combination of nn's prime divisors, i.e., i-p=a_1p_1+a_2p_2+...+a_np_ni-p=a_1p_1+a_2p_2+...+a_np_n, where \\{p_i\\}\\{p_i\\} are the prime divisors of nn and \\{a_i\\}\\{a_i\\} are non-negative integers, then i-p+pi-p+p is definitely a linear combination too: pp's coefficient becomes a+1\\ge0a+1\\ge0.</p>  <p>Hence,</p> <ul> <li>f[i] = f[i] \\text{ or } f[i-p]f[i] = f[i] \\text{ or } f[i-p]</li> </ul> <p>The boundary condition is f[0]f[0] = <code>True</code>, i.e., an empty centrifuge is balanced.</p> <p>The whole function is extremely short:</p> <pre><code>def centrifuge_dp(n: int, k: int) -&gt; bool:\n    prime_divisors = get_prime_divisors(n) # cached, `prime_divisors` is sorted\n    f = [True] + [False] * n\n    for p in prime_divisors: # TODO: DP only once for a given `n`\n        for i in range(p, n+1):\n            f[i] = f[i] or f[i-p]\n    return f[k] and f[n-k]\n</code></pre>","title":"Method 3: Dynamic Programming"},{"location":"posts/centrifuge-problem/#performance-comparison","text":"<p>Obviously, the Method 2 and 3 are much faster than the na\u00efve Method 1. Method 3 does not even use recursion and is the fastest.</p>  <p>Note</p> <p>A note there is that if we are to check all 1\\le k\\le n1\\le k\\le n, e.g., <code>[i for i in range(1, n+1) if centrifuge(n,i)]</code>, we need to make some adjustment to the functions above so as to bootstrap or perform DP only once for each nn. This is trivial.</p>","title":"Performance Comparison"},{"location":"posts/centrifuge-problem/#visualization","text":"<p>Below are some plots of balanced centrifuges. Note that for a particular value of kk, there can be more than one way to balance the centrifuge. Here, I illustrate only one.</p> 6-hole10-hole12-hole12-hole20-hole24-hole33-hole   <p><pre><code>plot_centrifuge(6, \"6-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(10, \"10-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(12, \"12-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(18, \"18-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(20, \"20-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(24, \"24-hole-centrifuge.svg\")\n</code></pre> </p>   <p><pre><code>plot_centrifuge(33, \"33-hole-centrifuge.svg\")\n</code></pre> </p>","title":"Visualization"},{"location":"posts/centrifuge-problem/#python-code","text":"<p>The code to generate the plots above:</p> <pre><code>from functools import lru_cache\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n@lru_cache(maxsize=None)\ndef prime_divisors(n):\n    \"\"\"Return list of n's prime divisors\"\"\"\n    primes = []\n    p = 2\n    while p**2 &lt;= n:\n        if n % p == 0:\n            primes.append(p)\n            n //= p\n        else:\n            p += 1 if p % 2 == 0 else 2\n    if n &gt; 1:\n        primes.append(n)\n    return primes\n\n\ndef centrifuge(n):\n    \"\"\"Return a list of which the k-th element represents if k tubes can balance the n-hole centrifuge\"\"\"\n    F = [True] + [False] * n\n    for p in prime_divisors(n):\n        for i in range(p, n + 1):\n            F[i] = F[i] or F[i - p]\n    return [F[k] and F[n - k] for k in range(n + 1)]\n\n\ndef factorize(k: int, nums: list) -&gt; list:\n    \"\"\"Given k, return the list of numbers from the given numbers which add up to k.\n    The given numbers are guaranteed to be able to generate k via a linear combination.\n\n    Examples:\n        &gt;&gt;&gt; factorize(5, [2, 3])\n        [2, 3]\n        &gt;&gt;&gt; factorize(6, [2, 3])\n        [2, 2, 2]\n        &gt;&gt;&gt; factorize(7, [2, 3])\n        [2, 2, 3]\n    \"\"\"\n\n    def _factorize(k, nums, res: list):\n        for p in nums:\n            if k % p == 0:\n                res.extend([p] * (k // p))\n                return True\n            else:\n                for i in range(1, k // p):\n                    if _factorize(k - p * i, [n for n in nums if n != p], res):\n                        res.extend([p] * i)\n                        return True\n        return False\n\n    res = []\n    _factorize(k, nums, res)\n    return res\n\n\n@lru_cache(maxsize=None)\ndef centrifuge_k(n, k):\n    \"\"\"Given (n, k) and that k balances a n-hole centrifuge, find the positions of k tubes\"\"\"\n    if n == k:\n        return [True] * n\n    factors = factorize(k, prime_divisors(n))\n    pos = [False] * n\n\n    def c(factors: list, pos: list) -&gt; bool:\n        if sum(pos) == k:\n            return True\n        if not factors:\n            return False\n        p = factors.pop(0)\n        pos_wanted = [n // p * i for i in range(p)]\n        for offset in range(n):\n            pos_rotated = [(i + offset) % n for i in pos_wanted]\n            # the intended positions of the p tubes are all available\n            if not any(pos[i] for i in pos_rotated):\n                # claim the positions\n                for i in pos_rotated:\n                    pos[i] = True\n                if not c(factors, pos):\n                    # unclaim the positions\n                    for i in pos_rotated:\n                        pos[i] = False\n                else:\n                    return True\n        # all rotated positions failed, add p back to factors to place later\n        factors.append(p)\n\n    c(factors, pos)\n    return pos\n\n\ndef plot_centrifuge(n, figname=\"centrifuge.svg\"):\n    ncols = max(int(n**0.5), 1)  # minimum 1 column\n    nrows = n // ncols if n % ncols == 0 else n // ncols + 1\n    height = 3 if nrows == ncols else 2\n    width = 2\n    fig, axes = plt.subplots(nrows, ncols, figsize=(height * nrows, width * ncols))\n    z = np.exp(2 * np.pi * 1j / n)\n\n    theta = np.linspace(0, 2 * np.pi, 20)\n    radius = 1 / (ncols + nrows)\n    a = radius * np.cos(theta)\n    b = radius * np.sin(theta)\n\n    cent = centrifuge(n)\n    for nr in range(nrows):\n        for nc in range(ncols):\n            k = nr * ncols + nc + 1\n            axis = axes[nr, nc] if ncols &gt; 1 else axes[nr]\n            if k &gt; n:\n                axis.axis(\"off\")\n                continue\n            # draw the n-holes\n            for i in [z**i for i in range(n)]:\n                axis.plot(a + i.real, b + i.imag, color=\"b\" if cent[k] else \"gray\")\n            # draw the k tubes\n            if cent[k]:\n                if k &gt; n // 2:\n                    pos = [not b for b in centrifuge_k(n, n - k)]\n                else:\n                    pos = centrifuge_k(n, k)\n                for i, ok in enumerate(pos):\n                    i = z**i\n                    if ok:\n                        axis.fill(a + i.real, b + i.imag, color=\"r\")\n\n            axis.set_aspect(1)\n            axis.set(xticklabels=[], yticklabels=[])\n            axis.set(xlabel=None)\n            axis.set_ylabel(f\"k={k}\", rotation=0, labelpad=10)\n            axis.tick_params(bottom=False, left=False)\n\n    fig.suptitle(f\"$k$ Test Tubes to Balance a {n}-Hole Centrifuge\")\n    fig.text(0.1, 0.05, \"Red dot represents the position of test tubes.\")\n    plt.savefig(figname)\n    plt.close(fig)\n\n\nif __name__ == \"__main__\":\n    for n in range(6, 51):\n        print(f\"Balancing {n}-hole centrifuge...\")\n        plot_centrifuge(n, f\"{n}-hole-centrifuge.png\")\n</code></pre>","title":"Python code"},{"location":"posts/centrifuge-problem/#download-plots-of-balanced-centrifuges","text":"<p>Success</p> <p>You can download the Python code and all plots of balanced nn-hole centrifuge, 6\\le n\\le506\\le n\\le50, which I calculated using the code above.</p>","title":"Download plots of balanced centrifuges"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/","text":"<p>In certain scenarios, we want to estimate a model's parameters on the sample for each observation with itself excluded. This can be achieved by estimating the model repeatedly on the leave-one-out samples but is very inefficient. If we estimate the model on the full sample, however, the coefficient estimates will certainly be biased. Thankfully, we have the Jackknife method to correct for the bias, which produces the Jackknifed coefficient estimates for each observation.</p>","title":"Compute Jackknife Coefficient Estimates in SAS"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/#variable-definition","text":"<p>Let's start with some variable definitions to help with the explanation.</p>    Variable Definition     b(i) the parameter estimates after deleting the iith observation   s^2(i)s^2(i) the variance estimate after deleting the iith observation   X(i)X(i) the XX matrix without the iith observation   \\hat{y}(i)\\hat{y}(i) the iith value predicted without using the iith observation   r_i = y_i - \\hat{y}_ir_i = y_i - \\hat{y}_i the iith residual   h_i = x_i(X'X)^{-1}x_i'h_i = x_i(X'X)^{-1}x_i' the iith diagonal of the projection matrix for the predictor space, also called the hat matrix   RStudent =\\frac{r_i}{s(i) \\sqrt{1-h_i}}RStudent =\\frac{r_i}{s(i) \\sqrt{1-h_i}} studentized residual   (X'X)_{jj}(X'X)_{jj} the (j,j)(j,j)th element of (X'X)^{-1}(X'X)^{-1}   DFBeta_j = \\frac{b_{j} - b_{(i)j}}{s(i)\\sqrt{(X'X)_{jj}}}DFBeta_j = \\frac{b_{j} - b_{(i)j}}{s(i)\\sqrt{(X'X)_{jj}}} the scaled measures of the change in the jjth parameter estimate calculated by deleting the iith observation","title":"Variable Definition"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/#objective","text":"<p>Compute the coefficient estiamtes with the iith observation excluded from the sample, i.e. b(i)b(i), or the Jackknifed coefficient estimate.</p>","title":"Objective"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/#formula","text":"<p>From the table above, we can get that the jjth Jackknifed coefficient estimate b_{(i)j}b_{(i)j} without using the iith observation is:</p>  b_{(i)j} = b_j - DFBeta_j \\times s(i) \\sqrt{(X'X)_{jj}}  b_{(i)j} = b_j - DFBeta_j \\times s(i) \\sqrt{(X'X)_{jj}}   <p>Hence,</p>  b_{(i)j} = b_j - DFBeta_j \\times \\frac{r_i}{RStudent\\times \\sqrt{1-h_i}} \\sqrt{(X'X)_{jj}} b_{(i)j} = b_j - DFBeta_j \\times \\frac{r_i}{RStudent\\times \\sqrt{1-h_i}} \\sqrt{(X'X)_{jj}}  <p>The good thing is that <code>PROC REG</code> produces the coefficient estimate b_jb_j for j=1,2,...Kj=1,2,...K, where KK is the number of coefficients, and the <code>INFLUENCE</code> and <code>I</code> options produce the remaining statistics just enough to compute b(i)b(i):</p>    Variable Option in <code>PROC REG</code> or <code>MODEL</code> statement Name in the output dataset     b_jb_j <code>Outest=</code> option in <code>PROC REG</code> <code>&lt;jthVariable&gt;</code>   r_ir_i <code>OutputStatistics=</code> from <code>INFLUENCE</code> option in <code>MODEL</code> statement <code>Residual</code>   RStudentRStudent <code>OutputStatistics=</code> from <code>INFLUENCE</code> option in <code>MODEL</code> statement <code>RStudent</code>   h_ih_i <code>OutputStatistics=</code> from <code>INFLUENCE</code> option in <code>MODEL</code> statement <code>HatDiagnol</code>   DFBeta_jDFBeta_j <code>OutputStatistics=</code> from <code>INFLUENCE</code> option in <code>MODEL</code> statement <code>DFB_&lt;jthVariable&gt;</code>   (X'X)_{jj}(X'X)_{jj} <code>InvXPX=</code> from <code>I</code> option in <code>MODEL</code> statement <code>&lt;jthVariable&gt;</code>","title":"Formula"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/#example","text":"","title":"Example"},{"location":"posts/compute-jackknife-coefficient-estimates-in-sas/#discretionary-accruals","text":"<p>Suppose we want to calculate the firm-level discretionary accruals for each year using the Jones (1991) model and Kothari et al (2005) model. For a firm ii, we need to first estimate the model for the industry-year excluding firm ii, then use the coefficient estimates to generate predicted accruals for firm ii. The firm's discretionary accruals is the actual accruals minus the predicted accruals.</p> <p>Below is an example <code>PROC REG</code> that produces three datasets named <code>work.params</code>, <code>work.outstats</code> and <code>work.xpxinv</code>, which contain sufficient statistics to compute the Jackknifed estimates and thus the predicted accruals.</p> <pre><code>ods listing close; \nproc reg data=work.funda edf outest=work.params;\n  /* industry-year regression */\n  by fyear sic2;\n  /* id is necessary for later matching Jackknifed coefficients to firm-year */\n  id key;\n  /* Jones Model */\n  Jones: model tac = inv_at_l drev ppe / noint influence i;\n  /* Kothari Model with ROA */\n  Kothari: model tac = inv_at_l drevadj ppe roa / noint influence i;\n  ods output OutputStatistics=work.outstats InvXPX=work.xpxinv;\nrun;\nods listing;\n</code></pre> <p>Full SAS program for estimating 5 different measures of discretionary accruals:</p>  SAS code for computing discretionary accruals <pre><code>/* Use Jackknife method to compute discretionary accruals */\n/* see https://mingze-gao.com/posts/compute-jackknife-coefficient-estimates-in-sas/ */\n\n/* UseHribarCollinsTotalAccruals:\n  - true:  use Hribar-Collins Cashflow Total Accruals \n  - false: use normal method */\n%let UseHribarCollinsTotalAccruals = false;\n\n/* Include %array and %do_over */\nfilename do_over url \"https://mingze-gao.com/utils/do_over.sas\";\nfilename array url \"https://mingze-gao.com/utils/array.sas\";\n%include do_over array;\n\n/* Winsorize macro */\nfilename winsor url \"https://mingze-gao.com/utils/winsor.sas\";\n%include winsor;\n\n/*\n  Earnings management models\n\n  Author: Mingze (Adrian) Gao, Feb 2019\n  Modified based on the work by Joost Impink, March 2016\n\n  Models estimated (Note that the intercept a0 is removed in the modified code below):\n  - Jones model,    tac = a0 + a1 1/TAt-1 + a2chSales + a3PPE + a4ROA + error.\n    - variable names DA_Jones  \n  - Modified Jones model, as Jones model, but using chSales - chREC to compute fitted values.\n    - variable names DA_mJones  \n  - Kothari 2005, controlling for ROA, tac = a0 + a1 1/TAt-1 + a2(chSales - chREC) + a3PPE + a4ROA + error.\n    - variable names DA_Kothari   \n  - Kothari 2005, performance matched, Jones model, difference in discretionary accruals between firm and closest firm in terms of (contemporaneous) roa\n    - variable names DA_pmKothari_Jones\n  - Kothari 2005, performance matched, modified Jones model, difference in discretionary accruals between firm and closest firm in terms of (contemporaneous) roa\n    - variable names DA_pmKothari_mJones\n\n  tac:      Total accruals, computed as net profit after tax before extraordinary items less cash flows from operations \n  1/TAt-1:  Inverse of beginning of year total assets\n  chSales:  Change in net sales revenue\n  chREC:        Change in net receivables\n  PPE:      Gross property, plant, and equipment\n  ROA:      Return on assets. \n  Variables used Compustat Funda\n  AT:       Total assets\n  IB:   Income Before Extraordinary Items\n  IBC:  Income Before Extraordinary Items (Cash Flow) (used if IB is missing)\n  OANCF:    Operating Activities - Net Cash Flow\n  PPEGT:    Property, Plant and Equipment - Total (Gross)\n  RECT:     Receivables - Total\n  SALE: Sales\n  INVT: Inventories - Total\n  LCO:  Current Liabilities Other Total\n  DP:       Depreciation and Amortization\n  ACO:  Current Assets Other Total\n  AP:       Accounts Payable - Trade\n*/\n\n/* Get Funda variables */\n%let fundaVars = at ib ibc oancf ppegt rect sale xidoc lco dp aco invt ap;\n\ndata work.a_funda(keep=key gvkey fyear datadate sich &amp;fundaVars);\n  set comp.funda;\n  if 1980 &lt;= fyear &lt;= 2018;\n  /* Generic filter */\n  if indfmt='INDL' and datafmt='STD' and popsrc='D' and consol='C';\n  /* Firm-year identifier */\n  key = gvkey || fyear;\n  /* Keep if sale &gt; 0, at &gt; 0 */\n  if sale &gt; 0 and at &gt; 0;\n  /* Use Income Before Extraordinary Items (Cash Flow) if ib is missing */\n  if ib =. then ib=ibc;\nrun;\n\n/* Lagged values for: at sale rect invt aco ap lco */\n%let lagVars = at sale rect invt aco ap lco;\n\n/* Self join to get lagged values at_l, sale_l, rect_l */\nproc sql;\n  create table work.b_funda as select a.*, %do_over(values=&amp;lagVars, between=comma, phrase=b.? as ?_l)\n  from work.a_funda a, work.a_funda b\n  where a.gvkey = b.gvkey and a.fyear-1 = b.fyear;\nquit;\n\n/* Construct additional variables */\ndata work.b_funda(compress=yes);\n  set work.b_funda;\n  /* 2-digit SIC  */\n  sic2 = int(sich/100);\n  /* variables */\n  if \"&amp;UseHribarCollinsTotalAccruals.\" eq \"false\" then\n    tac     = ((rect-rect_l)+(invt-invt_l)+(aco-aco_l)-(ap-ap_l)-(lco-lco_l)-dp)/at_l; /* Accruals ratio */\n  else\n    tac     = (ibc - oancf + xidoc)/at_l;  /* Hribar Collins total cash flow accruals */\n  inv_at_l      = 1 / at_l;\n  drev          = (sale - sale_l) / at_l;\n  drevadj       = (sale - sale_l)/at_l - (rect - rect_l)/at_l;\n  ppe           = ppegt / at_l;\n  roa       = ib / at_l;\n  /* these variables may not be missing (cmiss counts missing variables)*/\n  *if cmiss  (of tac inv_at_l drevadj ppe roa) eq 0;\nrun;\n\n/* Optional winsorization before industry-year regression */\n%let winsVars = tac inv_at_l drev drevadj ppe roa  ; \n%winsor(dsetin=work.b_funda, dsetout=work.b_funda_wins, byvar=fyear, vars=&amp;winsVars, type=winsor, pctl=1 99);\n\n/* Regression by industry-year \nedf(error degrees of freedom) + #params will equal the number of obs (no need for proc univariate to count) */\nproc sort data=work.b_funda_wins; by fyear sic2; run;\n/* regressors */\n%array(vars, values=inv_at_l drev ppe drevadj roa);\nods listing close;\nproc reg data=work.b_funda_wins edf outest=work.c_parms;\n  by fyear sic2;\n  id key;\n  /* Jones Model */\n  Jones:        model tac = inv_at_l drev ppe / noint influence i;  \n  /* Kothari with ROA in model */ \n  Kothari:  model tac = inv_at_l drevadj ppe roa / noint influence i;\n  ods output OutputStatistics=work.outstats InvXPX=work.xpxinv;\nrun;\nods listing;\n\n/* Compute discretionary accrual measures */\nproc sql;\n  /* Compute firm-year Jackknifed coefficient estimates */\n  create table work.xpxinv2 as\n  /* Extract the diagnol elements of the symmetric inv(X'X) for each firm-year */\n    select fyear, sic2, model,\n      %do_over(vars, phrase=sum(case when variable=\"?\" then xpxinv else . end) as ?, between=comma)\n    from (select fyear, sic2, model, variable,\n        case %do_over(vars, phrase=when variable=\"?\" then ?) else . end as xpxinv\n      from work.xpxinv where variable ~= 'tac')\n    group by fyear, sic2, model\n    order by fyear, sic2, model;\n  /* The difference between original coefficient estimates and the Jackknifed estimates */\n  create table work.bias as\n    select a.fyear, a.sic2, a.model, a.key,\n      %do_over(vars, phrase=a.DFB_?*(a.Residual/(a.RStudent*sqrt(1-a.HatDiagonal)))*sqrt(b.?) as bias_?, between=comma)\n    from work.outstats as a left join work.xpxinv2 as b\n    on a.fyear=b.fyear and a.sic2=b.sic2 and a.model=b.model\n    order by a.fyear, a.sic2, a.model, a.key;\n  /* Compute Jackknifed coefficient estimates by subtracting the bias from the original estimates */\n  create table work.Jackknifed_params as\n    select a.fyear, a.sic2, a.model, a.key, %do_over(vars, phrase=b.? - a.bias_? as ?, between=comma), b._EDF_\n    from work.bias as a left join work.c_parms as b\n    on a.fyear=b.fyear and a.sic2=b.sic2 and a.model=b._MODEL_\n    order by a.fyear, a.sic2, a.model, a.key;\n  /* Compute discretionary accruals */\n  create table work.tmp as\n    select distinct a.fyear, a.sic2, a.gvkey, a.key,\n      /* Jones model at a minimum 8 obs (5 degrees of freedom + 3 params) */\n      sum(case when b.model eq 'Jones' and b._EDF_ ge 5 then\n        a.tac - (%do_over(values=inv_at_l drev ppe, between=%str(+), phrase=a.? * b.?)) else . end) as DA_Jones,\n      /* Modified Jones model: drev is used in first model, but drevadj is used to compute fitted value */\n      sum(case when b.model eq 'Jones' and b._EDF_ ge 5 then\n        a.tac - (a.drevadj * b.drev + %do_over(values=inv_at_l ppe, between=%str(+), phrase=a.? * b.?)) else . end) as DA_mJones,\n      /* Kothari model (with ROA in regression) at a minimum 8 obs (4 degrees of freedom + 4 params) */\n      sum(case when b.model eq 'Kothari' and b._EDF_ ge 4 then\n        a.tac - (%do_over(values=inv_at_l drevadj ppe roa, between=%str(+), phrase=a.? * b.?)) else . end) as DA_Kothari\n    from work.b_funda_wins as a left join work.Jackknifed_params as b\n    on a.key=b.key\n    group by a.key\n    order by a.gvkey, a.fyear;\n  /* Kothari performance matching: get DA_Jones (DA_mJones) accruals for the matched firm closest in ROA */\n  create table work.da_roa as select a.*, b.roa from work.tmp as a left join work.b_funda_wins as b on a.key=b.key;\n  create table work.da_all as\n    select a.*,\n      /* gvkey of matched firm */\n      b.gvkey as gvkey_m, \n      /* difference in ROA */\n      abs(a.roa - b.roa) as Difference, \n      /* difference in DA_Jones */\n      a.DA_Jones - b.DA_Jones as DA_pmKothari_Jones,\n      a.DA_mJones - b.DA_mJones as DA_pmKothari_mJones\n    from work.da_roa as a left join  work.da_roa as b\n    on a.fyear = b.fyear and a.sic2 = b.sic2 /* same 2-digit SIC industry-year */       \n    and a.key ne b.key /* not the same firm */\n    group by a.gvkey, a.fyear\n    having Difference = min(Difference) /* keep best match for size difference */\n    order by gvkey, fyear;\nquit;\n\n/* drop possible multiple matches (with the same difference) in previous step */\nproc sort data=work.da_all nodupkey; by key; run;\n\n%let DAVars = DA_Jones DA_mJones DA_Kothari DA_pmKothari_Jones DA_pmKothari_mJones;\n\n/* Winsorize discretionary accrual variables (Optional) */\n%winsor(dsetin=work.da_all, dsetout=work.accruals_HribarCollins_&amp;UseHribarCollinsTotalAccruals., byvar=fyear, vars=&amp;DAVars, type=winsor, pctl=1 99);\n\n/* Means, medians for key variables */\nproc means data=work.accruals_HribarCollins_&amp;UseHribarCollinsTotalAccruals. n mean min median max; var &amp;DAVars; run; \n</code></pre>","title":"Discretionary accruals"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/","text":"<p>Computing the weekly returns from the CRSP daily stock data is a common task but may be tricky sometimes. Let's discuss a few different ways to get it done incorrectly and correctly.</p>  <p>TL;DR Take me to the final solution!</p> <p>Surely -&gt; The solution</p>","title":"Compute Weekly Return from Daily CRSP Data"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#incorrect-ways","text":"<p>Let me start with a few incorrect ways, which may seem perfectly okay at first glance. This part is important because it shows you how a small mistake can lead to hard-to-discover bugs.</p>","title":"INCORRECT ways"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#weekly-index-return-from-daily-data","text":"Date as the Friday of the weekDate as the last trading day of the week   <p>Using <code>intnx()</code>, we can derive the Friday of the week given a date, as shown below. </p> <pre><code>proc sql;\n/* Compute weekly marekt return from daily data */\ncreate table mktret_weekly as \nselect distinct date, \n    year(date) as Year,\n    week(date) as Week,\n    case when weekday(date)=6 then date\n    else intnx(\"week.6\",date,1) end as FridayOfWeek format=date9.,\n    (exp(sum(log(1+sprtrn)))-1)*100 as mktret label=\"Weekly SP500 Index Return (%)\"\nfrom crsp.dsi \nwhere \n    year(date) between &amp;startyear. and &amp;endyear.\ngroup by year(date), week(date) order by date;\nquit;\n</code></pre> <p>Note that <code>intnx(\"weekday.6\", date, 0)</code> will give the last Friday, which is not what we want. We want the next Friday of the week for a given date, so we use <code>intnx(\"weekday.6\", date, 1)</code>. The <code>case...when...</code> statement ensures that if the given date is already a Friday, we don't go for the next one. Below is a sample output of the <code>mktret_weekly</code> table generated. </p>  Example output of <code>mktret_weekly</code>    Obs Date Year Week FridayOfWeek mktret     1 19860102 1986 0 03JAN1986 -0.1893222   2 19860103 1986 0 03JAN1986 -0.1893222   3 19860106 1986 1 10JAN1986 -2.333080418   4 19860107 1986 1 10JAN1986 -2.333080418   5 19860108 1986 1 10JAN1986 -2.333080418   6 19860109 1986 1 10JAN1986 -2.333080418   7 19860110 1986 1 10JAN1986 -2.333080418   8 19860113 1986 2 17JAN1986 1.1992620931   9 19860114 1986 2 17JAN1986 1.1992620931     <p>We can verify that the <code>FridayOfWeek</code> indeed gives the Friday of the week. Therefore, the final weekly dataset using Friday as the date identifier just need to keep <code>FridayOfWeek</code> and <code>mktret</code>.</p> <pre><code>proc sql;\n/* Compute weekly marekt return from daily data */\ncreate table mktret_weekly as \nselect distinct\n    case when weekday(date)=6 then date else intnx(\"week.6\",date,1) end \n        as date format=date9. label=\"Friday of the Week\",\n    (exp(sum(log(1+sprtrn)))-1)*100 \n        as mktret label=\"Weekly SP500 Index Return (%)\"\nfrom crsp.dsi \nwhere \n    year(date) between &amp;startyear. and &amp;endyear.\ngroup by year(date), week(date) order by date;\nquit;\n</code></pre>  Example output of <code>mktret_weekly</code>    Obs date mktret     1 03JAN1986 -0.1893222   2 10JAN1986 -2.333080418   3 17JAN1986 1.1992620931   4 24JAN1986 -0.959555101   5 31JAN1986 2.5916781551   6 07FEB1986 1.3126828796       <pre><code>%let startyear=1986;\n%let endyear=2019;\n\nproc sql;\n/* Compute weekly marekt return from daily data */\ncreate table mktret_weekly as \nselect distinct date, \n    (exp(sum(log(1+sprtrn)))-1)*100 as mktret label=\"Weekly SP500 Index Return (%)\"\nfrom crsp.dsi where year(date) between &amp;startyear. and &amp;endyear. \ngroup by year(date), week(date) \nhaving date=max(date) \norder by date;\nquit;\n</code></pre> <p>Note here that it's tempting to use <code>having weekday(date)=6</code> to make sure the dates are all Friday. However, if Friday in a week is not the last trading day, then the weekly return will be missing. This is why here I use <code>date=max(date)</code> to ensure non-missing weekly returns. The date is the last trading day in any given week, consistent with the CRSP's daily stock file.</p> <p>The caveat here is that since the dates are the weekly last trading days, when merged with other weekly datasets, you should be very careful about whether the other dataset is using Friday or the last trading day per week as its date variable.</p>","title":"Weekly index return from daily data"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#weekly-stock-return-from-daily-data","text":"<p>Following the same logic, we can calculate the weekly stock returns from daily CRSP data, where dates are aligned to the Friday of the week.</p> <pre><code>proc sql;\n/* Stocks (ordinary shares only) in the financial sector */\ncreate table stocks as select distinct permno from crsp.stocknames\nwhere shrcd in (10, 11) and floor(siccd/100) between 60 and 67;\n\ncreate table stockrets_weekly as \nselect distinct permno,\n    case when weekday(date)=6 then date else intnx(\"week.6\",date,1) end \n        as date format=date9. label=\"Friday of the Week\",\n    (exp(sum(log(1+ret)))-1)*100 as ret label=\"Weekly Return (%)\"\nfrom crsp.dsf \nwhere \n    year(date) between &amp;startyear. and &amp;endyear.\n    and permno in (select * from stocks) \n    and prc&gt;0 and not missing(ret)\ngroup by year(date), week(date), permno order by permno, date;\nquit;\n</code></pre>","title":"Weekly stock return from daily data"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#whats-wrong","text":"<p>The code above seems okay. We know that CRSP daily stock file contains many observations where the daily trading volume is 0, in which case the price is recorded as the negative bid-ask midpoint. Therefore, we restrict to only those with positive stock prices. So what's the problem?</p>  <p>The problem is that a week can span two calendar years.</p>  <p>For example, check out the last week of 2019:</p>    Mon Tue Wed Thu Fri Sat Sun     30 31 1 2 3 4 5    <ul> <li>Dec30 and Dec31 belong to week 53 of 2019, while the code above will use these   two days' returns to compute the weekly return and align the date to Jan03 of   2020.</li> <li>Jan01 to Jan03 belong to week 0 of 2020, so the code above will use these   three days' returns to compute the weekly return and align the date to Jan03   of 2020.</li> </ul> <p>Now we have a mistake. A single week is broken into two because of the use of <code>week()</code> function in SAS. Another consequence is that when there're many years of data, there will be a lot of duplicates.</p>","title":"What's wrong?"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#correct-ways","text":"<p>Now let's explore two ways that avoid this mistake. Although both generate the same result (there can be a few differences, see the caveat), the second one is much faster.</p>","title":"CORRECT ways"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#1-start-with-a-list-of-dates-slow-version","text":"<p>Now we can write some correct code to compute the weekly returns. We'll generate a series of Fridays first, then we merge based on the past 5 calendar days. This will ensure all trading days with non-missing data will be included in the weekly return calculation, and correct the mistake mentioned above.</p> <pre><code>%let start_date = 01Jan1986;\n%let end_date   = 31Dec2019;\n\n/* Generate a series of Fridays */\ndata fridays;\ndate=\"&amp;start_date\"d;\ndo while (date&lt;=\"&amp;end_date\"d);\n    if weekday(date)=6 then output;\n    date=intnx('day', date, 1, 's');\nend;\nformat date date9.;\nrun;\n</code></pre> Weekly index return from daily data (as at Friday)Weekly stock return from daily data (as at Friday)   <pre><code>proc sql;\n/* Compute weekly index return from daily data */\ncreate table mktret_weekly as \nselect distinct a.date,\n    (exp(sum(log(1+sprtrn)))-1)*100 \n        as mktret label=\"Weekly SP500 Index Return (%)\"\nfrom fridays as a left join crsp.dsi as dsi\non dsi.date between intnx('day', a.date, -4) and a.date\ngroup by a.date\norder by a.date;\nquit;\n</code></pre>   <p>Note that this version is inefficient and takes a long time to run.</p> <pre><code>proc sql;\n/* Stocks (ordinary shares) in the financial sector (2-digit SIC=60-67) */\ncreate table stocks as select distinct permno from crsp.stocknames\nwhere shrcd in (10, 11) and floor(siccd/100) between 60 and 67;\n\n/* Compute weekly stock return from daily data */\ncreate table stockrets_weekly as \nselect distinct a.date, dsf.permno, dsf.hsiccd,\n    (exp(sum(log(1+ret)))-1)*100 as ret label=\"Weekly Return (%)\"\nfrom fridays as a left join crsp.dsf as dsf\non dsf.date between intnx('day', a.date, -4) and a.date\n    and dsf.permno in (select * from stocks) \n    and dsf.prc&gt;0 and not missing(dsf.ret)\ngroup by dsf.permno, a.date\norder by dsf.permno, a.date;\nquit;\n</code></pre>","title":"1. Start with a list of dates (slow version)"},{"location":"posts/compute-weekly-return-from-daily-crsp-data/#2-group-using-aligned-dates-fast-version-with-caveat","text":"<p>This version uses a similar logic from the previous incorrect one, but it groups based on the aligned dates instead of <code>year(date)</code> and <code>week(date)</code>.</p> <pre><code>proc sql;\n/* Compute weekly stock return from daily data */\ncreate table stockrets_weekly2 as \nselect distinct permno, hsiccd,\n    case when weekday(date)=6 then date else intnx(\"week.6\",date,1) end \n        as date format=date9. label=\"Friday of the Week\",\n    (exp(sum(log(1+ret)))-1)*100 as ret label=\"Weekly Return (%)\"\nfrom crsp.dsf (keep=permno date ret prc shrout hsiccd)\nwhere \n    date between \"01Jan1986\"d and \"31Dec2019\"d\n    and permno in (select * from stocks) \n    and prc&gt;0 and not missing(ret)\ngroup by permno, calculated date order by permno, date;\nquit;\n</code></pre>  <p>Caveat</p> <p>If the beginning and ending dates, <code>\"01Jan1986\"d and \"31Dec2019\"d</code> in the  example, are not Fridays, then the first and last weekly returns for all  stocks will be incorrect, because they are not using all the daily data in  those weeks.</p> <p>To fix this minor issue, simply extand the beginning and ending dates beyond your sample period by a few weeks.</p>","title":"2. Group using aligned dates (fast version with caveat)"},{"location":"posts/convert-between-numeric-and-character-variables/","text":"<p>Converting between numeric and character variables is one of the most frequently encountered issues when processing datasets. This article explains how to do this conversion correctly and efficiently.</p>","title":"Convert Between Numeric and Character Variable"},{"location":"posts/convert-between-numeric-and-character-variables/#numeric-to-character","text":"<p>Assume there's an imported dataset named <code>filings</code>, where <code>cik</code> is stored as a numeric variable as shown below:</p>    cik file_type date     1000229 8-K 2011-09-30   100591 8-K 2006-05-11   100826 8-K 2009-06-30   93542 8-K 2007-01-25    <p>Because <code>cik</code> is of different digits, to convert the numeric <code>cik</code> into a character variable, the natural procedure is to pad it with leading zeros. For example, <code>cik</code> (Central Index Key) itself is a 10-digit number used by SEC.</p> <p>In SAS, convert numeric variable to string with leading zeros (assuming 10-digit fixed length) is done via <code>PUT()</code> function:</p> <pre><code>data filings(drop=cik); set filings;\n    cik_char = put(cik, z10.); \nrun;\n</code></pre>  <p>Tip</p> <p><code>PUT()</code> function also works in <code>PROC SQL</code>.</p>  <p>The generated <code>cik_char</code> variable is of format and informat <code>$10.</code>, and the dataset becomes:</p>    cik_char file_type date     0001000229 8-K 2011-09-30   0000100591 8-K 2006-05-11   0000100826 8-K 2009-06-30   0000093542 8-K 2007-01-25    <p>In STATA, convert numeric variable to string with leading zeros (assuming 6-digit fixed length) can be achieved via the <code>string()</code> function.</p> <pre><code>gen char_var = string(num_var,\"%06.0f\")\n</code></pre>","title":"Numeric to Character"},{"location":"posts/convert-between-numeric-and-character-variables/#character-to-numeric","text":"<p>In SAS, converting a character variable to a numeric one uses the <code>INPUT()</code> function:</p> <pre><code>var_numeric = input(var_char, best12.);\n</code></pre> <p>In STATA, this conversion be can be done via either <code>real()</code> function or <code>destring</code> command.</p> <pre><code>gen num_var = real(char_var);\n</code></pre> <p>The <code>real()</code> function works on a single variable. <code>destring</code> command can convert all character variables into numeric in one go.</p> <pre><code>destring, repalce\n</code></pre>  <p>Warning</p> <p>If a character variable has non-numeric characters in it, then it will not be converted. In such a case, you may choose to use the <code>encode</code> command, although it in fact is generating categories.</p>  <p>A more detailed explanation with examples is available at stats.idre.ucla.edu</p>","title":"Character to Numeric"},{"location":"posts/decomposing-hhi-index/","text":"<p>Herfindahl\u2013Hirschman (HHI) Index is a well-known market concentration measure determined by two factors: </p> <ol> <li>the size distribution (variance) of firms, and</li> <li>the number of firms. </li> </ol> <p>Intuitively, having a hundred similar-sized gas stations in town means a far less concentrated environment than just one or two available, and when the number of firms is constant, their size distribution or variance determines the magnitude of market concentration.</p> <p>Since these two properties jointly determine the HHI measure of concentration, naturally we want a decomposition of HHI that can reflects these two dimensions respectively. This is particularly useful when two distinct markets have the same level of HHI measure, but the concentration may result from different sources. Note that here these two markets do not necessarily have to be industry A vesus industry B, but can be the same industry niche in two geographical areas, for example.</p> <p>Thus, we can think of HHI as the sum of the actual marekt state's deviation from 1) all firms having the same size, and the deviation from 2) a fully competitive environment with infinite number of firms in the market. Some simple math can solve our problem.</p>","title":"Decomposing Herfindahl\u2013Hirschman (HHI) Index"},{"location":"posts/decomposing-hhi-index/#some-math","text":"<p>Let's say in a market ther are n firms sized x_1, x_2, ... x_nx_1, x_2, ... x_n, thus we can describe the market using a \\mathbb R_+^n\\mathbb R_+^n vector:</p>  \\mathbf{x}=(x_1, x_2, ... x_n) \\mathbf{x}=(x_1, x_2, ... x_n)  <p>In the first scenario where all firms' sizes are equal, we can describe it with:</p>  \\mathbf{\\bar{x}}=(\\bar{x}, \\bar{x}, ... \\bar{x})  \\mathbf{\\bar{x}}=(\\bar{x}, \\bar{x}, ... \\bar{x})   <p>where \\bar{x}=\\frac{1}{n} \\sum_{i=1}^{n}x_i\\bar{x}=\\frac{1}{n} \\sum_{i=1}^{n}x_i is the average firm size.</p> <p>The Euclidean distance between the point \\mathbf{x}\\mathbf{x} and \\mathbf{\\bar{x}}\\mathbf{\\bar{x}}, denoted as d(\\mathbf{x}, \\mathbf{\\bar{x}})d(\\mathbf{x}, \\mathbf{\\bar{x}}), is thus</p>  d(\\mathbf{x}, \\mathbf{\\bar{x}})=\\sqrt{ \\sum_{i=1}^{n} x_{i}^2 - n \\bar{x}^2 }  d(\\mathbf{x}, \\mathbf{\\bar{x}})=\\sqrt{ \\sum_{i=1}^{n} x_{i}^2 - n \\bar{x}^2 }   <p>For the ease of discussion, let's consider the other spectrum of the second scenario where there's only one firm in the market instead of infinite firms, assuming its size is the sum of all firms in the first scenario (i.e. its size is n\\bar{x}n\\bar{x}), we know that this market is the most concentrated state, \\mathbf{x^*}\\mathbf{x^*}. In other words, its distance to the market state in scenario one is the largest.</p>   \\max_{x} d(\\mathbf{x}, \\mathbf{\\bar{x}})=d(\\mathbf{x^*}, \\mathbf{\\bar{x}}) = ... = \\sqrt{ (n-1)n \\bar{x}^2 }  \\max_{x} d(\\mathbf{x}, \\mathbf{\\bar{x}})=d(\\mathbf{x^*}, \\mathbf{\\bar{x}}) = ... = \\sqrt{ (n-1)n \\bar{x}^2 }  <p>Hence, the distance of any market state \\mathbf{x}\\mathbf{x} to the first scenario, the equidistribution point \\mathbf{\\bar{x}}\\mathbf{\\bar{x}}, should lie between 00 to d(\\mathbf{x^*}, \\mathbf{\\bar{x}})d(\\mathbf{x^*}, \\mathbf{\\bar{x}}). </p> <p>Thus we can derive a relative index of concentration (when n&gt;1n&gt;1) as \\tau\\tau:</p>   \\tau=\\frac{ d(\\mathbf{x}, \\mathbf{\\bar{x}}) }{ d(\\mathbf{x^*}, \\mathbf{\\bar{x}}) } \\in [0, 1]  \\tau=\\frac{ d(\\mathbf{x}, \\mathbf{\\bar{x}}) }{ d(\\mathbf{x^*}, \\mathbf{\\bar{x}}) } \\in [0, 1]  <p>Now, given the definition of Herfindahl-Hirschman Index HH that</p>  H=\\sum_{i=1}^{n} (\\frac{x_i}{n\\bar{x}})^2 H=\\sum_{i=1}^{n} (\\frac{x_i}{n\\bar{x}})^2  <p>we can get:</p>  \\tau=\\sqrt{\\frac{n}{n-1}(H-\\frac{1}{n})} = \\sqrt{\\frac{nH-1}{n-1}} \\tau=\\sqrt{\\frac{n}{n-1}(H-\\frac{1}{n})} = \\sqrt{\\frac{nH-1}{n-1}}  <p>Here comes the important implications. Recall that \\tau\\tau represents the ratio of the distance between a market state and the equidistribution point to the maximum possible distance given a total market size of n\\bar{x}n\\bar{x}. </p> <p>When we observe a market state \\mathbf{x}=(x_1, x_2, ... x_n)\\mathbf{x}=(x_1, x_2, ... x_n) at a given time, the total market size is fixed and thus \\tau\\tau is only varying with the distance between the observed actual market state and the equidistribution state where all firms have the same size. This implies that \\tau\\tau could be a measure of the first determinant of market concentration, i.e. the size distribution (variance) of firms.</p> <p>Further, \\tau\\tau represents a sequence of functions whose limit is \\sqrt{H}\\sqrt{H} as n \\to +\\inftyn \\to +\\infty, when the market is in a fully competitive environment. Thus, given a H'H' from the knowledge of n'n' and \\mathbf{x'}\\mathbf{x'}, we know there is one and only one matching \\tau'\\tau' and its limit of \\sqrt{H'}\\sqrt{H'} in the fully competitive environment.</p> <p>The graph below shows that HH can therefore be decomposed into two components, that is</p>   H = E_i + E_n   H = E_i + E_n   <p>where E_i = \\tau^2E_i = \\tau^2, and E_n = H-\\tau^2E_n = H-\\tau^2.</p> <p>We mentioned before that \\tau\\tau can be measure of the market concentration resulted from the size distribution (variance) of firms, such that E_i=\\tau^2E_i=\\tau^2 can be an even better one since it's smaller than HH, which enables us to measure the concentration contributed from the number of firms, measured by E_nE_n.</p> <p></p> <p>This decomposition is appealing also in that E_nE_n, from the graph above, effectively is the horizontal difference between the two curves, i.e. the 'distance' between the actual market state and the fully competitive market with infinite number of firms (scenario two). </p> <p>Thus, it's safe to say this decomposition produces two components explaining the observed market concentration, 1) E_iE_i, the inequality of firm sizes effect, and 2) E_nE_n, the number of firms effect.</p> <p>Another finding from the graph is that with higher market concentration measured by HH, the relative importance of the two components is changing. </p> <ul> <li> <p>When HH is small, most of the concentration is resulted from E_nE_n as   highlighted below, which means the number of firms has a greater impact on   market concentration.</p> </li> <li> <p>When HH is larger, on the other hand, E_iE_i contributes more to HH, which   means the firm size inequality plays a bigger role in market concentration.</p> </li> </ul> <p></p> <p>A potential implication for regulators who are concerned about market concentration, I think, is to 1) focus more on reducing the entry barrier if the current concentration level is moderate, and to 2) focus more on antitrust if the concentration level is already high.</p> <p>Another implication for researchers is that even though H \\in [\\frac{1}{n}, 1]H \\in [\\frac{1}{n}, 1] is affected by the number of firms in a market, we should not attempt to use the \\text{normalized HHI}=\\frac{H-1/n}{1-1/n} \\in [0,1]\\text{normalized HHI}=\\frac{H-1/n}{1-1/n} \\in [0,1]. The reason is now very simple and clear -- the normalized HHI is nothing but E_i=\\tau^2E_i=\\tau^2, which reflects only the market concentration due to the inequality of firm sizes. When we compare across markets or the same market over time, apparently a market with 1,000 firms has a different competitive landscape than a market with only 2 firms.</p>","title":"Some math"},{"location":"posts/decomposing-hhi-index/#acknowledgement","text":"<p>This post is largely a replicate of the paper \"A Decomposition of the Herfindahl Index of Concentration\" by Giacomo de Gioia in 2017.</p>","title":"Acknowledgement"},{"location":"posts/docker-nginx-letsencrypt/","text":"<p>This is a note for setting up a Docker, Nginx and Let's Encrypt environment on Ubuntu 20.04 LTS.</p>","title":"Setup Docker/Ngnix and Let's Encrypt on Ubuntu"},{"location":"posts/docker-nginx-letsencrypt/#create-a-ubuntu-2004-lts-instance","text":"","title":"Create a Ubuntu 20.04 LTS instance"},{"location":"posts/docker-nginx-letsencrypt/#install-docker-using-the-convenience-script","text":"<pre><code>$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\n</code></pre>","title":"Install Docker using the convenience script"},{"location":"posts/docker-nginx-letsencrypt/#manage-docker-as-a-non-root-user","text":"<p>If you don't want to preface the <code>docker</code> command with <code>sudo</code>, create a Unix group called <code>docker</code> and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the <code>docker</code> group.</p> <p>To create the <code>docker</code> group and add your user:</p> <ol> <li>Create the <code>docker</code> group.</li> </ol> <pre><code>$ sudo groupadd docker\n</code></pre> <ol> <li>Add your user to the <code>docker</code> group.</li> </ol> <pre><code>$ sudo usermod -aG docker $USER\n</code></pre> <ol> <li> <p>Log out and log back in so that your group membership is re-evaluated.</p> <p>On Linux, you can also run the following command to activate the changes to groups:</p> </li> </ol> <pre><code>$ newgrp docker \n</code></pre>","title":"Manage Docker as a non-root user"},{"location":"posts/docker-nginx-letsencrypt/#configure-docker-to-start-on-boot","text":"<pre><code>$ sudo systemctl enable docker\n</code></pre> <p>To disable this behavior, use <code>disable</code> instead.</p> <pre><code>$ sudo systemctl disable docker\n</code></pre>","title":"Configure Docker to start on boot"},{"location":"posts/docker-nginx-letsencrypt/#install-docker-compose","text":"<p>On Linux, you can download the Docker Compose binary from the Compose repository release page on GitHub. Follow the instructions from the link, which involve running the <code>curl</code> command in your terminal to download the binaries. These step-by-step instructions are also included below.</p> <ol> <li>Run this command to download the current stable release of Docker Compose:</li> </ol> <pre><code>$ sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n</code></pre>  <p>Note</p> <p>To install a different version of Compose, substitute <code>1.25.5</code> with the version of Compose you want to use.</p>  <ol> <li>Apply executable permissions to the binary:</li> </ol> <pre><code>$ sudo chmod +x /usr/local/bin/docker-compose\n</code></pre>  <p>Note</p> <p>If the command <code>docker-compose</code> fails after installation, check your path. You can also create a symbolic link to <code>/usr/bin</code> or any other directory in your path. For example:</p> <pre><code>$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n</code></pre>","title":"Install Docker Compose"},{"location":"posts/docker-nginx-letsencrypt/#set-up-nginx-proxy","text":"<p>Create a unique network for nginx-proxy and other Docker containers to communicate through.</p> <pre><code>$ docker network create nginx-proxy\n</code></pre> <p>Create a directory <code>nginx-proxy</code> for the compose file.</p> <pre><code>$ mkdir nginx-proxy &amp;&amp; cd nginx-proxy\n</code></pre> <p>In the nginx-proxy directory, create a new file named <code>docker-compose.yml</code> and paste in the following text:</p>  example <code>docker-compose.yml</code> for nginx-proxy <pre><code>version: '3'\n\nservices:\n  nginx:\n    image: nginx\n    restart: always\n    container_name: nginx-proxy\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - conf:/etc/nginx/conf.d\n      - vhost:/etc/nginx/vhost.d\n      - html:/usr/share/nginx/html\n      - certs:/etc/nginx/certs\n    labels:\n      - \"com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy=true\"\n\n  dockergen:\n    image: jwilder/docker-gen\n    restart: always\n    container_name: nginx-proxy-gen\n    depends_on:\n      - nginx\n    command: -notify-sighup nginx-proxy -watch -wait 5s:30s /etc/docker-gen/templates/nginx.tmpl /etc/nginx/conf.d/default.conf\n    volumes:\n      - conf:/etc/nginx/conf.d\n      - vhost:/etc/nginx/vhost.d\n      - html:/usr/share/nginx/html\n      - certs:/etc/nginx/certs\n      - /var/run/docker.sock:/tmp/docker.sock:ro\n      - ./nginx.tmpl:/etc/docker-gen/templates/nginx.tmpl:ro\n\n  letsencrypt:\n    image: jrcs/letsencrypt-nginx-proxy-companion\n    restart: always\n    container_name: nginx-proxy-le\n    depends_on:\n      - nginx\n      - dockergen\n    environment:\n      NGINX_PROXY_CONTAINER: nginx-proxy\n      NGINX_DOCKER_GEN_CONTAINER: nginx-proxy-gen\n    volumes:\n      - conf:/etc/nginx/conf.d\n      - vhost:/etc/nginx/vhost.d\n      - html:/usr/share/nginx/html\n      - certs:/etc/nginx/certs\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n\nvolumes:\n  conf:\n  vhost:\n  html:\n  certs:\n\nnetworks:\n  default:\n    external:\n      name: nginx-proxy\n</code></pre>  <p>Inside of the <code>nginx-proxy</code> directory, use the following <code>curl</code> command to copy the developer\u2019s sample <code>nginx.tmpl</code> file to your VPS.</p> <pre><code>$ curl https://raw.githubusercontent.com/jwilder/nginx-proxy/master/nginx.tmpl &gt; nginx.tmpl\n</code></pre>  <p>Increase upload file size</p> <p>To increase the maximum upload size, for example, add <code>client_max_body_size 100M;</code> to the <code>server{}</code> section in the <code>nginx.tmpl</code> template file. For WordPress, </p>  <p>Running <code>nginx-proxy</code>.</p> <pre><code>$ docker-compose up -d\n</code></pre>","title":"Set up Nginx-Proxy"},{"location":"posts/docker-nginx-letsencrypt/#add-a-wordpress-container","text":"<p>Create a directory for the <code>docker-compose.yml</code> with:</p>  example <code>docker-compose.yml</code> for WordPress container <pre><code>version: \"3\"\n\nservices:\n  db_node_domain:\n    image: mysql:5.7\n    volumes:\n        - db_data:/var/lib/mysql\n    restart: always\n    environment:\n        MYSQL_ROOT_PASSWORD: somewordpress\n        MYSQL_DATABASE: wordpress\n        MYSQL_USER: wordpress\n        MYSQL_PASSWORD: wordpress\n    container_name: wp_test_db\n\n  wordpress:\n    depends_on:\n        - db_node_domain\n    image: wordpress:latest\n    expose:\n        - 80\n    restart: always\n    environment:\n        VIRTUAL_HOST: blog.example.com\n        LETSENCRYPT_HOST: blog.example.com\n        LETSENCRYPT_EMAIL: foo@example.com\n        WORDPRESS_DB_HOST: db_node_domain:3306\n        WORDPRESS_DB_USER: wordpress\n        WORDPRESS_DB_PASSWORD: wordpress\n    container_name: wp_test\nvolumes:\n  db_data:\n\nnetworks:\n  default:\n    external:\n      name: nginx-proxy\n</code></pre>  <p>To create a second WordPress container, add <code>MYSQL_TCP_PORT</code> environment variable and set it to a different port.</p>","title":"Add a WordPress container"},{"location":"posts/docker-nginx-letsencrypt/#increase-maximum-wordpress-upload-file-size","text":"<p>Enter the bash of the WordPress container.</p> <pre><code>$ docker exec -t wordpress_container_name bash\n</code></pre> <p>Move inside your /var/www/html directory (already there if you\u2019re using the standard Docker Compose image). Run the following command to insert the values.</p> <pre><code>$ sed -i '/^# END WordPress.*/i php_value upload_max_filesize 256M\\nphp_value post_max_size 256M' .htaccess\n</code></pre>  <p>Note</p> <p>To restore the values, run <code>$ sed -i \"11,12d\" .htaccess</code></p>","title":"Increase maximum WordPress upload file size"},{"location":"posts/encode-password-for-sas-remote-submission/","text":"<p>The Wharton Research Data Services (WRDS) allows one to submit and execute SAS programs to the cloud. WRDS has an instruction on accessing WRDS data from SAS on our own PCs. Generally, you should use:</p> <pre><code>%let wrds=wrds-cloud.wharton.upenn.edu 4016;\noptions comamid=TCP remote=WRDS;\nsignon username=_prompt_;\n\nrsubmit;\n\n/* Code for remote execution goes here. */\n\nendrsubmit;\nsignoff;\n</code></pre> <p>However, if you want to save the effort of entering username and password every time, you'll need to encode your password. Concluding the two articles, basically you just need to follow the steps below.</p>","title":"Encode Password for SAS Remote Submission"},{"location":"posts/encode-password-for-sas-remote-submission/#simple-steps","text":"<p>First, open your SAS program locally on your PC, run the following command and replace <code>1234567890</code> with your WRDS password:</p> <pre><code>proc pwencode in=\"1234567890\"; run;\n</code></pre> <p>The output <code>{SAS002}23AA9C2811439227077603C8365060A44800CA1F</code> is the encoded password (which is <code>1234567890</code> in this example).</p>  <p>Do NOT share your SAS program with encoded password!</p> <p>Encoded password functions the same as your plain-text password. You should never make public your password in any way.</p>  <p>Next, put the following statements at the beginning of your SAS program and replace <code>my_username</code> with your WRDS username:</p> <pre><code>%let wrds=wrds-cloud.wharton.upenn.edu 4016;\noptions comamid=TCP remote=WRDS;\nsignon username=my_username password=\"{SAS002}23AA9C2811439227077603C8365060A44800CA1F\";\n</code></pre> <p>After these statements, you'll be able to submit your SAS program remotely to and execute on the WRDS server by enclosing your statements with <code>rsubmit</code> and <code>endrsubmit</code>. An example would be:</p> <pre><code>rsubmit;\nproc download data=comp.funda out=funda; run;\nendrsubmit;\n</code></pre> <p>As you can guess, this statement actually downloads the whole Compustat Fundamentals Annual to the local work directory, with the downloaded dataset also named <code>funda</code>.</p> <p>Lastly, after everything, you should run <code>signoff</code> to close the connection with WRDS.</p> <p>Full code is as below.</p> <pre><code>%let wrds=wrds-cloud.wharton.upenn.edu 4016;\noptions comamid=TCP remote=WRDS;\nsignon username=my_username password=\"{SAS002}23AA9C2811439227077603C8365060A44800CA1F\";\n\nrsubmit;\nproc download data=comp.funda out=funda; run;\nendrsubmit;\nsignoff;\n</code></pre> <p>Replace <code>my_username</code> and the encoded password with your actual WRDS username and encoded password, paste it in the SAS program editor and press <code>F3</code>. You'll be downloading <code>comp.funda</code> in a few seconds!</p>","title":"Simple Steps"},{"location":"posts/encode-password-for-sas-remote-submission/#video-instruction","text":"<p>I made a short video introduction as well, available on my YouTube channel.</p>","title":"Video Instruction"},{"location":"posts/estimate-organization-capital/","text":"<p>As in Eisfeldt and Papanikolaou (2013), we obtain firm-year accounting data from the Compustat and compute the stock of organization capital for firms using the perpetual inventory method that recursively calculates the stock of OC by accumulating the deflated value of SG&amp;A expenses.</p>","title":"Estimate Organization Capital"},{"location":"posts/estimate-organization-capital/#organization-capital","text":"<p>$$ OC_{i,t} = (1-\\delta_{OC})OC_{i,t-1} + \\frac{SGA_{i,t}}{CPI_t} $$ where SGA_{i,t} is firm ii's SG&amp;A expenses in year tt, CPI_tCPI_t is the consumer price index, and \\delta_{OC}\\delta_{OC} is the depreciation rate of OC stock, which is set to be 15% as used by the U.S. Bureau of Economic Analysis (BEA). The initial value of OC stock is set to: $$ OC_{i,0} = \\frac{SGA_{i,1}}{g+\\delta_{OC}} $$ where gg is the average real growth rate of firm-level SG&amp;A expenses, which is 10% in Eisfeldt and Papanikolaou (2013) or specific for an industry-decade in Li, Qiu and Shen (2018).</p>","title":"Organization Capital"},{"location":"posts/estimate-organization-capital/#code","text":"<p>This code estimates the organization capital for all Compustat firm-years.</p> <p>Note that it requires an external dataset of CPI. You need to name it <code>cpiaucsl</code> and store it in your WRDS home directory.</p> <pre><code>%let wrds=wrds-cloud.wharton.upenn.edu 4016;\noptions comamid=TCP remote=WRDS;\nsignon username=_prompt_;\n\nrsubmit;\n\n/* ==============================================================================================\n * This SAS program calcualtes the firm-year Organization Capital, measured by the capitalized \n *      SG&amp;A expenses using perpetual inventory method.\n * See e.g. Eisfeldt and Papanikolaou (2013), Li, Qiu and Shen (2018), Gao, Leung and Qiu (2021).\n *\n * Input: Compustat from WRDS.\n * Output:\n *  sgastock: capitalized SG&amp;A expenses\n *  oc: capitalized SG&amp;A expenses scaled by CPI-adjusted total assets\n *  indadj_oc: industry median adjusted oc\n *  rank_oc: annual decile rank of oc\n *  rank_indadj_oc: annual decile rank of indadj_oc\n *\n * Note:\n *  This program requires an external dataset of CPI named `cpiaucsl` in your home directory.\n *  I use the Consumer Price Index for All Urban Consumers: All Items (CPIAUCSL)\n *  sourced from Federal Reserve Bank of St.Louis,\n *  available at https://fred.stlouisfed.org/series/CPIAUCSL/\n *  Also, the industry-adjustment is based on sich from compustat only.\n *  This code may contain error. Please check before use.\n *\n * Author: Mingze (Adrian) Gao\n * mingze.gao@sydney.edu.au\n *\n * Last Modifed: 24 Feb 2019\n * ============================================================================================== */\n\nlibname home \"~/\";\n\ndata funda(keep=gvkey cusip cik fyear datadate at xsga xrd xad sic2);\n    /*  Variables from Compustat:\n    *   AT:     Assets Total;\n    *   XSGA:   Selling, General and Administrative Expense;\n    *   XRD:    Research and Development Expense;\n    *   XAD:    Advertising Expense; */\n    set comp.funda;\n    if cmiss(of fyear datadate)=0;\n    if indfmt = 'INDL' and datafmt='STD' and popsrc='D' and consol='C';\n    sic2 = int(sich/100);\nrun;\n\nproc sql;\n    /* Keep only obs from the first year with non-missing XSGA */\n    create table funda_nonmissing_xsga as\n        select distinct a.*\n        from funda as a left join \n            /* This subquery selects the first year of appearance \n                with non-missing XSGA */\n            (select gvkey, fyear as firstfyear from funda \n             where xsga is not missing \n             group by gvkey having fyear = min(fyear)) as b\n        on a.gvkey=b.gvkey\n        where a.fyear&gt;=b.firstfyear; \n\n    /* CPIAUCSL: Consumer Price Index for All Urban Consumers: All Items\n        Source: https://fred.stlouisfed.org/series/CPIAUCSL/ */\n    create table funda_cpi as\n        select distinct a.*, b.cpiaucsl as cpi\n        from funda_nonmissing_xsga as a left join home.cpiaucsl as b\n        on year(a.datadate) = year(b.date) and month(a.datadate) = month(b.date)\n        order by gvkey, fyear;\nquit;\n\n/* Sanity Check -- No Duplicates */\nproc sort nodupkey data=funda_cpi; \n    by gvkey fyear;\nrun;\n\ndata funda_adj;\n    set funda_cpi;\n    by gvkey fyear;\n    /* Replace missing XSGA, XRD and XAD with 0 */\n    if xsga=. then xsga=0;\n    if xrd=. then xrd=0;\n    if xad=. then xad=0;\n    /* Total assets adjusted for CPI */\n    adjat = at / cpi;\n    /* Two alternative SG&amp;A measures */\n    adjxsga1 = xsga / cpi;\n    adjxsga2 = sum(xsga, -xrd, -xad) / cpi;\nrun;\n\ndata sgastock(drop=cnt adjxsga1 adjxsga2 lag:);\n    set funda_adj(keep=gvkey cik cusip datadate fyear sic2 adj:);\n    by gvkey;\n    if first.gvkey then call missing(of cnt lag:);\n    cnt+1;\n    array adjxsga adjxsga1-adjxsga2;\n    array sgastock sgastock1-sgastock2;\n    array sgastock_r sgastock_r1-sgastock_r2;\n    array lag_sgastock lag_sgastock1-lag_sgastock2;\n    select (cnt);\n        when (1) do;\n            /* Under Perpetual Inventory Method, \n            * the initial value of capitalized SG&amp;A at time 0, O(0), is:\n            *   O(0)=O(1)/(g+delta)\n            * where g is average SGA growth rate (10%) and delta is depreciation rate (15%).\n            * So that,\n            *   O(0)=SGA(1)/(0.15+0.1)=SGA(1)/0.25=SGA(1)*4\n            * This is why `adjxsga*4` is used below, specifically,\n            *   O(1)=O(0)*0.85+SGA(1)\n            *       =SGA(0)*4*0.85+SGA(1) */\n            do over sgastock;\n                sgastock = (adjxsga * 4)* 0.85 + adjxsga; end;\n        end;\n        otherwise do;\n            /* When t&gt;1,\n            * the capitalized SG&amp;A at time t, O(t), is:\n            *   O(t)=O(t-1)*(1-delta)+SGA(t)\n            * where g is average SGA growth rate (10%) and delta is depreciation rate (15%).\n            * Note that here SG&amp;A is adjusted for CPI. */\n            do over sgastock;\n                sgastock = lag_sgastock * 0.85 + adjxsga; end;\n        end;\n    end;\n    do over sgastock;\n        lag_sgastock = sgastock;\n        /* `sgastock_r` is sgastock scaled by adjusted total assets. */\n        sgastock_r = sgastock / adjat;\n        if adjat=. then sgastock_r=0;\n    end;\n    output;\n    retain lag:;\nrun;\n\n/* industry-adjusted OC and rank-based OC measures */\nproc sql;\n    create table tmp as\n        select gvkey, cik, cusip, datadate, fyear, sic2,\n            sgastock_r1 as oc1,\n            sgastock_r2 as oc2,\n            sgastock_r1 - median(sgastock_r1) as indadj_oc1,\n            sgastock_r2 - median(sgastock_r2) as indadj_oc2\n        from sgastock\n        group by fyear, sic2\n        order by gvkey, fyear;\nquit;\nproc sort data=tmp; by fyear; run;\nproc rank data=tmp out=result groups=10;\n    by fyear;\n    var oc1 oc2 indadj_oc1 indadj_oc2;\n    ranks rank_oc1 rank_oc2 rank_indadj_oc1 rank_indadj_oc2;\nrun;\n\ndata download(compress=yes); set work.result; run;\nproc download data=work.download out=sgastock; run;\n\nendrsubmit;\nsignoff;\n</code></pre> <p>Lastly, if you use this code above, please consider citing the following article for which it was written.</p>  <p>Gao, M. Leung, H. and Qiu, B. (2021). Organization Capital and Executive Performance Incentives, Journal of Banking &amp; Finance, 123, 106017.</p>","title":"Code"},{"location":"posts/firm-historical-headquarter-state-from-10k/","text":"","title":"Firm Historical Headquarter State from SEC 10K/Q Filings"},{"location":"posts/firm-historical-headquarter-state-from-10k/#why-the-need-to-use-sec-filings","text":"<p>In the Compustat database, a firm's headquarter state (and other identification) is in fact the current record stored in <code>comp.company</code>. This means once a firm relocates (or updates its incorporate state, address, etc.), all historical observations will be updated and not recording historical state information anymore.</p> <p>To resolve this issue, an effective way is to use the firm's historical SEC filings. You can follow my previous post Textual Analysis on SEC filings to extract the header information, which includes a wide range of meta data. Alternatively, the University of Notre Dame's Software Repository for Accounting and Finance provides an augmented 10-X header dataset.</p>","title":"Why the need to use SEC filings?"},{"location":"posts/firm-historical-headquarter-state-from-10k/#do-i-have-to-use-sec-filings","text":"<p>I'll skip the parsing procedure for now. The most important point is that using the historical SEC filings, you can ensure that you truly are using the historical headquarter state in your empirical estimation. Based on the augmented 10-X header dataset, I find that around 2-3% of Compustat firms changed their headquarter state (as indicated by their business address) each year.</p>    Year Firms Changed State Total Firms % Firms Changed State     1995 22 4205 0.52   1996 69 7939 0.87   1997 199 8101 2.46   1998 206 8126 2.54   1999 202 8199 2.46   2000 202 8252 2.45   2001 204 7802 2.61   2002 167 7421 2.25   2003 214 6930 3.09   2004 175 6742 2.6   2005 154 6478 2.38   2006 156 6267 2.49   2007 144 6091 2.36   2008 125 5797 2.16   2009 127 5523 2.3   2010 128 5479 2.34   2011 152 5445 2.79   2012 160 5494 2.91   2013 171 5491 3.11   2014 195 5455 3.57   2015 147 5322 2.76   2016 117 5092 2.3   2017 129 4914 2.63   2018 107 4847 2.21    <p>Moreover, 2,947 out of the 17,221 firms, or about 17% firms changed their headquarter state in the merged sample. This is by no means a small number that can be ignored. So, whenever possible, you should try to use the historical information from past SEC filings' metadata.</p>","title":"Do I have to use SEC filings?"},{"location":"posts/firm-historical-headquarter-state-from-10k/#how-to-get-the-actual-historical-firm-hq-state-using-sec-filings","text":"","title":"How to get the actual historical firm HQ state using SEC filings?"},{"location":"posts/firm-historical-headquarter-state-from-10k/#1969-2003","text":"<p>I start with the firm historical HQ state provided by Bai, Fairhurst and Serfling (2020 RFS). This dataset contains the historical HQ locations from 1969 to 2003, which is based on the SEC filings post 1994 and hand-collected by the authors from the Moody\u2019s Manuals (later Mergent Manuals) and Dun &amp; Bradstreet\u2019s Million Dollar Directory (later bought by Mergent).1</p>","title":"1969 - 2003"},{"location":"posts/firm-historical-headquarter-state-from-10k/#1994-2018","text":"<p>To extend the dataset, I download the augmented 10-X header dataset and use the following Python script to extract the business address (state) filed.</p> <pre><code>import pandas as pd\n\nfilepath = \"~/Downloads/LM_EDGAR_10X_Header_1994_2018.csv\"\n\nif __name__ == \"__main__\":\n\n    df = pd.read_csv(\n        filepath,\n        usecols=[\"cik\", \"file_date\", \"ba_state\"],\n        dtype={\"cik\": str},\n        parse_dates=[\"file_date\"],\n    )\n    # Some `ba_stata` codes are lowercase\n    df[\"ba_state\"] = df[\"ba_state\"].str.upper()\n    # Some  `ba_state` codes are not valid US states\n    df = df[df[\"ba_state\"].str.isalpha() &amp; ~pd.isnull(df[\"ba_state\"])]\n    df.drop_duplicates().to_stata(\n        \"~/Downloads/historical_state_1994_2018.dta\",\n        write_index=False,\n        convert_dates={\"file_date\": \"td\"},\n    )\n</code></pre> <p>The result is a <code>historical_state.dta</code> Stata file like this:</p> <p></p>","title":"1994 - 2018"},{"location":"posts/firm-historical-headquarter-state-from-10k/#1969-2018-merged","text":"<p>Finally, to merge the two datasets together, I imported them into WRDS Cloud and run the following SAS script:</p> <ul> <li>Pre-2003, use Bai, Fairhurst and Serfling (2020 RFS).</li> <li>Post-2003, use the business address as in the header of 10K/Q filings, and the Compustat records if the business address is missing and invalid from parsing the headers.</li> </ul> <pre><code>libname hs \"~/historical_state\";\n\n/* Historical HQ state (1994 to 2018) from augmented 10-X header dataset */\nproc import datafile=\"~/historical_state/historical_state_1994_2018.dta\"\n    out=historical_state_1994_2018 dbms=stata replace;\n/* Historical HQ state (1969 to 2003) from Bai, Fairhurst and Serfling (2020 RFS) */\nproc import datafile=\"~/historical_state/hist_headquarters_Bai_et_al.dta\"\n    out=hist_headquarters_Bai_et_al dbms=stata replace;\n\n/* Build the post-1994 dataset using SEC filings */\nproc sql;\ncreate table funda as \nselect gvkey, cik, datadate, fyear from comp.funda\nwhere indfmt= 'INDL' and datafmt='STD' and popsrc='D' and consol='C'\nand year(datadate) between 1994 and 2018\n/* \"For firms that change fiscal year within a calendar year, \n    we take the last reported date when extracting financial data. \n    This leaves us with one set of observations for each firm (gvkey) in each year.\" \n    -- Pelueger, Siriwardane and Sunderam (2020 QJE) */\ngroup by gvkey, fyear having datadate=max(datadate);\n\ncreate table firm_historical_state as \nselect a.*, b.ba_state as state_sec label=\"State from SEC filings\"\nfrom funda as a left join historical_state as b \non a.cik=b.cik and year(a.datadate)=year(b.file_date) and b.file_date&lt;=a.datadate\ngroup by a.gvkey, a.datadate\n/* use the SEC filing closet to and before the Compustat datadate */\nhaving b.file_date=max(b.file_date);\n\ncreate table historical_state_1994_2018 as\nselect a.*, b.state as state_comp label=\"State from Compustat\"\nfrom firm_historical_state as a left join comp.company as b \non a.gvkey=b.gvkey\norder by a.gvkey, a.datadate;\nquit;\n\n/* Sanity check: no duplicated gvkey-fyear */\nproc sort data=historical_state_1994_2018 nodupkey; by gvkey datadate; run;\n\n\nproc sql;\ncreate table hist_headquarters_Bai_et_al as \nselect put(gvkeyn, z6.) as gvkey, fyear, state \nfrom hist_headquarters_Bai_et_al;\nquit;\n\n/* Stack together the two datasets */\ndata states; \nset hist_headquarters_Bai_et_al \n    historical_state_1994_2018(where=(fyear&gt;2003) keep=gvkey fyear state:);\nrun;\n\nproc sql;\ncreate table hs.corrected_hist_state_1969_2018 as \nselect *, coalesce(state, state_sec, state_comp) as corrected_state\nfrom states where not missing(calculated corrected_state)\norder by gvkey, fyear;\nquit;\n\n/* Sanity check: no duplicated gvkey-fyear */\nproc sort data=hs.corrected_hist_state_1969_2018 nodupkey; by gvkey fyear; run;\n</code></pre>","title":"1969 - 2018 merged"},{"location":"posts/firm-historical-headquarter-state-from-10k/#data-available-for-download","text":"<p>You can download the data I compiled here: corrected_hist_state_1969_2018.dta.zip (1MB).</p>","title":"Data available for download"},{"location":"posts/firm-historical-headquarter-state-from-10k/#suggested-citation","text":"<p>Lastly, if you use the code/data above, please consider citing the following article for which it was written/constructed.</p>  <p>Gao, M. Leung, H. and Qiu, B. (2021). Organization Capital and Executive Performance Incentives, Journal of Banking &amp; Finance, 123, 106017.</p>    <ol> <li> <p>The authors note that \"for our final sample of 115,432 firm-year observations, we find that over the 1969 to 2003 period, 9,847 (87.50%) never relocate, 1,211 (10.76%) relocate once, 178 (1.58%) relocate twice, and 18 (0.16%) relocate three times.\"\u00a0\u21a9</p> </li> </ol>","title":"Suggested citation"},{"location":"posts/generate-fama-french-industry-classification-from-sic/","text":"<p>This STATA program creates the Fama-French industry classification from SIC code.</p>","title":"Generate Fama-French Industry Classification From SIC"},{"location":"posts/generate-fama-french-industry-classification-from-sic/#basic-usage","text":"<pre><code>ffind sic, generate(\u201cFF48\u201d) type(48)\n</code></pre> <p>where sic is SIC code, FF48 is the generated industry variable name, and we are using 48-industry classification. Alternatively, one can choose 5, 10, 12, 17, 30, 38 or 49 industries.</p>","title":"Basic usage"},{"location":"posts/generate-fama-french-industry-classification-from-sic/#full-stata-code","text":"<code>ffind.ado</code> <pre><code>/****************************************\n* ffind.ado\n* Creates variable containing Fama-French\n* industry classification.\n*\n* Author:  Judson Caskey, UCLA\n*          December 9, 2007\n*\n* Revised by Malcolm Wardlaw, Uiversity of Texas at Dallas (November 1, 2011)\n****************************************/\n\ncapture program drop ffind\n\nprogram define ffind\n    version 9.2\n    syntax varlist(min=1 max=1 numeric) [if] [in], Generate(string) Type(numlist max=1 min=1)\n\n    tempvar ftyp\n    tokenize \"`type'\"\n    local `ftyp'=`1'\n\n    * Check if generate is valid variable name\n    capture confirm new variable `generate'\n    if _rc != 0 {\n        di as error \"Variable `generate' is invalid\"\n        exit 111\n        }\n\n    * Check type\n    if ~inlist(``ftyp'',5,10,12,17,30,38,48,49) {\n        di as error \"Type must be 5, 10, 12, 17, 30, 38, 48 or 49\"\n        exit 111\n        }\n\n    * Set industries\n\n    tempvar ffind\n    tokenize \"`varlist'\"\n    local `ffind' \"`1'\"\n\n\n    qui gen `generate'=.\n    label variable `generate' \"Fama-French industry code (``ftyp'' industries)\"\n\n    capture label drop `generate'\n    if ``ftyp''==5 {\n        label define `generate' 1 \"Consumer Durables, NonDurables, Wholesale, Retail, and Some Services (Laundries, Repair Shops)\" 2 \"Manufacturing, Energy, and Utilities\" 3 \"Business Equipment, Telephone and Television Transmission\" 4 \"Healthcare, Medical Equipment, and Drugs\" 5 \"Other -- Mines, Constr, BldMt, Trans, Hotels, Bus Serv, Entertainment, Finance\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,999) | inrange(``ffind'',2000,2399) | inrange(``ffind'',2700,2749) | inrange(``ffind'',2770,2799) | inrange(``ffind'',3100,3199) | inrange(``ffind'',3940,3989) | inrange(``ffind'',2500,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',3630,3659) | inrange(``ffind'',3710,3711) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3900,3939) | inrange(``ffind'',3990,3999) | inrange(``ffind'',5000,5999) | inrange(``ffind'',7200,7299) | inrange(``ffind'',7600,7699)\n        qui replace `generate'=2 if inrange(``ffind'',2520,2589) | inrange(``ffind'',2600,2699) | inrange(``ffind'',2750,2769) | inrange(``ffind'',2800,2829) | inrange(``ffind'',2840,2899) | inrange(``ffind'',3000,3099) | inrange(``ffind'',3200,3569) | inrange(``ffind'',3580,3629) | inrange(``ffind'',3700,3709) | inrange(``ffind'',3712,3713) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3717,3749) | inrange(``ffind'',3752,3791) | inrange(``ffind'',3793,3799) | inrange(``ffind'',3830,3839) | inrange(``ffind'',3860,3899) | inrange(``ffind'',1200,1399) | inrange(``ffind'',2900,2999) | inrange(``ffind'',4900,4949)\n        qui replace `generate'=3 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3622,3622) | inrange(``ffind'',3660,3692) | inrange(``ffind'',3694,3699) | inrange(``ffind'',3810,3839) | inrange(``ffind'',7370,7372) | inrange(``ffind'',7373,7373) | inrange(``ffind'',7374,7374) | inrange(``ffind'',7375,7375) | inrange(``ffind'',7376,7376) | inrange(``ffind'',7377,7377) | inrange(``ffind'',7378,7378) | inrange(``ffind'',7379,7379) | inrange(``ffind'',7391,7391) | inrange(``ffind'',8730,8734) | inrange(``ffind'',4800,4899)\n        qui replace `generate'=4 if inrange(``ffind'',2830,2839) | inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3859) | inrange(``ffind'',8000,8099)\n        qui replace `generate'=5 if missing(`generate') &amp; ~missing(``ffind'')\n        }\n    else if ``ftyp''==10 {\n        label define `generate' 1 \"Consumer NonDurables -- Food, Tobacco, Textiles, Apparel, Leather, Toys\" 2 \"Consumer Durables -- Cars, TV's, Furniture, Household Appliances\" 3 \"Manufacturing -- Machinery, Trucks, Planes, Chemicals, Off Furn, Paper, Com Printing\" 4 \"Oil, Gas, and Coal Extraction and Products\" 5 \"Business Equipment -- Computers, Software, and Electronic Equipment\" 6 \"Telephone and Television Transmission\" 7 \"Wholesale, Retail, and Some Services (Laundries, Repair Shops)\" 8 \"Healthcare, Medical Equipment, and Drugs\" 9 \"Utilities\" 10 \"Other -- Mines, Constr, BldMt, Trans, Hotels, Bus Serv, Entertainment, Finance\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,999) | inrange(``ffind'',2000,2399) | inrange(``ffind'',2700,2749) | inrange(``ffind'',2770,2799) | inrange(``ffind'',3100,3199) | inrange(``ffind'',3940,3989)\n        qui replace `generate'=2 if inrange(``ffind'',2500,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',3630,3659) | inrange(``ffind'',3710,3711) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3900,3939) | inrange(``ffind'',3990,3999)\n        qui replace `generate'=3 if inrange(``ffind'',2520,2589) | inrange(``ffind'',2600,2699) | inrange(``ffind'',2750,2769) | inrange(``ffind'',2800,2829) | inrange(``ffind'',2840,2899) | inrange(``ffind'',3000,3099) | inrange(``ffind'',3200,3569) | inrange(``ffind'',3580,3629) | inrange(``ffind'',3700,3709) | inrange(``ffind'',3712,3713) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3717,3749) | inrange(``ffind'',3752,3791) | inrange(``ffind'',3793,3799) | inrange(``ffind'',3830,3839) | inrange(``ffind'',3860,3899)\n        qui replace `generate'=4 if inrange(``ffind'',1200,1399) | inrange(``ffind'',2900,2999)\n        qui replace `generate'=5 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3622,3622) | inrange(``ffind'',3660,3692) | inrange(``ffind'',3694,3699) | inrange(``ffind'',3810,3839) | inrange(``ffind'',7370,7372) | inrange(``ffind'',7373,7373) | inrange(``ffind'',7374,7374) | inrange(``ffind'',7375,7375) | inrange(``ffind'',7376,7376) | inrange(``ffind'',7377,7377) | inrange(``ffind'',7378,7378) | inrange(``ffind'',7379,7379) | inrange(``ffind'',7391,7391) | inrange(``ffind'',8730,8734)\n        qui replace `generate'=6 if inrange(``ffind'',4800,4899)\n        qui replace `generate'=7 if inrange(``ffind'',5000,5999) | inrange(``ffind'',7200,7299) | inrange(``ffind'',7600,7699)\n        qui replace `generate'=8 if inrange(``ffind'',2830,2839) | inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3859) | inrange(``ffind'',8000,8099)\n        qui replace `generate'=9 if inrange(``ffind'',4900,4949)\n        qui replace `generate'=10 if missing(`generate') &amp; ~missing(``ffind'')\n        }\n    else if ``ftyp''==12 {\n        label define `generate' 1 \"Consumer NonDurables -- Food, Tobacco, Textiles, Apparel, Leather, Toys\" 2 \"Consumer Durables -- Cars, TV's, Furniture, Household Appliances\" 3 \"Manufacturing -- Machinery, Trucks, Planes, Off Furn, Paper, Com Printing\" 4 \"Oil, Gas, and Coal Extraction and Products\" 5 \"Chemicals and Allied Products\" 6 \"Business Equipment -- Computers, Software, and Electronic Equipment\" 7 \"Telephone and Television Transmission\" 8 \"Utilities\" 9 \"Wholesale, Retail, and Some Services (Laundries, Repair Shops)\" 10 \"Healthcare, Medical Equipment, and Drugs\" 11 \"Finance\" 12 \"Other -- Mines, Constr, BldMt, Trans, Hotels, Bus Serv, Entertainment\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,999) | inrange(``ffind'',2000,2399) | inrange(``ffind'',2700,2749) | inrange(``ffind'',2770,2799) | inrange(``ffind'',3100,3199) | inrange(``ffind'',3940,3989)\n        qui replace `generate'=2 if inrange(``ffind'',2500,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',3630,3659) | inrange(``ffind'',3710,3711) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3900,3939) | inrange(``ffind'',3990,3999)\n        qui replace `generate'=3 if inrange(``ffind'',2520,2589) | inrange(``ffind'',2600,2699) | inrange(``ffind'',2750,2769) | inrange(``ffind'',3000,3099) | inrange(``ffind'',3200,3569) | inrange(``ffind'',3580,3629) | inrange(``ffind'',3700,3709) | inrange(``ffind'',3712,3713) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3717,3749) | inrange(``ffind'',3752,3791) | inrange(``ffind'',3793,3799) | inrange(``ffind'',3830,3839) | inrange(``ffind'',3860,3899)\n        qui replace `generate'=4 if inrange(``ffind'',1200,1399) | inrange(``ffind'',2900,2999)\n        qui replace `generate'=5 if inrange(``ffind'',2800,2829) | inrange(``ffind'',2840,2899)\n        qui replace `generate'=6 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3660,3692) | inrange(``ffind'',3694,3699) | inrange(``ffind'',3810,3829) | inrange(``ffind'',7370,7379)\n        qui replace `generate'=7 if inrange(``ffind'',4800,4899)\n        qui replace `generate'=8 if inrange(``ffind'',4900,4949)\n        qui replace `generate'=9 if inrange(``ffind'',5000,5999) | inrange(``ffind'',7200,7299) | inrange(``ffind'',7600,7699)\n        qui replace `generate'=10 if inrange(``ffind'',2830,2839) | inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3859) | inrange(``ffind'',8000,8099)\n        qui replace `generate'=11 if inrange(``ffind'',6000,6999)\n        qui replace `generate'=12 if missing(`generate') &amp; ~missing(``ffind'')\n        }\n\n    else if ``ftyp''==17 {\n        label define `generate' 1 \"Food\" 2 \"Mining and Minerals\" 3 \"Oil and Petroleum Products\" 4 \"Textiles, Apparel &amp; Footware\" 5 \"Consumer Durables\" 6 \"Chemicals\" 7 \"Drugs, Soap, Prfums, Tobacco\" 8 \"Construction and Construction Materials\" 9 \"Steel Works Etc\" 10 \"Fabricated Products\" 11 \"Machinery and Business Equipment\" 12 \"Automobiles\" 13 \"Transportation\" 14 \"Utilities\" 15 \"Retail Stores\" 16 \"Banks, Insurance Companies, and Other Financials\" 17 \"Other\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,199) | inrange(``ffind'',200,299) | inrange(``ffind'',700,799) | inrange(``ffind'',900,999) | inrange(``ffind'',2000,2009) | inrange(``ffind'',2010,2019) | inrange(``ffind'',2020,2029) | inrange(``ffind'',2030,2039) | inrange(``ffind'',2040,2046) | inrange(``ffind'',2047,2047) | inrange(``ffind'',2048,2048) | inrange(``ffind'',2050,2059) | inrange(``ffind'',2060,2063) | inrange(``ffind'',2064,2068) | inrange(``ffind'',2070,2079) | inrange(``ffind'',2080,2080) | inrange(``ffind'',2082,2082) | inrange(``ffind'',2083,2083) | inrange(``ffind'',2084,2084) | inrange(``ffind'',2085,2085) | inrange(``ffind'',2086,2086) | inrange(``ffind'',2087,2087) | inrange(``ffind'',2090,2092) | inrange(``ffind'',2095,2095) | inrange(``ffind'',2096,2096) | inrange(``ffind'',2097,2097) | inrange(``ffind'',2098,2099) | inrange(``ffind'',5140,5149) | inrange(``ffind'',5150,5159) | inrange(``ffind'',5180,5182) | inrange(``ffind'',5191,5191)\n        qui replace `generate'=2 if inrange(``ffind'',1000,1009) | inrange(``ffind'',1010,1019) | inrange(``ffind'',1020,1029) | inrange(``ffind'',1030,1039) | inrange(``ffind'',1040,1049) | inrange(``ffind'',1060,1069) | inrange(``ffind'',1080,1089) | inrange(``ffind'',1090,1099) | inrange(``ffind'',1200,1299) | inrange(``ffind'',1400,1499) | inrange(``ffind'',5050,5052)\n        qui replace `generate'=3 if inrange(``ffind'',1300,1300) | inrange(``ffind'',1310,1319) | inrange(``ffind'',1320,1329) | inrange(``ffind'',1380,1380) | inrange(``ffind'',1381,1381) | inrange(``ffind'',1382,1382) | inrange(``ffind'',1389,1389) | inrange(``ffind'',2900,2912) | inrange(``ffind'',5170,5172)\n        qui replace `generate'=4 if inrange(``ffind'',2200,2269) | inrange(``ffind'',2270,2279) | inrange(``ffind'',2280,2284) | inrange(``ffind'',2290,2295) | inrange(``ffind'',2296,2296) | inrange(``ffind'',2297,2297) | inrange(``ffind'',2298,2298) | inrange(``ffind'',2299,2299) | inrange(``ffind'',2300,2390) | inrange(``ffind'',2391,2392) | inrange(``ffind'',2393,2395) | inrange(``ffind'',2396,2396) | inrange(``ffind'',2397,2399) | inrange(``ffind'',3020,3021) | inrange(``ffind'',3100,3111) | inrange(``ffind'',3130,3131) | inrange(``ffind'',3140,3149) | inrange(``ffind'',3150,3151) | inrange(``ffind'',3963,3965) | inrange(``ffind'',5130,5139)\n        qui replace `generate'=5 if inrange(``ffind'',2510,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',3060,3069) | inrange(``ffind'',3070,3079) | inrange(``ffind'',3080,3089) | inrange(``ffind'',3090,3099) | inrange(``ffind'',3630,3639) | inrange(``ffind'',3650,3651) | inrange(``ffind'',3652,3652) | inrange(``ffind'',3860,3861) | inrange(``ffind'',3870,3873) | inrange(``ffind'',3910,3911) | inrange(``ffind'',3914,3914) | inrange(``ffind'',3915,3915) | inrange(``ffind'',3930,3931) | inrange(``ffind'',3940,3949) | inrange(``ffind'',3960,3962) | inrange(``ffind'',5020,5023) | inrange(``ffind'',5064,5064) | inrange(``ffind'',5094,5094) | inrange(``ffind'',5099,5099)\n        qui replace `generate'=6 if inrange(``ffind'',2800,2809) | inrange(``ffind'',2810,2819) | inrange(``ffind'',2820,2829) | inrange(``ffind'',2860,2869) | inrange(``ffind'',2870,2879) | inrange(``ffind'',2890,2899) | inrange(``ffind'',5160,5169)\n        qui replace `generate'=7 if inrange(``ffind'',2100,2199) | inrange(``ffind'',2830,2830) | inrange(``ffind'',2831,2831) | inrange(``ffind'',2833,2833) | inrange(``ffind'',2834,2834) | inrange(``ffind'',2840,2843) | inrange(``ffind'',2844,2844) | inrange(``ffind'',5120,5122) | inrange(``ffind'',5194,5194)\n        qui replace `generate'=8 if inrange(``ffind'',800,899) | inrange(``ffind'',1500,1511) | inrange(``ffind'',1520,1529) | inrange(``ffind'',1530,1539) | inrange(``ffind'',1540,1549) | inrange(``ffind'',1600,1699) | inrange(``ffind'',1700,1799) | inrange(``ffind'',2400,2439) | inrange(``ffind'',2440,2449) | inrange(``ffind'',2450,2459) | inrange(``ffind'',2490,2499) | inrange(``ffind'',2850,2859) | inrange(``ffind'',2950,2952) | inrange(``ffind'',3200,3200) | inrange(``ffind'',3210,3211) | inrange(``ffind'',3240,3241) | inrange(``ffind'',3250,3259) | inrange(``ffind'',3261,3261) | inrange(``ffind'',3264,3264) | inrange(``ffind'',3270,3275) | inrange(``ffind'',3280,3281) | inrange(``ffind'',3290,3293) | inrange(``ffind'',3420,3429) | inrange(``ffind'',3430,3433) | inrange(``ffind'',3440,3441) | inrange(``ffind'',3442,3442) | inrange(``ffind'',3446,3446) | inrange(``ffind'',3448,3448) | inrange(``ffind'',3449,3449) | inrange(``ffind'',3450,3451) | inrange(``ffind'',3452,3452) | inrange(``ffind'',5030,5039) | inrange(``ffind'',5070,5078) | inrange(``ffind'',5198,5198) | inrange(``ffind'',5210,5211) | inrange(``ffind'',5230,5231) | inrange(``ffind'',5250,5251)\n        qui replace `generate'=9 if inrange(``ffind'',3300,3300) | inrange(``ffind'',3310,3317) | inrange(``ffind'',3320,3325) | inrange(``ffind'',3330,3339) | inrange(``ffind'',3340,3341) | inrange(``ffind'',3350,3357) | inrange(``ffind'',3360,3369) | inrange(``ffind'',3390,3399)\n        qui replace `generate'=10 if inrange(``ffind'',3410,3412) | inrange(``ffind'',3443,3443) | inrange(``ffind'',3444,3444) | inrange(``ffind'',3460,3469) | inrange(``ffind'',3470,3479) | inrange(``ffind'',3480,3489) | inrange(``ffind'',3490,3499)\n        qui replace `generate'=11 if inrange(``ffind'',3510,3519) | inrange(``ffind'',3520,3529) | inrange(``ffind'',3530,3530) | inrange(``ffind'',3531,3531) | inrange(``ffind'',3532,3532) | inrange(``ffind'',3533,3533) | inrange(``ffind'',3534,3534) | inrange(``ffind'',3535,3535) | inrange(``ffind'',3536,3536) | inrange(``ffind'',3540,3549) | inrange(``ffind'',3550,3559) | inrange(``ffind'',3560,3569) | inrange(``ffind'',3570,3579) | inrange(``ffind'',3580,3580) | inrange(``ffind'',3581,3581) | inrange(``ffind'',3582,3582) | inrange(``ffind'',3585,3585) | inrange(``ffind'',3586,3586) | inrange(``ffind'',3589,3589) | inrange(``ffind'',3590,3599) | inrange(``ffind'',3600,3600) | inrange(``ffind'',3610,3613) | inrange(``ffind'',3620,3621) | inrange(``ffind'',3622,3622) | inrange(``ffind'',3623,3629) | inrange(``ffind'',3670,3679) | inrange(``ffind'',3680,3680) | inrange(``ffind'',3681,3681) | inrange(``ffind'',3682,3682) | inrange(``ffind'',3683,3683) | inrange(``ffind'',3684,3684) | inrange(``ffind'',3685,3685) | inrange(``ffind'',3686,3686) | inrange(``ffind'',3687,3687) | inrange(``ffind'',3688,3688) | inrange(``ffind'',3689,3689) | inrange(``ffind'',3690,3690) | inrange(``ffind'',3691,3692) | inrange(``ffind'',3693,3693) | inrange(``ffind'',3694,3694) | inrange(``ffind'',3695,3695) | inrange(``ffind'',3699,3699) | inrange(``ffind'',3810,3810) | inrange(``ffind'',3811,3811) | inrange(``ffind'',3812,3812) | inrange(``ffind'',3820,3820) | inrange(``ffind'',3821,3821) | inrange(``ffind'',3822,3822) | inrange(``ffind'',3823,3823) | inrange(``ffind'',3824,3824) | inrange(``ffind'',3825,3825) | inrange(``ffind'',3826,3826) | inrange(``ffind'',3827,3827) | inrange(``ffind'',3829,3829) | inrange(``ffind'',3830,3839) | inrange(``ffind'',3950,3955) | inrange(``ffind'',5060,5060) | inrange(``ffind'',5063,5063) | inrange(``ffind'',5065,5065) | inrange(``ffind'',5080,5080) | inrange(``ffind'',5081,5081)\n        qui replace `generate'=12 if inrange(``ffind'',3710,3710) | inrange(``ffind'',3711,3711) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3792,3792) | inrange(``ffind'',5010,5015) | inrange(``ffind'',5510,5521) | inrange(``ffind'',5530,5531) | inrange(``ffind'',5560,5561) | inrange(``ffind'',5570,5571) | inrange(``ffind'',5590,5599)\n        qui replace `generate'=13 if inrange(``ffind'',3713,3713) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3720,3720) | inrange(``ffind'',3721,3721) | inrange(``ffind'',3724,3724) | inrange(``ffind'',3725,3725) | inrange(``ffind'',3728,3728) | inrange(``ffind'',3730,3731) | inrange(``ffind'',3732,3732) | inrange(``ffind'',3740,3743) | inrange(``ffind'',3760,3769) | inrange(``ffind'',3790,3790) | inrange(``ffind'',3795,3795) | inrange(``ffind'',3799,3799) | inrange(``ffind'',4000,4013) | inrange(``ffind'',4100,4100) | inrange(``ffind'',4110,4119) | inrange(``ffind'',4120,4121) | inrange(``ffind'',4130,4131) | inrange(``ffind'',4140,4142) | inrange(``ffind'',4150,4151) | inrange(``ffind'',4170,4173) | inrange(``ffind'',4190,4199) | inrange(``ffind'',4200,4200) | inrange(``ffind'',4210,4219) | inrange(``ffind'',4220,4229) | inrange(``ffind'',4230,4231) | inrange(``ffind'',4400,4499) | inrange(``ffind'',4500,4599) | inrange(``ffind'',4600,4699) | inrange(``ffind'',4700,4700) | inrange(``ffind'',4710,4712) | inrange(``ffind'',4720,4729) | inrange(``ffind'',4730,4739) | inrange(``ffind'',4740,4742) | inrange(``ffind'',4780,4780) | inrange(``ffind'',4783,4783) | inrange(``ffind'',4785,4785) | inrange(``ffind'',4789,4789)\n        qui replace `generate'=14 if inrange(``ffind'',4900,4900) | inrange(``ffind'',4910,4911) | inrange(``ffind'',4920,4922) | inrange(``ffind'',4923,4923) | inrange(``ffind'',4924,4925) | inrange(``ffind'',4930,4931) | inrange(``ffind'',4932,4932) | inrange(``ffind'',4939,4939) | inrange(``ffind'',4940,4942)\n        qui replace `generate'=15 if inrange(``ffind'',5260,5261) | inrange(``ffind'',5270,5271) | inrange(``ffind'',5300,5300) | inrange(``ffind'',5310,5311) | inrange(``ffind'',5320,5320) | inrange(``ffind'',5330,5331) | inrange(``ffind'',5334,5334) | inrange(``ffind'',5390,5399) | inrange(``ffind'',5400,5400) | inrange(``ffind'',5410,5411) | inrange(``ffind'',5412,5412) | inrange(``ffind'',5420,5421) | inrange(``ffind'',5430,5431) | inrange(``ffind'',5440,5441) | inrange(``ffind'',5450,5451) | inrange(``ffind'',5460,5461) | inrange(``ffind'',5490,5499) | inrange(``ffind'',5540,5541) | inrange(``ffind'',5550,5551) | inrange(``ffind'',5600,5699) | inrange(``ffind'',5700,5700) | inrange(``ffind'',5710,5719) | inrange(``ffind'',5720,5722) | inrange(``ffind'',5730,5733) | inrange(``ffind'',5734,5734) | inrange(``ffind'',5735,5735) | inrange(``ffind'',5736,5736) | inrange(``ffind'',5750,5750) | inrange(``ffind'',5800,5813) | inrange(``ffind'',5890,5890) | inrange(``ffind'',5900,5900) | inrange(``ffind'',5910,5912) | inrange(``ffind'',5920,5921) | inrange(``ffind'',5930,5932) | inrange(``ffind'',5940,5940) | inrange(``ffind'',5941,5941) | inrange(``ffind'',5942,5942) | inrange(``ffind'',5943,5943) | inrange(``ffind'',5944,5944) | inrange(``ffind'',5945,5945) | inrange(``ffind'',5946,5946) | inrange(``ffind'',5947,5947) | inrange(``ffind'',5948,5948) | inrange(``ffind'',5949,5949) | inrange(``ffind'',5960,5963) | inrange(``ffind'',5980,5989) | inrange(``ffind'',5990,5990) | inrange(``ffind'',5992,5992) | inrange(``ffind'',5993,5993) | inrange(``ffind'',5994,5994) | inrange(``ffind'',5995,5995) | inrange(``ffind'',5999,5999)\n        qui replace `generate'=16 if inrange(``ffind'',6010,6019) | inrange(``ffind'',6020,6020) | inrange(``ffind'',6021,6021) | inrange(``ffind'',6022,6022) | inrange(``ffind'',6023,6023) | inrange(``ffind'',6025,6025) | inrange(``ffind'',6026,6026) | inrange(``ffind'',6028,6029) | inrange(``ffind'',6030,6036) | inrange(``ffind'',6040,6049) | inrange(``ffind'',6050,6059) | inrange(``ffind'',6060,6062) | inrange(``ffind'',6080,6082) | inrange(``ffind'',6090,6099) | inrange(``ffind'',6100,6100) | inrange(``ffind'',6110,6111) | inrange(``ffind'',6112,6112) | inrange(``ffind'',6120,6129) | inrange(``ffind'',6140,6149) | inrange(``ffind'',6150,6159) | inrange(``ffind'',6160,6163) | inrange(``ffind'',6172,6172) | inrange(``ffind'',6199,6199) | inrange(``ffind'',6200,6299) | inrange(``ffind'',6300,6300) | inrange(``ffind'',6310,6312) | inrange(``ffind'',6320,6324) | inrange(``ffind'',6330,6331) | inrange(``ffind'',6350,6351) | inrange(``ffind'',6360,6361) | inrange(``ffind'',6370,6371) | inrange(``ffind'',6390,6399) | inrange(``ffind'',6400,6411) | inrange(``ffind'',6500,6500) | inrange(``ffind'',6510,6510) | inrange(``ffind'',6512,6512) | inrange(``ffind'',6513,6513) | inrange(``ffind'',6514,6514) | inrange(``ffind'',6515,6515) | inrange(``ffind'',6517,6519) | inrange(``ffind'',6530,6531) | inrange(``ffind'',6532,6532) | inrange(``ffind'',6540,6541) | inrange(``ffind'',6550,6553) | inrange(``ffind'',6611,6611) | inrange(``ffind'',6700,6700) | inrange(``ffind'',6710,6719) | inrange(``ffind'',6720,6722) | inrange(``ffind'',6723,6723) | inrange(``ffind'',6724,6724) | inrange(``ffind'',6725,6725) | inrange(``ffind'',6726,6726) | inrange(``ffind'',6730,6733) | inrange(``ffind'',6790,6790) | inrange(``ffind'',6792,6792) | inrange(``ffind'',6794,6794) | inrange(``ffind'',6795,6795) | inrange(``ffind'',6798,6798) | inrange(``ffind'',6799,6799)\n        qui replace `generate'=17 if missing(`generate') &amp; ~missing(``ffind'')\n\n        }\n\n    else if ``ftyp''==30 {\n        label define `generate' 1 \"Food Products\" 2 \"Beer &amp; Liquor\" 3 \"Tobacco Products\" 4 \"Recreation\" 5 \"Printing and Publishing\" 6 \"Consumer Goods\" 7 \"Apparel\" 8 \"Healthcare, Medical Equipment, Pharmaceutical Products\" 9 \"Chemicals\" 10 \"Textiles\" 11 \"Construction and Construction Materials\" 12 \"Steel Works Etc\" 13 \"Fabricated Products and Machinery\" 14 \"Electrical Equipment\" 15 \"Automobiles and Trucks\" 16 \"Aircraft, ships, and railroad equipment\" 17 \"Precious Metals, Non-Metallic, and Industrial Metal Mining\" 18 \"Coal\" 19 \"Petroleum and Natural Gas\" 20 \"Utilities\" 21 \"Communication\" 22 \"Personal and Business Services\" 23 \"Business Equipment\" 24 \"Business Supplies and Shipping Containers\" 25 \"Transportation\" 26 \"Wholesale\" 27 \"Retail\" 28 \"Restaraunts, Hotels, Motels\" 29 \"Banking, Insurance, Real Estate, Trading\" 30 \"Everything Else\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,199) | inrange(``ffind'',200,299) | inrange(``ffind'',700,799) | inrange(``ffind'',910,919) | inrange(``ffind'',2000,2009) | inrange(``ffind'',2010,2019) | inrange(``ffind'',2020,2029) | inrange(``ffind'',2030,2039) | inrange(``ffind'',2040,2046) | inrange(``ffind'',2048,2048) | inrange(``ffind'',2050,2059) | inrange(``ffind'',2060,2063) | inrange(``ffind'',2064,2068) | inrange(``ffind'',2070,2079) | inrange(``ffind'',2086,2086) | inrange(``ffind'',2087,2087) | inrange(``ffind'',2090,2092) | inrange(``ffind'',2095,2095) | inrange(``ffind'',2096,2096) | inrange(``ffind'',2097,2097) | inrange(``ffind'',2098,2099)\n        qui replace `generate'=2 if inrange(``ffind'',2080,2080) | inrange(``ffind'',2082,2082) | inrange(``ffind'',2083,2083) | inrange(``ffind'',2084,2084) | inrange(``ffind'',2085,2085)\n        qui replace `generate'=3 if inrange(``ffind'',2100,2199)\n        qui replace `generate'=4 if inrange(``ffind'',920,999) | inrange(``ffind'',3650,3651) | inrange(``ffind'',3652,3652) | inrange(``ffind'',3732,3732) | inrange(``ffind'',3930,3931) | inrange(``ffind'',3940,3949) | inrange(``ffind'',7800,7829) | inrange(``ffind'',7830,7833) | inrange(``ffind'',7840,7841) | inrange(``ffind'',7900,7900) | inrange(``ffind'',7910,7911) | inrange(``ffind'',7920,7929) | inrange(``ffind'',7930,7933) | inrange(``ffind'',7940,7949) | inrange(``ffind'',7980,7980) | inrange(``ffind'',7990,7999)\n        qui replace `generate'=5 if inrange(``ffind'',2700,2709) | inrange(``ffind'',2710,2719) | inrange(``ffind'',2720,2729) | inrange(``ffind'',2730,2739) | inrange(``ffind'',2740,2749) | inrange(``ffind'',2750,2759) | inrange(``ffind'',2770,2771) | inrange(``ffind'',2780,2789) | inrange(``ffind'',2790,2799) | inrange(``ffind'',3993,3993)\n        qui replace `generate'=6 if inrange(``ffind'',2047,2047) | inrange(``ffind'',2391,2392) | inrange(``ffind'',2510,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',2840,2843) | inrange(``ffind'',2844,2844) | inrange(``ffind'',3160,3161) | inrange(``ffind'',3170,3171) | inrange(``ffind'',3172,3172) | inrange(``ffind'',3190,3199) | inrange(``ffind'',3229,3229) | inrange(``ffind'',3260,3260) | inrange(``ffind'',3262,3263) | inrange(``ffind'',3269,3269) | inrange(``ffind'',3230,3231) | inrange(``ffind'',3630,3639) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3800,3800) | inrange(``ffind'',3860,3861) | inrange(``ffind'',3870,3873) | inrange(``ffind'',3910,3911) | inrange(``ffind'',3914,3914) | inrange(``ffind'',3915,3915) | inrange(``ffind'',3960,3962) | inrange(``ffind'',3991,3991) | inrange(``ffind'',3995,3995)\n        qui replace `generate'=7 if inrange(``ffind'',2300,2390) | inrange(``ffind'',3020,3021) | inrange(``ffind'',3100,3111) | inrange(``ffind'',3130,3131) | inrange(``ffind'',3140,3149) | inrange(``ffind'',3150,3151) | inrange(``ffind'',3963,3965)\n        qui replace `generate'=8 if inrange(``ffind'',2830,2830) | inrange(``ffind'',2831,2831) | inrange(``ffind'',2833,2833) | inrange(``ffind'',2834,2834) | inrange(``ffind'',2835,2835) | inrange(``ffind'',2836,2836) | inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3849) | inrange(``ffind'',3850,3851) | inrange(``ffind'',8000,8099)\n        qui replace `generate'=9 if inrange(``ffind'',2800,2809) | inrange(``ffind'',2810,2819) | inrange(``ffind'',2820,2829) | inrange(``ffind'',2850,2859) | inrange(``ffind'',2860,2869) | inrange(``ffind'',2870,2879) | inrange(``ffind'',2890,2899)\n        qui replace `generate'=10 if inrange(``ffind'',2200,2269) | inrange(``ffind'',2270,2279) | inrange(``ffind'',2280,2284) | inrange(``ffind'',2290,2295) | inrange(``ffind'',2297,2297) | inrange(``ffind'',2298,2298) | inrange(``ffind'',2299,2299) | inrange(``ffind'',2393,2395) | inrange(``ffind'',2397,2399)\n        qui replace `generate'=11 if inrange(``ffind'',800,899) | inrange(``ffind'',1500,1511) | inrange(``ffind'',1520,1529) | inrange(``ffind'',1530,1539) | inrange(``ffind'',1540,1549) | inrange(``ffind'',1600,1699) | inrange(``ffind'',1700,1799) | inrange(``ffind'',2400,2439) | inrange(``ffind'',2450,2459) | inrange(``ffind'',2490,2499) | inrange(``ffind'',2660,2661) | inrange(``ffind'',2950,2952) | inrange(``ffind'',3200,3200) | inrange(``ffind'',3210,3211) | inrange(``ffind'',3240,3241) | inrange(``ffind'',3250,3259) | inrange(``ffind'',3261,3261) | inrange(``ffind'',3264,3264) | inrange(``ffind'',3270,3275) | inrange(``ffind'',3280,3281) | inrange(``ffind'',3290,3293) | inrange(``ffind'',3295,3299) | inrange(``ffind'',3420,3429) | inrange(``ffind'',3430,3433) | inrange(``ffind'',3440,3441) | inrange(``ffind'',3442,3442) | inrange(``ffind'',3446,3446) | inrange(``ffind'',3448,3448) | inrange(``ffind'',3449,3449) | inrange(``ffind'',3450,3451) | inrange(``ffind'',3452,3452) | inrange(``ffind'',3490,3499) | inrange(``ffind'',3996,3996)\n        qui replace `generate'=12 if inrange(``ffind'',3300,3300) | inrange(``ffind'',3310,3317) | inrange(``ffind'',3320,3325) | inrange(``ffind'',3330,3339) | inrange(``ffind'',3340,3341) | inrange(``ffind'',3350,3357) | inrange(``ffind'',3360,3369) | inrange(``ffind'',3370,3379) | inrange(``ffind'',3390,3399)\n        qui replace `generate'=13 if inrange(``ffind'',3400,3400) | inrange(``ffind'',3443,3443) | inrange(``ffind'',3444,3444) | inrange(``ffind'',3460,3469) | inrange(``ffind'',3470,3479) | inrange(``ffind'',3510,3519) | inrange(``ffind'',3520,3529) | inrange(``ffind'',3530,3530) | inrange(``ffind'',3531,3531) | inrange(``ffind'',3532,3532) | inrange(``ffind'',3533,3533) | inrange(``ffind'',3534,3534) | inrange(``ffind'',3535,3535) | inrange(``ffind'',3536,3536) | inrange(``ffind'',3538,3538) | inrange(``ffind'',3540,3549) | inrange(``ffind'',3550,3559) | inrange(``ffind'',3560,3569) | inrange(``ffind'',3580,3580) | inrange(``ffind'',3581,3581) | inrange(``ffind'',3582,3582) | inrange(``ffind'',3585,3585) | inrange(``ffind'',3586,3586) | inrange(``ffind'',3589,3589) | inrange(``ffind'',3590,3599)\n        qui replace `generate'=14 if inrange(``ffind'',3600,3600) | inrange(``ffind'',3610,3613) | inrange(``ffind'',3620,3621) | inrange(``ffind'',3623,3629) | inrange(``ffind'',3640,3644) | inrange(``ffind'',3645,3645) | inrange(``ffind'',3646,3646) | inrange(``ffind'',3648,3649) | inrange(``ffind'',3660,3660) | inrange(``ffind'',3690,3690) | inrange(``ffind'',3691,3692) | inrange(``ffind'',3699,3699)\n        qui replace `generate'=15 if inrange(``ffind'',2296,2296) | inrange(``ffind'',2396,2396) | inrange(``ffind'',3010,3011) | inrange(``ffind'',3537,3537) | inrange(``ffind'',3647,3647) | inrange(``ffind'',3694,3694) | inrange(``ffind'',3700,3700) | inrange(``ffind'',3710,3710) | inrange(``ffind'',3711,3711) | inrange(``ffind'',3713,3713) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3790,3791) | inrange(``ffind'',3799,3799)\n        qui replace `generate'=16 if inrange(``ffind'',3720,3720) | inrange(``ffind'',3721,3721) | inrange(``ffind'',3723,3724) | inrange(``ffind'',3725,3725) | inrange(``ffind'',3728,3729) | inrange(``ffind'',3730,3731) | inrange(``ffind'',3740,3743)\n        qui replace `generate'=17 if inrange(``ffind'',1000,1009) | inrange(``ffind'',1010,1019) | inrange(``ffind'',1020,1029) | inrange(``ffind'',1030,1039) | inrange(``ffind'',1040,1049) | inrange(``ffind'',1050,1059) | inrange(``ffind'',1060,1069) | inrange(``ffind'',1070,1079) | inrange(``ffind'',1080,1089) | inrange(``ffind'',1090,1099) | inrange(``ffind'',1100,1119) | inrange(``ffind'',1400,1499)\n        qui replace `generate'=18 if inrange(``ffind'',1200,1299)\n        qui replace `generate'=19 if inrange(``ffind'',1300,1300) | inrange(``ffind'',1310,1319) | inrange(``ffind'',1320,1329) | inrange(``ffind'',1330,1339) | inrange(``ffind'',1370,1379) | inrange(``ffind'',1380,1380) | inrange(``ffind'',1381,1381) | inrange(``ffind'',1382,1382) | inrange(``ffind'',1389,1389) | inrange(``ffind'',2900,2912) | inrange(``ffind'',2990,2999)\n        qui replace `generate'=20 if inrange(``ffind'',4900,4900) | inrange(``ffind'',4910,4911) | inrange(``ffind'',4920,4922) | inrange(``ffind'',4923,4923) | inrange(``ffind'',4924,4925) | inrange(``ffind'',4930,4931) | inrange(``ffind'',4932,4932) | inrange(``ffind'',4939,4939) | inrange(``ffind'',4940,4942)\n        qui replace `generate'=21 if inrange(``ffind'',4800,4800) | inrange(``ffind'',4810,4813) | inrange(``ffind'',4820,4822) | inrange(``ffind'',4830,4839) | inrange(``ffind'',4840,4841) | inrange(``ffind'',4880,4889) | inrange(``ffind'',4890,4890) | inrange(``ffind'',4891,4891) | inrange(``ffind'',4892,4892) | inrange(``ffind'',4899,4899)\n        qui replace `generate'=22 if inrange(``ffind'',7020,7021) | inrange(``ffind'',7030,7033) | inrange(``ffind'',7200,7200) | inrange(``ffind'',7210,7212) | inrange(``ffind'',7214,7214) | inrange(``ffind'',7215,7216) | inrange(``ffind'',7217,7217) | inrange(``ffind'',7218,7218) | inrange(``ffind'',7219,7219) | inrange(``ffind'',7220,7221) | inrange(``ffind'',7230,7231) | inrange(``ffind'',7240,7241) | inrange(``ffind'',7250,7251) | inrange(``ffind'',7260,7269) | inrange(``ffind'',7270,7290) | inrange(``ffind'',7291,7291) | inrange(``ffind'',7292,7299) | inrange(``ffind'',7300,7300) | inrange(``ffind'',7310,7319) | inrange(``ffind'',7320,7329) | inrange(``ffind'',7330,7339) | inrange(``ffind'',7340,7342) | inrange(``ffind'',7349,7349) | inrange(``ffind'',7350,7351) | inrange(``ffind'',7352,7352) | inrange(``ffind'',7353,7353) | inrange(``ffind'',7359,7359) | inrange(``ffind'',7360,7369) | inrange(``ffind'',7370,7372) | inrange(``ffind'',7374,7374) | inrange(``ffind'',7375,7375) | inrange(``ffind'',7376,7376) | inrange(``ffind'',7377,7377) | inrange(``ffind'',7378,7378) | inrange(``ffind'',7379,7379) | inrange(``ffind'',7380,7380) | inrange(``ffind'',7381,7382) | inrange(``ffind'',7383,7383) | inrange(``ffind'',7384,7384) | inrange(``ffind'',7385,7385) | inrange(``ffind'',7389,7390) | inrange(``ffind'',7391,7391) | inrange(``ffind'',7392,7392) | inrange(``ffind'',7393,7393) | inrange(``ffind'',7394,7394) | inrange(``ffind'',7395,7395) | inrange(``ffind'',7396,7396) | inrange(``ffind'',7397,7397) | inrange(``ffind'',7399,7399) | inrange(``ffind'',7500,7500) | inrange(``ffind'',7510,7519) | inrange(``ffind'',7520,7529) | inrange(``ffind'',7530,7539) | inrange(``ffind'',7540,7549) | inrange(``ffind'',7600,7600) | inrange(``ffind'',7620,7620) | inrange(``ffind'',7622,7622) | inrange(``ffind'',7623,7623) | inrange(``ffind'',7629,7629) | inrange(``ffind'',7630,7631) | inrange(``ffind'',7640,7641) | inrange(``ffind'',7690,7699) | inrange(``ffind'',8100,8199) | inrange(``ffind'',8200,8299) | inrange(``ffind'',8300,8399) | inrange(``ffind'',8400,8499) | inrange(``ffind'',8600,8699) | inrange(``ffind'',8700,8700) | inrange(``ffind'',8710,8713) | inrange(``ffind'',8720,8721) | inrange(``ffind'',8730,8734) | inrange(``ffind'',8740,8748) | inrange(``ffind'',8800,8899) | inrange(``ffind'',8900,8910) | inrange(``ffind'',8911,8911) | inrange(``ffind'',8920,8999)\n        qui replace `generate'=23 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3622,3622) | inrange(``ffind'',3661,3661) | inrange(``ffind'',3662,3662) | inrange(``ffind'',3663,3663) | inrange(``ffind'',3664,3664) | inrange(``ffind'',3665,3665) | inrange(``ffind'',3666,3666) | inrange(``ffind'',3669,3669) | inrange(``ffind'',3670,3679) | inrange(``ffind'',3680,3680) | inrange(``ffind'',3681,3681) | inrange(``ffind'',3682,3682) | inrange(``ffind'',3683,3683) | inrange(``ffind'',3684,3684) | inrange(``ffind'',3685,3685) | inrange(``ffind'',3686,3686) | inrange(``ffind'',3687,3687) | inrange(``ffind'',3688,3688) | inrange(``ffind'',3689,3689) | inrange(``ffind'',3695,3695) | inrange(``ffind'',3810,3810) | inrange(``ffind'',3811,3811) | inrange(``ffind'',3812,3812) | inrange(``ffind'',3820,3820) | inrange(``ffind'',3821,3821) | inrange(``ffind'',3822,3822) | inrange(``ffind'',3823,3823) | inrange(``ffind'',3824,3824) | inrange(``ffind'',3825,3825) | inrange(``ffind'',3826,3826) | inrange(``ffind'',3827,3827) | inrange(``ffind'',3829,3829) | inrange(``ffind'',3830,3839) | inrange(``ffind'',7373,7373)\n        qui replace `generate'=24 if inrange(``ffind'',2440,2449) | inrange(``ffind'',2520,2549) | inrange(``ffind'',2600,2639) | inrange(``ffind'',2640,2659) | inrange(``ffind'',2670,2699) | inrange(``ffind'',2760,2761) | inrange(``ffind'',3220,3221) | inrange(``ffind'',3410,3412) | inrange(``ffind'',3950,3955)\n        qui replace `generate'=25 if inrange(``ffind'',4000,4013) | inrange(``ffind'',4040,4049) | inrange(``ffind'',4100,4100) | inrange(``ffind'',4110,4119) | inrange(``ffind'',4120,4121) | inrange(``ffind'',4130,4131) | inrange(``ffind'',4140,4142) | inrange(``ffind'',4150,4151) | inrange(``ffind'',4170,4173) | inrange(``ffind'',4190,4199) | inrange(``ffind'',4200,4200) | inrange(``ffind'',4210,4219) | inrange(``ffind'',4220,4229) | inrange(``ffind'',4230,4231) | inrange(``ffind'',4240,4249) | inrange(``ffind'',4400,4499) | inrange(``ffind'',4500,4599) | inrange(``ffind'',4600,4699) | inrange(``ffind'',4700,4700) | inrange(``ffind'',4710,4712) | inrange(``ffind'',4720,4729) | inrange(``ffind'',4730,4739) | inrange(``ffind'',4740,4749) | inrange(``ffind'',4780,4780) | inrange(``ffind'',4782,4782) | inrange(``ffind'',4783,4783) | inrange(``ffind'',4784,4784) | inrange(``ffind'',4785,4785) | inrange(``ffind'',4789,4789)\n        qui replace `generate'=26 if inrange(``ffind'',5000,5000) | inrange(``ffind'',5010,5015) | inrange(``ffind'',5020,5023) | inrange(``ffind'',5030,5039) | inrange(``ffind'',5040,5042) | inrange(``ffind'',5043,5043) | inrange(``ffind'',5044,5044) | inrange(``ffind'',5045,5045) | inrange(``ffind'',5046,5046) | inrange(``ffind'',5047,5047) | inrange(``ffind'',5048,5048) | inrange(``ffind'',5049,5049) | inrange(``ffind'',5050,5059) | inrange(``ffind'',5060,5060) | inrange(``ffind'',5063,5063) | inrange(``ffind'',5064,5064) | inrange(``ffind'',5065,5065) | inrange(``ffind'',5070,5078) | inrange(``ffind'',5080,5080) | inrange(``ffind'',5081,5081) | inrange(``ffind'',5082,5082) | inrange(``ffind'',5083,5083) | inrange(``ffind'',5084,5084) | inrange(``ffind'',5085,5085) | inrange(``ffind'',5086,5087) | inrange(``ffind'',5088,5088) | inrange(``ffind'',5090,5090) | inrange(``ffind'',5091,5092) | inrange(``ffind'',5093,5093) | inrange(``ffind'',5094,5094) | inrange(``ffind'',5099,5099) | inrange(``ffind'',5100,5100) | inrange(``ffind'',5110,5113) | inrange(``ffind'',5120,5122) | inrange(``ffind'',5130,5139) | inrange(``ffind'',5140,5149) | inrange(``ffind'',5150,5159) | inrange(``ffind'',5160,5169) | inrange(``ffind'',5170,5172) | inrange(``ffind'',5180,5182) | inrange(``ffind'',5190,5199)\n        qui replace `generate'=27 if inrange(``ffind'',5200,5200) | inrange(``ffind'',5210,5219) | inrange(``ffind'',5220,5229) | inrange(``ffind'',5230,5231) | inrange(``ffind'',5250,5251) | inrange(``ffind'',5260,5261) | inrange(``ffind'',5270,5271) | inrange(``ffind'',5300,5300) | inrange(``ffind'',5310,5311) | inrange(``ffind'',5320,5320) | inrange(``ffind'',5330,5331) | inrange(``ffind'',5334,5334) | inrange(``ffind'',5340,5349) | inrange(``ffind'',5390,5399) | inrange(``ffind'',5400,5400) | inrange(``ffind'',5410,5411) | inrange(``ffind'',5412,5412) | inrange(``ffind'',5420,5429) | inrange(``ffind'',5430,5439) | inrange(``ffind'',5440,5449) | inrange(``ffind'',5450,5459) | inrange(``ffind'',5460,5469) | inrange(``ffind'',5490,5499) | inrange(``ffind'',5500,5500) | inrange(``ffind'',5510,5529) | inrange(``ffind'',5530,5539) | inrange(``ffind'',5540,5549) | inrange(``ffind'',5550,5559) | inrange(``ffind'',5560,5569) | inrange(``ffind'',5570,5579) | inrange(``ffind'',5590,5599) | inrange(``ffind'',5600,5699) | inrange(``ffind'',5700,5700) | inrange(``ffind'',5710,5719) | inrange(``ffind'',5720,5722) | inrange(``ffind'',5730,5733) | inrange(``ffind'',5734,5734) | inrange(``ffind'',5735,5735) | inrange(``ffind'',5736,5736) | inrange(``ffind'',5750,5799) | inrange(``ffind'',5900,5900) | inrange(``ffind'',5910,5912) | inrange(``ffind'',5920,5929) | inrange(``ffind'',5930,5932) | inrange(``ffind'',5940,5940) | inrange(``ffind'',5941,5941) | inrange(``ffind'',5942,5942) | inrange(``ffind'',5943,5943) | inrange(``ffind'',5944,5944) | inrange(``ffind'',5945,5945) | inrange(``ffind'',5946,5946) | inrange(``ffind'',5947,5947) | inrange(``ffind'',5948,5948) | inrange(``ffind'',5949,5949) | inrange(``ffind'',5950,5959) | inrange(``ffind'',5960,5969) | inrange(``ffind'',5970,5979) | inrange(``ffind'',5980,5989) | inrange(``ffind'',5990,5990) | inrange(``ffind'',5992,5992) | inrange(``ffind'',5993,5993) | inrange(``ffind'',5994,5994) | inrange(``ffind'',5995,5995) | inrange(``ffind'',5999,5999)\n        qui replace `generate'=28 if inrange(``ffind'',5800,5819) | inrange(``ffind'',5820,5829) | inrange(``ffind'',5890,5899) | inrange(``ffind'',7000,7000) | inrange(``ffind'',7010,7019) | inrange(``ffind'',7040,7049) | inrange(``ffind'',7213,7213)\n        qui replace `generate'=29 if inrange(``ffind'',6000,6000) | inrange(``ffind'',6010,6019) | inrange(``ffind'',6020,6020) | inrange(``ffind'',6021,6021) | inrange(``ffind'',6022,6022) | inrange(``ffind'',6023,6024) | inrange(``ffind'',6025,6025) | inrange(``ffind'',6026,6026) | inrange(``ffind'',6027,6027) | inrange(``ffind'',6028,6029) | inrange(``ffind'',6030,6036) | inrange(``ffind'',6040,6059) | inrange(``ffind'',6060,6062) | inrange(``ffind'',6080,6082) | inrange(``ffind'',6090,6099) | inrange(``ffind'',6100,6100) | inrange(``ffind'',6110,6111) | inrange(``ffind'',6112,6113) | inrange(``ffind'',6120,6129) | inrange(``ffind'',6130,6139) | inrange(``ffind'',6140,6149) | inrange(``ffind'',6150,6159) | inrange(``ffind'',6160,6169) | inrange(``ffind'',6170,6179) | inrange(``ffind'',6190,6199) | inrange(``ffind'',6200,6299) | inrange(``ffind'',6300,6300) | inrange(``ffind'',6310,6319) | inrange(``ffind'',6320,6329) | inrange(``ffind'',6330,6331) | inrange(``ffind'',6350,6351) | inrange(``ffind'',6360,6361) | inrange(``ffind'',6370,6379) | inrange(``ffind'',6390,6399) | inrange(``ffind'',6400,6411) | inrange(``ffind'',6500,6500) | inrange(``ffind'',6510,6510) | inrange(``ffind'',6512,6512) | inrange(``ffind'',6513,6513) | inrange(``ffind'',6514,6514) | inrange(``ffind'',6515,6515) | inrange(``ffind'',6517,6519) | inrange(``ffind'',6520,6529) | inrange(``ffind'',6530,6531) | inrange(``ffind'',6532,6532) | inrange(``ffind'',6540,6541) | inrange(``ffind'',6550,6553) | inrange(``ffind'',6590,6599) | inrange(``ffind'',6610,6611) | inrange(``ffind'',6700,6700) | inrange(``ffind'',6710,6719) | inrange(``ffind'',6720,6722) | inrange(``ffind'',6723,6723) | inrange(``ffind'',6724,6724) | inrange(``ffind'',6725,6725) | inrange(``ffind'',6726,6726) | inrange(``ffind'',6730,6733) | inrange(``ffind'',6740,6779) | inrange(``ffind'',6790,6791) | inrange(``ffind'',6792,6792) | inrange(``ffind'',6793,6793) | inrange(``ffind'',6794,6794) | inrange(``ffind'',6795,6795) | inrange(``ffind'',6798,6798) | inrange(``ffind'',6799,6799)\n        qui replace `generate'=30 if missing(`generate') &amp; ~missing(``ffind'')\n        }\n\n    else if ``ftyp''==38 {\n        label define `generate' 1 \"Agriculture, forestry, and fishing\" 2 \"Mining\" 3 \"Oil and Gas Extraction\" 4 \"Nonmetalic Minerals Except Fuels\" 5 \"Construction\" 6 \"Food and Kindred Products\" 7 \"Tobacco Products\" 8 \"Textile Mill Products\" 9 \"Apparel and other Textile Products\" 10 \"Lumber and Wood Products\" 11 \"Furniture and Fixtures\" 12 \"Paper and Allied Products\" 13 \"Printing and Publishing\" 14 \"Chemicals and Allied Products\" 15 \"Petroleum and Coal Products\" 16 \"Rubber and Miscellaneous Plastics Products\" 17 \"Leather and Leather Products\" 18 \"Stone, Clay and Glass Products\" 19 \"Primary Metal Industries\" 20 \"Fabricated Metal Products\" 21 \"Machinery, Except Electrical\" 22 \"Electrical and Electronic Equipment\" 23 \"Transportation Equipment\" 24 \"Instruments and Related Products\" 25 \"Miscellaneous Manufacturing Industries\" 26 \"Transportation\" 27 \"Telephone and Telegraph Communication\" 28 \"Radio and Television Broadcasting\" 29 \"Electric, Gas, and Water Supply\" 30 \"Sanitary Services\" 31 \"Steam Supply\" 32 \"Irrigation Systems\" 33 \"Wholesale\" 34 \"Retail Stores\" 35 \"Finance, Insurance, and Real Estate\" 36 \"Services\" 37 \"Public Administration\" 38 \"Almost Nothing\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,999)\n        qui replace `generate'=2 if inrange(``ffind'',1000,1299)\n        qui replace `generate'=3 if inrange(``ffind'',1300,1399)\n        qui replace `generate'=4 if inrange(``ffind'',1400,1499)\n        qui replace `generate'=5 if inrange(``ffind'',1500,1799)\n        qui replace `generate'=6 if inrange(``ffind'',2000,2099)\n        qui replace `generate'=7 if inrange(``ffind'',2100,2199)\n        qui replace `generate'=8 if inrange(``ffind'',2200,2299)\n        qui replace `generate'=9 if inrange(``ffind'',2300,2399)\n        qui replace `generate'=10 if inrange(``ffind'',2400,2499)\n        qui replace `generate'=11 if inrange(``ffind'',2500,2599)\n        qui replace `generate'=12 if inrange(``ffind'',2600,2661)\n        qui replace `generate'=13 if inrange(``ffind'',2700,2799)\n        qui replace `generate'=14 if inrange(``ffind'',2800,2899)\n        qui replace `generate'=15 if inrange(``ffind'',2900,2999)\n        qui replace `generate'=16 if inrange(``ffind'',3000,3099)\n        qui replace `generate'=17 if inrange(``ffind'',3100,3199)\n        qui replace `generate'=18 if inrange(``ffind'',3200,3299)\n        qui replace `generate'=19 if inrange(``ffind'',3300,3399)\n        qui replace `generate'=20 if inrange(``ffind'',3400,3499)\n        qui replace `generate'=21 if inrange(``ffind'',3500,3599)\n        qui replace `generate'=22 if inrange(``ffind'',3600,3699)\n        qui replace `generate'=23 if inrange(``ffind'',3700,3799)\n        qui replace `generate'=24 if inrange(``ffind'',3800,3879)\n        qui replace `generate'=25 if inrange(``ffind'',3900,3999)\n        qui replace `generate'=26 if inrange(``ffind'',4000,4799)\n        qui replace `generate'=27 if inrange(``ffind'',4800,4829)\n        qui replace `generate'=28 if inrange(``ffind'',4830,4899)\n        qui replace `generate'=29 if inrange(``ffind'',4900,4949)\n        qui replace `generate'=30 if inrange(``ffind'',4950,4959)\n        qui replace `generate'=31 if inrange(``ffind'',4960,4969)\n        qui replace `generate'=32 if inrange(``ffind'',4970,4979)\n        qui replace `generate'=33 if inrange(``ffind'',5000,5199)\n        qui replace `generate'=34 if inrange(``ffind'',5200,5999)\n        qui replace `generate'=35 if inrange(``ffind'',6000,6999)\n        qui replace `generate'=36 if inrange(``ffind'',7000,8999)\n        qui replace `generate'=37 if inrange(``ffind'',9000,9999)\n        qui replace `generate'=38 if missing(`generate') &amp; ~missing(``ffind'')\n\n        }\n    else if ``ftyp''==48 {\n        label define `generate' 1 \"Agriculture\" 2 \"Food Products\" 3 \"Candy &amp; Soda\" 4 \"Beer &amp; Liquor\" 5 \"Tobacco Products\" 6 \"Recreation\" 7 \"Entertainment\" 8 \"Printing and Publishing\" 9 \"Consumer Goods\" 10 \"Apparel\" 11 \"Healthcare\" 12 \"Medical Equipment\" 13 \"Pharmaceutical Products\" 14 \"Chemicals\" 15 \"Rubber and Plastic Products\" 16 \"Textiles\" 17 \"Construction Materials\" 18 \"Construction\" 19 \"Steel Works Etc\" 20 \"Fabricated Products\" 21 \"Machinery\" 22 \"Electrical Equipment\" 23 \"Automobiles and Trucks\" 24 \"Aircraft\" 25 \"Shipbuilding, Railroad Equipment\" 26 \"Defense\" 27 \"Precious Metals\" 28 \"Non-Metallic and Industrial Metal Mining\" 29 \"Coal\" 30 \"Petroleum and Natural Gas\" 31 \"Utilities\" 32 \"Communication\" 33 \"Personal Services\" 34 \"Business Services\" 35 \"Computers\" 36 \"Electronic Equipment\" 37 \"Measuring and Control Equipment\" 38 \"Business Supplies\" 39 \"Shipping Containers\" 40 \"Transportation\" 41 \"Wholesale\" 42 \"Retail\" 43 \"Restaraunts, Hotels, Motels\" 44 \"Banking\" 45 \"Insurance\" 46 \"Real Estate\" 47 \"Trading\" 48 \"Almost Nothing\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,199) | inrange(``ffind'',200,299) | inrange(``ffind'',700,799) | inrange(``ffind'',910,919) | inrange(``ffind'',2048,2048)\n        qui replace `generate'=2 if inrange(``ffind'',2000,2009) | inrange(``ffind'',2010,2019) | inrange(``ffind'',2020,2029) | inrange(``ffind'',2030,2039) | inrange(``ffind'',2040,2046) | inrange(``ffind'',2050,2059) | inrange(``ffind'',2060,2063) | inrange(``ffind'',2070,2079) | inrange(``ffind'',2090,2092) | inrange(``ffind'',2095,2095) | inrange(``ffind'',2098,2099)\n        qui replace `generate'=3 if inrange(``ffind'',2064,2068) | inrange(``ffind'',2086,2086) | inrange(``ffind'',2087,2087) | inrange(``ffind'',2096,2096) | inrange(``ffind'',2097,2097)\n        qui replace `generate'=4 if inrange(``ffind'',2080,2080) | inrange(``ffind'',2082,2082) | inrange(``ffind'',2083,2083) | inrange(``ffind'',2084,2084) | inrange(``ffind'',2085,2085)\n        qui replace `generate'=5 if inrange(``ffind'',2100,2199)\n        qui replace `generate'=6 if inrange(``ffind'',920,999) | inrange(``ffind'',3650,3651) | inrange(``ffind'',3652,3652) | inrange(``ffind'',3732,3732) | inrange(``ffind'',3930,3931) | inrange(``ffind'',3940,3949)\n        qui replace `generate'=7 if inrange(``ffind'',7800,7829) | inrange(``ffind'',7830,7833) | inrange(``ffind'',7840,7841) | inrange(``ffind'',7900,7900) | inrange(``ffind'',7910,7911) | inrange(``ffind'',7920,7929) | inrange(``ffind'',7930,7933) | inrange(``ffind'',7940,7949) | inrange(``ffind'',7980,7980) | inrange(``ffind'',7990,7999)\n        qui replace `generate'=8 if inrange(``ffind'',2700,2709) | inrange(``ffind'',2710,2719) | inrange(``ffind'',2720,2729) | inrange(``ffind'',2730,2739) | inrange(``ffind'',2740,2749) | inrange(``ffind'',2770,2771) | inrange(``ffind'',2780,2789) | inrange(``ffind'',2790,2799)\n        qui replace `generate'=9 if inrange(``ffind'',2047,2047) | inrange(``ffind'',2391,2392) | inrange(``ffind'',2510,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',2840,2843) | inrange(``ffind'',2844,2844) | inrange(``ffind'',3160,3161) | inrange(``ffind'',3170,3171) | inrange(``ffind'',3172,3172) | inrange(``ffind'',3190,3199) | inrange(``ffind'',3229,3229) | inrange(``ffind'',3260,3260) | inrange(``ffind'',3262,3263) | inrange(``ffind'',3269,3269) | inrange(``ffind'',3230,3231) | inrange(``ffind'',3630,3639) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3800,3800) | inrange(``ffind'',3860,3861) | inrange(``ffind'',3870,3873) | inrange(``ffind'',3910,3911) | inrange(``ffind'',3914,3914) | inrange(``ffind'',3915,3915) | inrange(``ffind'',3960,3962) | inrange(``ffind'',3991,3991) | inrange(``ffind'',3995,3995)\n        qui replace `generate'=10 if inrange(``ffind'',2300,2390) | inrange(``ffind'',3020,3021) | inrange(``ffind'',3100,3111) | inrange(``ffind'',3130,3131) | inrange(``ffind'',3140,3149) | inrange(``ffind'',3150,3151) | inrange(``ffind'',3963,3965)\n        qui replace `generate'=11 if inrange(``ffind'',8000,8099)\n        qui replace `generate'=12 if inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3849) | inrange(``ffind'',3850,3851)\n        qui replace `generate'=13 if inrange(``ffind'',2830,2830) | inrange(``ffind'',2831,2831) | inrange(``ffind'',2833,2833) | inrange(``ffind'',2834,2834) | inrange(``ffind'',2835,2835) | inrange(``ffind'',2836,2836)\n        qui replace `generate'=14 if inrange(``ffind'',2800,2809) | inrange(``ffind'',2810,2819) | inrange(``ffind'',2820,2829) | inrange(``ffind'',2850,2859) | inrange(``ffind'',2860,2869) | inrange(``ffind'',2870,2879) | inrange(``ffind'',2890,2899)\n        qui replace `generate'=15 if inrange(``ffind'',3031,3031) | inrange(``ffind'',3041,3041) | inrange(``ffind'',3050,3053) | inrange(``ffind'',3060,3069) | inrange(``ffind'',3070,3079) | inrange(``ffind'',3080,3089) | inrange(``ffind'',3090,3099)\n        qui replace `generate'=16 if inrange(``ffind'',2200,2269) | inrange(``ffind'',2270,2279) | inrange(``ffind'',2280,2284) | inrange(``ffind'',2290,2295) | inrange(``ffind'',2297,2297) | inrange(``ffind'',2298,2298) | inrange(``ffind'',2299,2299) | inrange(``ffind'',2393,2395) | inrange(``ffind'',2397,2399)\n        qui replace `generate'=17 if inrange(``ffind'',800,899) | inrange(``ffind'',2400,2439) | inrange(``ffind'',2450,2459) | inrange(``ffind'',2490,2499) | inrange(``ffind'',2660,2661) | inrange(``ffind'',2950,2952) | inrange(``ffind'',3200,3200) | inrange(``ffind'',3210,3211) | inrange(``ffind'',3240,3241) | inrange(``ffind'',3250,3259) | inrange(``ffind'',3261,3261) | inrange(``ffind'',3264,3264) | inrange(``ffind'',3270,3275) | inrange(``ffind'',3280,3281) | inrange(``ffind'',3290,3293) | inrange(``ffind'',3295,3299) | inrange(``ffind'',3420,3429) | inrange(``ffind'',3430,3433) | inrange(``ffind'',3440,3441) | inrange(``ffind'',3442,3442) | inrange(``ffind'',3446,3446) | inrange(``ffind'',3448,3448) | inrange(``ffind'',3449,3449) | inrange(``ffind'',3450,3451) | inrange(``ffind'',3452,3452) | inrange(``ffind'',3490,3499) | inrange(``ffind'',3996,3996)\n        qui replace `generate'=18 if inrange(``ffind'',1500,1511) | inrange(``ffind'',1520,1529) | inrange(``ffind'',1530,1539) | inrange(``ffind'',1540,1549) | inrange(``ffind'',1600,1699) | inrange(``ffind'',1700,1799)\n        qui replace `generate'=19 if inrange(``ffind'',3300,3300) | inrange(``ffind'',3310,3317) | inrange(``ffind'',3320,3325) | inrange(``ffind'',3330,3339) | inrange(``ffind'',3340,3341) | inrange(``ffind'',3350,3357) | inrange(``ffind'',3360,3369) | inrange(``ffind'',3370,3379) | inrange(``ffind'',3390,3399)\n        qui replace `generate'=20 if inrange(``ffind'',3400,3400) | inrange(``ffind'',3443,3443) | inrange(``ffind'',3444,3444) | inrange(``ffind'',3460,3469) | inrange(``ffind'',3470,3479)\n        qui replace `generate'=21 if inrange(``ffind'',3510,3519) | inrange(``ffind'',3520,3529) | inrange(``ffind'',3530,3530) | inrange(``ffind'',3531,3531) | inrange(``ffind'',3532,3532) | inrange(``ffind'',3533,3533) | inrange(``ffind'',3534,3534) | inrange(``ffind'',3535,3535) | inrange(``ffind'',3536,3536) | inrange(``ffind'',3538,3538) | inrange(``ffind'',3540,3549) | inrange(``ffind'',3550,3559) | inrange(``ffind'',3560,3569) | inrange(``ffind'',3580,3580) | inrange(``ffind'',3581,3581) | inrange(``ffind'',3582,3582) | inrange(``ffind'',3585,3585) | inrange(``ffind'',3586,3586) | inrange(``ffind'',3589,3589) | inrange(``ffind'',3590,3599)\n        qui replace `generate'=22 if inrange(``ffind'',3600,3600) | inrange(``ffind'',3610,3613) | inrange(``ffind'',3620,3621) | inrange(``ffind'',3623,3629) | inrange(``ffind'',3640,3644) | inrange(``ffind'',3645,3645) | inrange(``ffind'',3646,3646) | inrange(``ffind'',3648,3649) | inrange(``ffind'',3660,3660) | inrange(``ffind'',3690,3690) | inrange(``ffind'',3691,3692) | inrange(``ffind'',3699,3699)\n        qui replace `generate'=23 if inrange(``ffind'',2296,2296) | inrange(``ffind'',2396,2396) | inrange(``ffind'',3010,3011) | inrange(``ffind'',3537,3537) | inrange(``ffind'',3647,3647) | inrange(``ffind'',3694,3694) | inrange(``ffind'',3700,3700) | inrange(``ffind'',3710,3710) | inrange(``ffind'',3711,3711) | inrange(``ffind'',3713,3713) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3790,3791) | inrange(``ffind'',3799,3799)\n        qui replace `generate'=24 if inrange(``ffind'',3720,3720) | inrange(``ffind'',3721,3721) | inrange(``ffind'',3723,3724) | inrange(``ffind'',3725,3725) | inrange(``ffind'',3728,3729)\n        qui replace `generate'=25 if inrange(``ffind'',3730,3731) | inrange(``ffind'',3740,3743)\n        qui replace `generate'=26 if inrange(``ffind'',3760,3769) | inrange(``ffind'',3795,3795) | inrange(``ffind'',3480,3489)\n        qui replace `generate'=27 if inrange(``ffind'',1040,1049)\n        qui replace `generate'=28 if inrange(``ffind'',1000,1009) | inrange(``ffind'',1010,1019) | inrange(``ffind'',1020,1029) | inrange(``ffind'',1030,1039) | inrange(``ffind'',1050,1059) | inrange(``ffind'',1060,1069) | inrange(``ffind'',1070,1079) | inrange(``ffind'',1080,1089) | inrange(``ffind'',1090,1099) | inrange(``ffind'',1100,1119) | inrange(``ffind'',1400,1499)\n        qui replace `generate'=29 if inrange(``ffind'',1200,1299)\n        qui replace `generate'=30 if inrange(``ffind'',1300,1300) | inrange(``ffind'',1310,1319) | inrange(``ffind'',1320,1329) | inrange(``ffind'',1330,1339) | inrange(``ffind'',1370,1379) | inrange(``ffind'',1380,1380) | inrange(``ffind'',1381,1381) | inrange(``ffind'',1382,1382) | inrange(``ffind'',1389,1389) | inrange(``ffind'',2900,2912) | inrange(``ffind'',2990,2999)\n        qui replace `generate'=31 if inrange(``ffind'',4900,4900) | inrange(``ffind'',4910,4911) | inrange(``ffind'',4920,4922) | inrange(``ffind'',4923,4923) | inrange(``ffind'',4924,4925) | inrange(``ffind'',4930,4931) | inrange(``ffind'',4932,4932) | inrange(``ffind'',4939,4939) | inrange(``ffind'',4940,4942)\n        qui replace `generate'=32 if inrange(``ffind'',4800,4800) | inrange(``ffind'',4810,4813) | inrange(``ffind'',4820,4822) | inrange(``ffind'',4830,4839) | inrange(``ffind'',4840,4841) | inrange(``ffind'',4880,4889) | inrange(``ffind'',4890,4890) | inrange(``ffind'',4891,4891) | inrange(``ffind'',4892,4892) | inrange(``ffind'',4899,4899)\n        qui replace `generate'=33 if inrange(``ffind'',7020,7021) | inrange(``ffind'',7030,7033) | inrange(``ffind'',7200,7200) | inrange(``ffind'',7210,7212) | inrange(``ffind'',7214,7214) | inrange(``ffind'',7215,7216) | inrange(``ffind'',7217,7217) | inrange(``ffind'',7219,7219) | inrange(``ffind'',7220,7221) | inrange(``ffind'',7230,7231) | inrange(``ffind'',7240,7241) | inrange(``ffind'',7250,7251) | inrange(``ffind'',7260,7269) | inrange(``ffind'',7270,7290) | inrange(``ffind'',7291,7291) | inrange(``ffind'',7292,7299) | inrange(``ffind'',7395,7395) | inrange(``ffind'',7500,7500) | inrange(``ffind'',7520,7529) | inrange(``ffind'',7530,7539) | inrange(``ffind'',7540,7549) | inrange(``ffind'',7600,7600) | inrange(``ffind'',7620,7620) | inrange(``ffind'',7622,7622) | inrange(``ffind'',7623,7623) | inrange(``ffind'',7629,7629) | inrange(``ffind'',7630,7631) | inrange(``ffind'',7640,7641) | inrange(``ffind'',7690,7699) | inrange(``ffind'',8100,8199) | inrange(``ffind'',8200,8299) | inrange(``ffind'',8300,8399) | inrange(``ffind'',8400,8499) | inrange(``ffind'',8600,8699) | inrange(``ffind'',8800,8899) | inrange(``ffind'',7510,7515)\n        qui replace `generate'=34 if inrange(``ffind'',2750,2759) | inrange(``ffind'',3993,3993) | inrange(``ffind'',7218,7218) | inrange(``ffind'',7300,7300) | inrange(``ffind'',7310,7319) | inrange(``ffind'',7320,7329) | inrange(``ffind'',7330,7339) | inrange(``ffind'',7340,7342) | inrange(``ffind'',7349,7349) | inrange(``ffind'',7350,7351) | inrange(``ffind'',7352,7352) | inrange(``ffind'',7353,7353) | inrange(``ffind'',7359,7359) | inrange(``ffind'',7360,7369) | inrange(``ffind'',7370,7372) | inrange(``ffind'',7374,7374) | inrange(``ffind'',7375,7375) | inrange(``ffind'',7376,7376) | inrange(``ffind'',7377,7377) | inrange(``ffind'',7378,7378) | inrange(``ffind'',7379,7379) | inrange(``ffind'',7380,7380) | inrange(``ffind'',7381,7382) | inrange(``ffind'',7383,7383) | inrange(``ffind'',7384,7384) | inrange(``ffind'',7385,7385) | inrange(``ffind'',7389,7390) | inrange(``ffind'',7391,7391) | inrange(``ffind'',7392,7392) | inrange(``ffind'',7393,7393) | inrange(``ffind'',7394,7394) | inrange(``ffind'',7396,7396) | inrange(``ffind'',7397,7397) | inrange(``ffind'',7399,7399) | inrange(``ffind'',7519,7519) | inrange(``ffind'',8700,8700) | inrange(``ffind'',8710,8713) | inrange(``ffind'',8720,8721) | inrange(``ffind'',8730,8734) | inrange(``ffind'',8740,8748) | inrange(``ffind'',8900,8910) | inrange(``ffind'',8911,8911) | inrange(``ffind'',8920,8999) | inrange(``ffind'',4220,4229)\n        qui replace `generate'=35 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3680,3680) | inrange(``ffind'',3681,3681) | inrange(``ffind'',3682,3682) | inrange(``ffind'',3683,3683) | inrange(``ffind'',3684,3684) | inrange(``ffind'',3685,3685) | inrange(``ffind'',3686,3686) | inrange(``ffind'',3687,3687) | inrange(``ffind'',3688,3688) | inrange(``ffind'',3689,3689) | inrange(``ffind'',3695,3695) | inrange(``ffind'',7373,7373)\n        qui replace `generate'=36 if inrange(``ffind'',3622,3622) | inrange(``ffind'',3661,3661) | inrange(``ffind'',3662,3662) | inrange(``ffind'',3663,3663) | inrange(``ffind'',3664,3664) | inrange(``ffind'',3665,3665) | inrange(``ffind'',3666,3666) | inrange(``ffind'',3669,3669) | inrange(``ffind'',3670,3679) | inrange(``ffind'',3810,3810) | inrange(``ffind'',3812,3812)\n        qui replace `generate'=37 if inrange(``ffind'',3811,3811) | inrange(``ffind'',3820,3820) | inrange(``ffind'',3821,3821) | inrange(``ffind'',3822,3822) | inrange(``ffind'',3823,3823) | inrange(``ffind'',3824,3824) | inrange(``ffind'',3825,3825) | inrange(``ffind'',3826,3826) | inrange(``ffind'',3827,3827) | inrange(``ffind'',3829,3829) | inrange(``ffind'',3830,3839)\n        qui replace `generate'=38 if inrange(``ffind'',2520,2549) | inrange(``ffind'',2600,2639) | inrange(``ffind'',2670,2699) | inrange(``ffind'',2760,2761) | inrange(``ffind'',3950,3955)\n        qui replace `generate'=39 if inrange(``ffind'',2440,2449) | inrange(``ffind'',2640,2659) | inrange(``ffind'',3220,3221) | inrange(``ffind'',3410,3412)\n        qui replace `generate'=40 if inrange(``ffind'',4000,4013) | inrange(``ffind'',4040,4049) | inrange(``ffind'',4100,4100) | inrange(``ffind'',4110,4119) | inrange(``ffind'',4120,4121) | inrange(``ffind'',4130,4131) | inrange(``ffind'',4140,4142) | inrange(``ffind'',4150,4151) | inrange(``ffind'',4170,4173) | inrange(``ffind'',4190,4199) | inrange(``ffind'',4200,4200) | inrange(``ffind'',4210,4219) | inrange(``ffind'',4230,4231) | inrange(``ffind'',4240,4249) | inrange(``ffind'',4400,4499) | inrange(``ffind'',4500,4599) | inrange(``ffind'',4600,4699) | inrange(``ffind'',4700,4700) | inrange(``ffind'',4710,4712) | inrange(``ffind'',4720,4729) | inrange(``ffind'',4730,4739) | inrange(``ffind'',4740,4749) | inrange(``ffind'',4780,4780) | inrange(``ffind'',4782,4782) | inrange(``ffind'',4783,4783) | inrange(``ffind'',4784,4784) | inrange(``ffind'',4785,4785) | inrange(``ffind'',4789,4789)\n        qui replace `generate'=41 if inrange(``ffind'',5000,5000) | inrange(``ffind'',5010,5015) | inrange(``ffind'',5020,5023) | inrange(``ffind'',5030,5039) | inrange(``ffind'',5040,5042) | inrange(``ffind'',5043,5043) | inrange(``ffind'',5044,5044) | inrange(``ffind'',5045,5045) | inrange(``ffind'',5046,5046) | inrange(``ffind'',5047,5047) | inrange(``ffind'',5048,5048) | inrange(``ffind'',5049,5049) | inrange(``ffind'',5050,5059) | inrange(``ffind'',5060,5060) | inrange(``ffind'',5063,5063) | inrange(``ffind'',5064,5064) | inrange(``ffind'',5065,5065) | inrange(``ffind'',5070,5078) | inrange(``ffind'',5080,5080) | inrange(``ffind'',5081,5081) | inrange(``ffind'',5082,5082) | inrange(``ffind'',5083,5083) | inrange(``ffind'',5084,5084) | inrange(``ffind'',5085,5085) | inrange(``ffind'',5086,5087) | inrange(``ffind'',5088,5088) | inrange(``ffind'',5090,5090) | inrange(``ffind'',5091,5092) | inrange(``ffind'',5093,5093) | inrange(``ffind'',5094,5094) | inrange(``ffind'',5099,5099) | inrange(``ffind'',5100,5100) | inrange(``ffind'',5110,5113) | inrange(``ffind'',5120,5122) | inrange(``ffind'',5130,5139) | inrange(``ffind'',5140,5149) | inrange(``ffind'',5150,5159) | inrange(``ffind'',5160,5169) | inrange(``ffind'',5170,5172) | inrange(``ffind'',5180,5182) | inrange(``ffind'',5190,5199)\n        qui replace `generate'=42 if inrange(``ffind'',5200,5200) | inrange(``ffind'',5210,5219) | inrange(``ffind'',5220,5229) | inrange(``ffind'',5230,5231) | inrange(``ffind'',5250,5251) | inrange(``ffind'',5260,5261) | inrange(``ffind'',5270,5271) | inrange(``ffind'',5300,5300) | inrange(``ffind'',5310,5311) | inrange(``ffind'',5320,5320) | inrange(``ffind'',5330,5331) | inrange(``ffind'',5334,5334) | inrange(``ffind'',5340,5349) | inrange(``ffind'',5390,5399) | inrange(``ffind'',5400,5400) | inrange(``ffind'',5410,5411) | inrange(``ffind'',5412,5412) | inrange(``ffind'',5420,5429) | inrange(``ffind'',5430,5439) | inrange(``ffind'',5440,5449) | inrange(``ffind'',5450,5459) | inrange(``ffind'',5460,5469) | inrange(``ffind'',5490,5499) | inrange(``ffind'',5500,5500) | inrange(``ffind'',5510,5529) | inrange(``ffind'',5530,5539) | inrange(``ffind'',5540,5549) | inrange(``ffind'',5550,5559) | inrange(``ffind'',5560,5569) | inrange(``ffind'',5570,5579) | inrange(``ffind'',5590,5599) | inrange(``ffind'',5600,5699) | inrange(``ffind'',5700,5700) | inrange(``ffind'',5710,5719) | inrange(``ffind'',5720,5722) | inrange(``ffind'',5730,5733) | inrange(``ffind'',5734,5734) | inrange(``ffind'',5735,5735) | inrange(``ffind'',5736,5736) | inrange(``ffind'',5750,5799) | inrange(``ffind'',5900,5900) | inrange(``ffind'',5910,5912) | inrange(``ffind'',5920,5929) | inrange(``ffind'',5930,5932) | inrange(``ffind'',5940,5940) | inrange(``ffind'',5941,5941) | inrange(``ffind'',5942,5942) | inrange(``ffind'',5943,5943) | inrange(``ffind'',5944,5944) | inrange(``ffind'',5945,5945) | inrange(``ffind'',5946,5946) | inrange(``ffind'',5947,5947) | inrange(``ffind'',5948,5948) | inrange(``ffind'',5949,5949) | inrange(``ffind'',5950,5959) | inrange(``ffind'',5960,5969) | inrange(``ffind'',5970,5979) | inrange(``ffind'',5980,5989) | inrange(``ffind'',5990,5990) | inrange(``ffind'',5992,5992) | inrange(``ffind'',5993,5993) | inrange(``ffind'',5994,5994) | inrange(``ffind'',5995,5995) | inrange(``ffind'',5999,5999)\n        qui replace `generate'=43 if inrange(``ffind'',5800,5819) | inrange(``ffind'',5820,5829) | inrange(``ffind'',5890,5899) | inrange(``ffind'',7000,7000) | inrange(``ffind'',7010,7019) | inrange(``ffind'',7040,7049) | inrange(``ffind'',7213,7213)\n        qui replace `generate'=44 if inrange(``ffind'',6000,6000) | inrange(``ffind'',6010,6019) | inrange(``ffind'',6020,6020) | inrange(``ffind'',6021,6021) | inrange(``ffind'',6022,6022) | inrange(``ffind'',6023,6024) | inrange(``ffind'',6025,6025) | inrange(``ffind'',6026,6026) | inrange(``ffind'',6027,6027) | inrange(``ffind'',6028,6029) | inrange(``ffind'',6030,6036) | inrange(``ffind'',6040,6059) | inrange(``ffind'',6060,6062) | inrange(``ffind'',6080,6082) | inrange(``ffind'',6090,6099) | inrange(``ffind'',6100,6100) | inrange(``ffind'',6110,6111) | inrange(``ffind'',6112,6113) | inrange(``ffind'',6120,6129) | inrange(``ffind'',6130,6139) | inrange(``ffind'',6140,6149) | inrange(``ffind'',6150,6159) | inrange(``ffind'',6160,6169) | inrange(``ffind'',6170,6179) | inrange(``ffind'',6190,6199)\n        qui replace `generate'=45 if inrange(``ffind'',6300,6300) | inrange(``ffind'',6310,6319) | inrange(``ffind'',6320,6329) | inrange(``ffind'',6330,6331) | inrange(``ffind'',6350,6351) | inrange(``ffind'',6360,6361) | inrange(``ffind'',6370,6379) | inrange(``ffind'',6390,6399) | inrange(``ffind'',6400,6411)\n        qui replace `generate'=46 if inrange(``ffind'',6500,6500) | inrange(``ffind'',6510,6510) | inrange(``ffind'',6512,6512) | inrange(``ffind'',6513,6513) | inrange(``ffind'',6514,6514) | inrange(``ffind'',6515,6515) | inrange(``ffind'',6517,6519) | inrange(``ffind'',6520,6529) | inrange(``ffind'',6530,6531) | inrange(``ffind'',6532,6532) | inrange(``ffind'',6540,6541) | inrange(``ffind'',6550,6553) | inrange(``ffind'',6590,6599) | inrange(``ffind'',6610,6611)\n        qui replace `generate'=47 if inrange(``ffind'',6200,6299) | inrange(``ffind'',6700,6700) | inrange(``ffind'',6710,6719) | inrange(``ffind'',6720,6722) | inrange(``ffind'',6723,6723) | inrange(``ffind'',6724,6724) | inrange(``ffind'',6725,6725) | inrange(``ffind'',6726,6726) | inrange(``ffind'',6730,6733) | inrange(``ffind'',6740,6779) | inrange(``ffind'',6790,6791) | inrange(``ffind'',6792,6792) | inrange(``ffind'',6793,6793) | inrange(``ffind'',6794,6794) | inrange(``ffind'',6795,6795) | inrange(``ffind'',6798,6798) | inrange(``ffind'',6799,6799)\n        qui replace `generate'=48 if missing(`generate') &amp; ~missing(``ffind'')\n\n        }\n    else if ``ftyp''==49 {\n        label define `generate' 1 \"Agriculture\" 2 \"Food Products\" 3 \"Candy &amp; Soda\" 4 \"Beer &amp; Liquor\" 5 \"Tobacco Products\" 6 \"Recreation\" 7 \"Entertainment\" 8 \"Printing and Publishing\" 9 \"Consumer Goods\" 10 \"Apparel\" 11 \"Healthcare\" 12 \"Medical Equipment\" 13 \"Pharmaceutical Products\" 14 \"Chemicals\" 15 \"Rubber and Plastic Products\" 16 \"Textiles\" 17 \"Construction Materials\" 18 \"Construction\" 19 \"Steel Works Etc\" 20 \"Fabricated Products\" 21 \"Machinery\" 22 \"Electrical Equipment\" 23 \"Automobiles and Trucks\" 24 \"Aircraft\" 25 \"Shipbuilding, Railroad Equipment\" 26 \"Defense\" 27 \"Precious Metals\" 28 \"Non-Metallic and Industrial Metal Mining\" 29 \"Coal\" 30 \"Petroleum and Natural Gas\" 31 \"Utilities\" 32 \"Communication\" 33 \"Personal Services\" 34 \"Business Services\" 35 \"Computer Hardware\" 36 \"Computer Software\" 37 \"Electronic Equipment\" 38 \"Measuring and Control Equipment\" 39 \"Business Supplies\" 40 \"Shipping Containers\" 41 \"Transportation\" 42 \"Wholesale\" 43 \"Retail\" 44 \"Restaraunts, Hotels, Motels\" 45 \"Banking\" 46 \"Insurance\" 47 \"Real Estate\" 48 \"Trading\" 49 \"Almost Nothing\"\n        label values `generate' `generate'\n\n        qui replace `generate'=1 if inrange(``ffind'',100,199) | inrange(``ffind'',200,299) | inrange(``ffind'',700,799) | inrange(``ffind'',910,919) | inrange(``ffind'',2048,2048)\n        qui replace `generate'=2 if inrange(``ffind'',2000,2009) | inrange(``ffind'',2010,2019) | inrange(``ffind'',2020,2029) | inrange(``ffind'',2030,2039) | inrange(``ffind'',2040,2046) | inrange(``ffind'',2050,2059) | inrange(``ffind'',2060,2063) | inrange(``ffind'',2070,2079) | inrange(``ffind'',2090,2092) | inrange(``ffind'',2095,2095) | inrange(``ffind'',2098,2099)\n        qui replace `generate'=3 if inrange(``ffind'',2064,2068) | inrange(``ffind'',2086,2086) | inrange(``ffind'',2087,2087) | inrange(``ffind'',2096,2096) | inrange(``ffind'',2097,2097)\n        qui replace `generate'=4 if inrange(``ffind'',2080,2080) | inrange(``ffind'',2082,2082) | inrange(``ffind'',2083,2083) | inrange(``ffind'',2084,2084) | inrange(``ffind'',2085,2085)\n        qui replace `generate'=5 if inrange(``ffind'',2100,2199)\n        qui replace `generate'=6 if inrange(``ffind'',920,999) | inrange(``ffind'',3650,3651) | inrange(``ffind'',3652,3652) | inrange(``ffind'',3732,3732) | inrange(``ffind'',3930,3931) | inrange(``ffind'',3940,3949)\n        qui replace `generate'=7 if inrange(``ffind'',7800,7829) | inrange(``ffind'',7830,7833) | inrange(``ffind'',7840,7841) | inrange(``ffind'',7900,7900) | inrange(``ffind'',7910,7911) | inrange(``ffind'',7920,7929) | inrange(``ffind'',7930,7933) | inrange(``ffind'',7940,7949) | inrange(``ffind'',7980,7980) | inrange(``ffind'',7990,7999)\n        qui replace `generate'=8 if inrange(``ffind'',2700,2709) | inrange(``ffind'',2710,2719) | inrange(``ffind'',2720,2729) | inrange(``ffind'',2730,2739) | inrange(``ffind'',2740,2749) | inrange(``ffind'',2770,2771) | inrange(``ffind'',2780,2789) | inrange(``ffind'',2790,2799)\n        qui replace `generate'=9 if inrange(``ffind'',2047,2047) | inrange(``ffind'',2391,2392) | inrange(``ffind'',2510,2519) | inrange(``ffind'',2590,2599) | inrange(``ffind'',2840,2843) | inrange(``ffind'',2844,2844) | inrange(``ffind'',3160,3161) | inrange(``ffind'',3170,3171) | inrange(``ffind'',3172,3172) | inrange(``ffind'',3190,3199) | inrange(``ffind'',3229,3229) | inrange(``ffind'',3260,3260) | inrange(``ffind'',3262,3263) | inrange(``ffind'',3269,3269) | inrange(``ffind'',3230,3231) | inrange(``ffind'',3630,3639) | inrange(``ffind'',3750,3751) | inrange(``ffind'',3800,3800) | inrange(``ffind'',3860,3861) | inrange(``ffind'',3870,3873) | inrange(``ffind'',3910,3911) | inrange(``ffind'',3914,3914) | inrange(``ffind'',3915,3915) | inrange(``ffind'',3960,3962) | inrange(``ffind'',3991,3991) | inrange(``ffind'',3995,3995)\n        qui replace `generate'=10 if inrange(``ffind'',2300,2390) | inrange(``ffind'',3020,3021) | inrange(``ffind'',3100,3111) | inrange(``ffind'',3130,3131) | inrange(``ffind'',3140,3149) | inrange(``ffind'',3150,3151) | inrange(``ffind'',3963,3965)\n        qui replace `generate'=11 if inrange(``ffind'',8000,8099)\n        qui replace `generate'=12 if inrange(``ffind'',3693,3693) | inrange(``ffind'',3840,3849) | inrange(``ffind'',3850,3851)\n        qui replace `generate'=13 if inrange(``ffind'',2830,2830) | inrange(``ffind'',2831,2831) | inrange(``ffind'',2833,2833) | inrange(``ffind'',2834,2834) | inrange(``ffind'',2835,2835) | inrange(``ffind'',2836,2836)\n        qui replace `generate'=14 if inrange(``ffind'',2800,2809) | inrange(``ffind'',2810,2819) | inrange(``ffind'',2820,2829) | inrange(``ffind'',2850,2859) | inrange(``ffind'',2860,2869) | inrange(``ffind'',2870,2879) | inrange(``ffind'',2890,2899)\n        qui replace `generate'=15 if inrange(``ffind'',3031,3031) | inrange(``ffind'',3041,3041) | inrange(``ffind'',3050,3053) | inrange(``ffind'',3060,3069) | inrange(``ffind'',3070,3079) | inrange(``ffind'',3080,3089) | inrange(``ffind'',3090,3099)\n        qui replace `generate'=16 if inrange(``ffind'',2200,2269) | inrange(``ffind'',2270,2279) | inrange(``ffind'',2280,2284) | inrange(``ffind'',2290,2295) | inrange(``ffind'',2297,2297) | inrange(``ffind'',2298,2298) | inrange(``ffind'',2299,2299) | inrange(``ffind'',2393,2395) | inrange(``ffind'',2397,2399)\n        qui replace `generate'=17 if inrange(``ffind'',800,899) | inrange(``ffind'',2400,2439) | inrange(``ffind'',2450,2459) | inrange(``ffind'',2490,2499) | inrange(``ffind'',2660,2661) | inrange(``ffind'',2950,2952) | inrange(``ffind'',3200,3200) | inrange(``ffind'',3210,3211) | inrange(``ffind'',3240,3241) | inrange(``ffind'',3250,3259) | inrange(``ffind'',3261,3261) | inrange(``ffind'',3264,3264) | inrange(``ffind'',3270,3275) | inrange(``ffind'',3280,3281) | inrange(``ffind'',3290,3293) | inrange(``ffind'',3295,3299) | inrange(``ffind'',3420,3429) | inrange(``ffind'',3430,3433) | inrange(``ffind'',3440,3441) | inrange(``ffind'',3442,3442) | inrange(``ffind'',3446,3446) | inrange(``ffind'',3448,3448) | inrange(``ffind'',3449,3449) | inrange(``ffind'',3450,3451) | inrange(``ffind'',3452,3452) | inrange(``ffind'',3490,3499) | inrange(``ffind'',3996,3996)\n        qui replace `generate'=18 if inrange(``ffind'',1500,1511) | inrange(``ffind'',1520,1529) | inrange(``ffind'',1530,1539) | inrange(``ffind'',1540,1549) | inrange(``ffind'',1600,1699) | inrange(``ffind'',1700,1799)\n        qui replace `generate'=19 if inrange(``ffind'',3300,3300) | inrange(``ffind'',3310,3317) | inrange(``ffind'',3320,3325) | inrange(``ffind'',3330,3339) | inrange(``ffind'',3340,3341) | inrange(``ffind'',3350,3357) | inrange(``ffind'',3360,3369) | inrange(``ffind'',3370,3379) | inrange(``ffind'',3390,3399)\n        qui replace `generate'=20 if inrange(``ffind'',3400,3400) | inrange(``ffind'',3443,3443) | inrange(``ffind'',3444,3444) | inrange(``ffind'',3460,3469) | inrange(``ffind'',3470,3479)\n        qui replace `generate'=21 if inrange(``ffind'',3510,3519) | inrange(``ffind'',3520,3529) | inrange(``ffind'',3530,3530) | inrange(``ffind'',3531,3531) | inrange(``ffind'',3532,3532) | inrange(``ffind'',3533,3533) | inrange(``ffind'',3534,3534) | inrange(``ffind'',3535,3535) | inrange(``ffind'',3536,3536) | inrange(``ffind'',3538,3538) | inrange(``ffind'',3540,3549) | inrange(``ffind'',3550,3559) | inrange(``ffind'',3560,3569) | inrange(``ffind'',3580,3580) | inrange(``ffind'',3581,3581) | inrange(``ffind'',3582,3582) | inrange(``ffind'',3585,3585) | inrange(``ffind'',3586,3586) | inrange(``ffind'',3589,3589) | inrange(``ffind'',3590,3599)\n        qui replace `generate'=22 if inrange(``ffind'',3600,3600) | inrange(``ffind'',3610,3613) | inrange(``ffind'',3620,3621) | inrange(``ffind'',3623,3629) | inrange(``ffind'',3640,3644) | inrange(``ffind'',3645,3645) | inrange(``ffind'',3646,3646) | inrange(``ffind'',3648,3649) | inrange(``ffind'',3660,3660) | inrange(``ffind'',3690,3690) | inrange(``ffind'',3691,3692) | inrange(``ffind'',3699,3699)\n        qui replace `generate'=23 if inrange(``ffind'',2296,2296) | inrange(``ffind'',2396,2396) | inrange(``ffind'',3010,3011) | inrange(``ffind'',3537,3537) | inrange(``ffind'',3647,3647) | inrange(``ffind'',3694,3694) | inrange(``ffind'',3700,3700) | inrange(``ffind'',3710,3710) | inrange(``ffind'',3711,3711) | inrange(``ffind'',3713,3713) | inrange(``ffind'',3714,3714) | inrange(``ffind'',3715,3715) | inrange(``ffind'',3716,3716) | inrange(``ffind'',3792,3792) | inrange(``ffind'',3790,3791) | inrange(``ffind'',3799,3799)\n        qui replace `generate'=24 if inrange(``ffind'',3720,3720) | inrange(``ffind'',3721,3721) | inrange(``ffind'',3723,3724) | inrange(``ffind'',3725,3725) | inrange(``ffind'',3728,3729)\n        qui replace `generate'=25 if inrange(``ffind'',3730,3731) | inrange(``ffind'',3740,3743)\n        qui replace `generate'=26 if inrange(``ffind'',3760,3769) | inrange(``ffind'',3795,3795) | inrange(``ffind'',3480,3489)\n        qui replace `generate'=27 if inrange(``ffind'',1040,1049)\n        qui replace `generate'=28 if inrange(``ffind'',1000,1009) | inrange(``ffind'',1010,1019) | inrange(``ffind'',1020,1029) | inrange(``ffind'',1030,1039) | inrange(``ffind'',1050,1059) | inrange(``ffind'',1060,1069) | inrange(``ffind'',1070,1079) | inrange(``ffind'',1080,1089) | inrange(``ffind'',1090,1099) | inrange(``ffind'',1100,1119) | inrange(``ffind'',1400,1499)\n        qui replace `generate'=29 if inrange(``ffind'',1200,1299)\n        qui replace `generate'=30 if inrange(``ffind'',1300,1300) | inrange(``ffind'',1310,1319) | inrange(``ffind'',1320,1329) | inrange(``ffind'',1330,1339) | inrange(``ffind'',1370,1379) | inrange(``ffind'',1380,1380) | inrange(``ffind'',1381,1381) | inrange(``ffind'',1382,1382) | inrange(``ffind'',1389,1389) | inrange(``ffind'',2900,2912) | inrange(``ffind'',2990,2999)\n        qui replace `generate'=31 if inrange(``ffind'',4900,4900) | inrange(``ffind'',4910,4911) | inrange(``ffind'',4920,4922) | inrange(``ffind'',4923,4923) | inrange(``ffind'',4924,4925) | inrange(``ffind'',4930,4931) | inrange(``ffind'',4932,4932) | inrange(``ffind'',4939,4939) | inrange(``ffind'',4940,4942)\n        qui replace `generate'=32 if inrange(``ffind'',4800,4800) | inrange(``ffind'',4810,4813) | inrange(``ffind'',4820,4822) | inrange(``ffind'',4830,4839) | inrange(``ffind'',4840,4841) | inrange(``ffind'',4880,4889) | inrange(``ffind'',4890,4890) | inrange(``ffind'',4891,4891) | inrange(``ffind'',4892,4892) | inrange(``ffind'',4899,4899)\n        qui replace `generate'=33 if inrange(``ffind'',7020,7021) | inrange(``ffind'',7030,7033) | inrange(``ffind'',7200,7200) | inrange(``ffind'',7210,7212) | inrange(``ffind'',7214,7214) | inrange(``ffind'',7215,7216) | inrange(``ffind'',7217,7217) | inrange(``ffind'',7219,7219) | inrange(``ffind'',7220,7221) | inrange(``ffind'',7230,7231) | inrange(``ffind'',7240,7241) | inrange(``ffind'',7250,7251) | inrange(``ffind'',7260,7269) | inrange(``ffind'',7270,7290) | inrange(``ffind'',7291,7291) | inrange(``ffind'',7292,7299) | inrange(``ffind'',7395,7395) | inrange(``ffind'',7500,7500) | inrange(``ffind'',7520,7529) | inrange(``ffind'',7530,7539) | inrange(``ffind'',7540,7549) | inrange(``ffind'',7600,7600) | inrange(``ffind'',7620,7620) | inrange(``ffind'',7622,7622) | inrange(``ffind'',7623,7623) | inrange(``ffind'',7629,7629) | inrange(``ffind'',7630,7631) | inrange(``ffind'',7640,7641) | inrange(``ffind'',7690,7699) | inrange(``ffind'',8100,8199) | inrange(``ffind'',8200,8299) | inrange(``ffind'',8300,8399) | inrange(``ffind'',8400,8499) | inrange(``ffind'',8600,8699) | inrange(``ffind'',8800,8899) | inrange(``ffind'',7510,7515)\n        qui replace `generate'=34 if inrange(``ffind'',2750,2759) | inrange(``ffind'',3993,3993) | inrange(``ffind'',7218,7218) | inrange(``ffind'',7300,7300) | inrange(``ffind'',7310,7319) | inrange(``ffind'',7320,7329) | inrange(``ffind'',7330,7339) | inrange(``ffind'',7340,7342) | inrange(``ffind'',7349,7349) | inrange(``ffind'',7350,7351) | inrange(``ffind'',7352,7352) | inrange(``ffind'',7353,7353) | inrange(``ffind'',7359,7359) | inrange(``ffind'',7360,7369) | inrange(``ffind'',7374,7374) | inrange(``ffind'',7376,7376) | inrange(``ffind'',7377,7377) | inrange(``ffind'',7378,7378) | inrange(``ffind'',7379,7379) | inrange(``ffind'',7380,7380) | inrange(``ffind'',7381,7382) | inrange(``ffind'',7383,7383) | inrange(``ffind'',7384,7384) | inrange(``ffind'',7385,7385) | inrange(``ffind'',7389,7390) | inrange(``ffind'',7391,7391) | inrange(``ffind'',7392,7392) | inrange(``ffind'',7393,7393) | inrange(``ffind'',7394,7394) | inrange(``ffind'',7396,7396) | inrange(``ffind'',7397,7397) | inrange(``ffind'',7399,7399) | inrange(``ffind'',7519,7519) | inrange(``ffind'',8700,8700) | inrange(``ffind'',8710,8713) | inrange(``ffind'',8720,8721) | inrange(``ffind'',8730,8734) | inrange(``ffind'',8740,8748) | inrange(``ffind'',8900,8910) | inrange(``ffind'',8911,8911) | inrange(``ffind'',8920,8999) | inrange(``ffind'',4220,4229)\n        qui replace `generate'=35 if inrange(``ffind'',3570,3579) | inrange(``ffind'',3680,3680) | inrange(``ffind'',3681,3681) | inrange(``ffind'',3682,3682) | inrange(``ffind'',3683,3683) | inrange(``ffind'',3684,3684) | inrange(``ffind'',3685,3685) | inrange(``ffind'',3686,3686) | inrange(``ffind'',3687,3687) | inrange(``ffind'',3688,3688) | inrange(``ffind'',3689,3689) | inrange(``ffind'',3695,3695)\n        qui replace `generate'=36 if inrange(``ffind'',7370,7372) | inrange(``ffind'',7375,7375) | inrange(``ffind'',7373,7373)\n        qui replace `generate'=37 if inrange(``ffind'',3622,3622) | inrange(``ffind'',3661,3661) | inrange(``ffind'',3662,3662) | inrange(``ffind'',3663,3663) | inrange(``ffind'',3664,3664) | inrange(``ffind'',3665,3665) | inrange(``ffind'',3666,3666) | inrange(``ffind'',3669,3669) | inrange(``ffind'',3670,3679) | inrange(``ffind'',3810,3810) | inrange(``ffind'',3812,3812)\n        qui replace `generate'=38 if inrange(``ffind'',3811,3811) | inrange(``ffind'',3820,3820) | inrange(``ffind'',3821,3821) | inrange(``ffind'',3822,3822) | inrange(``ffind'',3823,3823) | inrange(``ffind'',3824,3824) | inrange(``ffind'',3825,3825) | inrange(``ffind'',3826,3826) | inrange(``ffind'',3827,3827) | inrange(``ffind'',3829,3829) | inrange(``ffind'',3830,3839)\n        qui replace `generate'=39 if inrange(``ffind'',2520,2549) | inrange(``ffind'',2600,2639) | inrange(``ffind'',2670,2699) | inrange(``ffind'',2760,2761) | inrange(``ffind'',3950,3955)\n        qui replace `generate'=40 if inrange(``ffind'',2440,2449) | inrange(``ffind'',2640,2659) | inrange(``ffind'',3220,3221) | inrange(``ffind'',3410,3412)\n        qui replace `generate'=41 if inrange(``ffind'',4000,4013) | inrange(``ffind'',4040,4049) | inrange(``ffind'',4100,4100) | inrange(``ffind'',4110,4119) | inrange(``ffind'',4120,4121) | inrange(``ffind'',4130,4131) | inrange(``ffind'',4140,4142) | inrange(``ffind'',4150,4151) | inrange(``ffind'',4170,4173) | inrange(``ffind'',4190,4199) | inrange(``ffind'',4200,4200) | inrange(``ffind'',4210,4219) | inrange(``ffind'',4230,4231) | inrange(``ffind'',4240,4249) | inrange(``ffind'',4400,4499) | inrange(``ffind'',4500,4599) | inrange(``ffind'',4600,4699) | inrange(``ffind'',4700,4700) | inrange(``ffind'',4710,4712) | inrange(``ffind'',4720,4729) | inrange(``ffind'',4730,4739) | inrange(``ffind'',4740,4749) | inrange(``ffind'',4780,4780) | inrange(``ffind'',4782,4782) | inrange(``ffind'',4783,4783) | inrange(``ffind'',4784,4784) | inrange(``ffind'',4785,4785) | inrange(``ffind'',4789,4789)\n        qui replace `generate'=42 if inrange(``ffind'',5000,5000) | inrange(``ffind'',5010,5015) | inrange(``ffind'',5020,5023) | inrange(``ffind'',5030,5039) | inrange(``ffind'',5040,5042) | inrange(``ffind'',5043,5043) | inrange(``ffind'',5044,5044) | inrange(``ffind'',5045,5045) | inrange(``ffind'',5046,5046) | inrange(``ffind'',5047,5047) | inrange(``ffind'',5048,5048) | inrange(``ffind'',5049,5049) | inrange(``ffind'',5050,5059) | inrange(``ffind'',5060,5060) | inrange(``ffind'',5063,5063) | inrange(``ffind'',5064,5064) | inrange(``ffind'',5065,5065) | inrange(``ffind'',5070,5078) | inrange(``ffind'',5080,5080) | inrange(``ffind'',5081,5081) | inrange(``ffind'',5082,5082) | inrange(``ffind'',5083,5083) | inrange(``ffind'',5084,5084) | inrange(``ffind'',5085,5085) | inrange(``ffind'',5086,5087) | inrange(``ffind'',5088,5088) | inrange(``ffind'',5090,5090) | inrange(``ffind'',5091,5092) | inrange(``ffind'',5093,5093) | inrange(``ffind'',5094,5094) | inrange(``ffind'',5099,5099) | inrange(``ffind'',5100,5100) | inrange(``ffind'',5110,5113) | inrange(``ffind'',5120,5122) | inrange(``ffind'',5130,5139) | inrange(``ffind'',5140,5149) | inrange(``ffind'',5150,5159) | inrange(``ffind'',5160,5169) | inrange(``ffind'',5170,5172) | inrange(``ffind'',5180,5182) | inrange(``ffind'',5190,5199)\n        qui replace `generate'=43 if inrange(``ffind'',5200,5200) | inrange(``ffind'',5210,5219) | inrange(``ffind'',5220,5229) | inrange(``ffind'',5230,5231) | inrange(``ffind'',5250,5251) | inrange(``ffind'',5260,5261) | inrange(``ffind'',5270,5271) | inrange(``ffind'',5300,5300) | inrange(``ffind'',5310,5311) | inrange(``ffind'',5320,5320) | inrange(``ffind'',5330,5331) | inrange(``ffind'',5334,5334) | inrange(``ffind'',5340,5349) | inrange(``ffind'',5390,5399) | inrange(``ffind'',5400,5400) | inrange(``ffind'',5410,5411) | inrange(``ffind'',5412,5412) | inrange(``ffind'',5420,5429) | inrange(``ffind'',5430,5439) | inrange(``ffind'',5440,5449) | inrange(``ffind'',5450,5459) | inrange(``ffind'',5460,5469) | inrange(``ffind'',5490,5499) | inrange(``ffind'',5500,5500) | inrange(``ffind'',5510,5529) | inrange(``ffind'',5530,5539) | inrange(``ffind'',5540,5549) | inrange(``ffind'',5550,5559) | inrange(``ffind'',5560,5569) | inrange(``ffind'',5570,5579) | inrange(``ffind'',5590,5599) | inrange(``ffind'',5600,5699) | inrange(``ffind'',5700,5700) | inrange(``ffind'',5710,5719) | inrange(``ffind'',5720,5722) | inrange(``ffind'',5730,5733) | inrange(``ffind'',5734,5734) | inrange(``ffind'',5735,5735) | inrange(``ffind'',5736,5736) | inrange(``ffind'',5750,5799) | inrange(``ffind'',5900,5900) | inrange(``ffind'',5910,5912) | inrange(``ffind'',5920,5929) | inrange(``ffind'',5930,5932) | inrange(``ffind'',5940,5940) | inrange(``ffind'',5941,5941) | inrange(``ffind'',5942,5942) | inrange(``ffind'',5943,5943) | inrange(``ffind'',5944,5944) | inrange(``ffind'',5945,5945) | inrange(``ffind'',5946,5946) | inrange(``ffind'',5947,5947) | inrange(``ffind'',5948,5948) | inrange(``ffind'',5949,5949) | inrange(``ffind'',5950,5959) | inrange(``ffind'',5960,5969) | inrange(``ffind'',5970,5979) | inrange(``ffind'',5980,5989) | inrange(``ffind'',5990,5990) | inrange(``ffind'',5992,5992) | inrange(``ffind'',5993,5993) | inrange(``ffind'',5994,5994) | inrange(``ffind'',5995,5995) | inrange(``ffind'',5999,5999)\n        qui replace `generate'=44 if inrange(``ffind'',5800,5819) | inrange(``ffind'',5820,5829) | inrange(``ffind'',5890,5899) | inrange(``ffind'',7000,7000) | inrange(``ffind'',7010,7019) | inrange(``ffind'',7040,7049) | inrange(``ffind'',7213,7213)\n        qui replace `generate'=45 if inrange(``ffind'',6000,6000) | inrange(``ffind'',6010,6019) | inrange(``ffind'',6020,6020) | inrange(``ffind'',6021,6021) | inrange(``ffind'',6022,6022) | inrange(``ffind'',6023,6024) | inrange(``ffind'',6025,6025) | inrange(``ffind'',6026,6026) | inrange(``ffind'',6027,6027) | inrange(``ffind'',6028,6029) | inrange(``ffind'',6030,6036) | inrange(``ffind'',6040,6059) | inrange(``ffind'',6060,6062) | inrange(``ffind'',6080,6082) | inrange(``ffind'',6090,6099) | inrange(``ffind'',6100,6100) | inrange(``ffind'',6110,6111) | inrange(``ffind'',6112,6113) | inrange(``ffind'',6120,6129) | inrange(``ffind'',6130,6139) | inrange(``ffind'',6140,6149) | inrange(``ffind'',6150,6159) | inrange(``ffind'',6160,6169) | inrange(``ffind'',6170,6179) | inrange(``ffind'',6190,6199)\n        qui replace `generate'=46 if inrange(``ffind'',6300,6300) | inrange(``ffind'',6310,6319) | inrange(``ffind'',6320,6329) | inrange(``ffind'',6330,6331) | inrange(``ffind'',6350,6351) | inrange(``ffind'',6360,6361) | inrange(``ffind'',6370,6379) | inrange(``ffind'',6390,6399) | inrange(``ffind'',6400,6411)\n        qui replace `generate'=47 if inrange(``ffind'',6500,6500) | inrange(``ffind'',6510,6510) | inrange(``ffind'',6512,6512) | inrange(``ffind'',6513,6513) | inrange(``ffind'',6514,6514) | inrange(``ffind'',6515,6515) | inrange(``ffind'',6517,6519) | inrange(``ffind'',6520,6529) | inrange(``ffind'',6530,6531) | inrange(``ffind'',6532,6532) | inrange(``ffind'',6540,6541) | inrange(``ffind'',6550,6553) | inrange(``ffind'',6590,6599) | inrange(``ffind'',6610,6611)\n        qui replace `generate'=48 if inrange(``ffind'',6200,6299) | inrange(``ffind'',6700,6700) | inrange(``ffind'',6710,6719) | inrange(``ffind'',6720,6722) | inrange(``ffind'',6723,6723) | inrange(``ffind'',6724,6724) | inrange(``ffind'',6725,6725) | inrange(``ffind'',6726,6726) | inrange(``ffind'',6730,6733) | inrange(``ffind'',6740,6779) | inrange(``ffind'',6790,6791) | inrange(``ffind'',6792,6792) | inrange(``ffind'',6793,6793) | inrange(``ffind'',6794,6794) | inrange(``ffind'',6795,6795) | inrange(``ffind'',6798,6798) | inrange(``ffind'',6799,6799)\n        qui replace `generate'=49 if missing(`generate') &amp; ~missing(``ffind'')\n\n        }\n    else {\n        di as error \"Type must be 5, 10, 12, 17, 30, 38, 48 or 49\"\n        exit 111\n        }\n\nend\n</code></pre>","title":"Full Stata code"},{"location":"posts/get-bank-holding-company-financials/","text":"","title":"Bank Holing Company Financials from FR Y-9C"},{"location":"posts/get-bank-holding-company-financials/#extract-bhc-balance-sheet-data","text":"<p>This is the SAS macro I write to consolidate and extract BHC's balance sheet data from WRDS Bank Regulatory database. It creates a <code>bhcf</code> dataset in the work directory.</p> <pre><code>%macro bhc_financials(loopdatestart,loopdateend);\n    /* Specify the variables to extract */\n    %let vars=rssd9999 rssd9001 rssd9007 rssd9008 bhck2170 bhck3210;\n    %let loopdatestart=%sysfunc(inputn(&amp;loopdatestart,anydtdte9.));\n    %let loopdateend=%sysfunc(inputn(&amp;loopdateend,anydtdte9.));\n    %let dif=%sysfunc(intck(month,&amp;loopdatestart,&amp;loopdateend));\n    %let dats=;\n    %do i=0 %to &amp;dif;\n        %let date=%sysfunc(intnx(month,&amp;loopdatestart,&amp;i,e));\n        %let month=%sysfunc(month(&amp;date),z2.);\n        %let year=%sysfunc(year(&amp;date));\n        %if &amp;month=3 or &amp;month=6 or &amp;month=9 or &amp;month=12 %then %do;\n        %let dats=&amp;dats bank.bhcf&amp;year&amp;month;\n        %end;\n    %end;\n    %put &amp;dats;\n    data bhcf(keep=&amp;vars); set &amp;dats; \n        rssd9999 = input(put(rssd9999, 8.), yymmdd10.);/* reporting date */\n        rssd9007 = input(put(rssd9007, 8.), yymmdd10.);/* date start */\n        rssd9008 = input(put(rssd9008, 8.), yymmdd10.);/* date end */\n        format rssd9999 date9.;\n        format rssd9007 date9.;\n        format rssd9008 date9.;\n        where rssd9999 between rssd9007 and rssd9008;\n    run;\n%mend bhc_financials;\n\n%bhc_financials(01jan1990,01dec2000);\n</code></pre>  <p>Warning</p> <p>RSSD dates are not always available, in which case lines 18-24 should be removed.</p>","title":"Extract BHC balance sheet data"},{"location":"posts/get-bank-holding-company-financials/#merge-with-compustatcrsp","text":"<p>The firm identifier in the Y-9C data is <code>RSSD9001</code>. To merge the BHC's balance sheet data with Compustat/CRSP, I use the <code>PERMCO-RSSD</code> link table by the Federal Reserve Bank of New York.1 I saved the most recent copy in my server, and formatted it so that it can used directly. It is available at https://mingze-gao.com/data/download/crsp_20181231.csv.</p> <pre><code>%let beg_yr = 1986;\n%let end_yr = 2018;\n\nproc sql;\ncreate table lnk as\nselect *\nfrom crsp.ccmxpf_lnkhist\nwhere\n    linktype in (\"LU\", \"LC\") and\n    (&amp;end_yr+1 &gt;= year(linkdt) or linkdt = .B) and \n    (&amp;beg_yr-1 &lt;= year(linkenddt) or linkenddt = .E)\norder by \n    gvkey, linkdt;\nquit;\n\n\n/* PERMCO-RSSD link table by New York FED */\nfilename csv url \"https://mingze-gao.com/data/download/crsp_20181231.csv\";\nproc import datafile=csv out=work.crsp_20181231 dbms=csv replace; run;\nproc sql;\ncreate table gvkey_permno_permco_rssd as \n    select *\nfrom lnk join crsp_20181231 as fed\non lnk.lpermco=fed.permco;\nquit;\n</code></pre>  <p>Note</p> <p>Please run these programs on the WRDS cloud. You'll need to modify them in order to run locally with SAS/Connect.</p>    <ol> <li> <p>https://www.newyorkfed.org/research/banking_research/datasets.html \u21a9</p> </li> </ol>","title":"Merge with Compustat/CRSP"},{"location":"posts/identify-chinese-state-owned-enterprise-using-csmar/","text":"<p>Many research papers on Chinese firms include a control variable that indicates if the firm is a state-owned enterprise (SOE). This is important as SOEs and non-SOEs differ in many aspects and may have structural differences. This post documents the way to construct this indicator variable from the CSMAR databases. </p> <p>Specifically, we need the CSMAR China Listed Firms Shareholders - Controlling Shareholders dataset. On WRDS, this dataset is named <code>hld_contrshr</code>, located at <code>/wrds/csmar/sasdata/hld</code>.</p> <p>Inside this dataset there're a few variables identifying the ultimate controlling shareholder.</p> <ul> <li><code>s0701b</code>: ultimate controlling shareholder.</li> <li><code>s0702b</code>: nature of ultimate controlling shareholder.</li> </ul> <p>According to the CSMAR documentation, <code>s0702b</code> can be one of the following. Apparently, <code>s0702b=1100</code> means the firm is a SOE.</p>    Code Type     1000 Enterprise business unit   1100 State-owned Enterprise   1210 Collective-owned enterprises   1200 Private Enterprises   1220 Enterprises with funds from Hong Kong, Macau and Taiwan   1230 Foreign-funded enterprises   2000 Administrative departments or institutions   2100 Central institution   2120 Local institution   2500 Social Organization   3000 Natural Persons   3110 Domestic natural persons   3120 Natural person from Hong Kong, Macao and Taiwan   3200 Foreign natural person   9999 Other    <p>Princeton University Library has another guide on other ways to identify Chinese SOE.</p>","title":"Identify Chinese State-Owned Enterprise using CSMAR"},{"location":"posts/legao-to-make-your-own-lego-mosaics/","text":"<p>I made an online app that turns a picture into a LEGO mosaic, available at mingze-gao.com/legao.</p> <p>A few weeks ago, I went to the new LEGO flagship store at Bondi with my fiancee, Sherry, and we were impressed by the LEGO Mosaics -- Sydney Harbour Bridge and Opera House in sunset, designed by Ryan McNaught (photo credit: jaysbrickblog.com).</p> <p></p> <p>This mosaic is made of 62,300 bricks and took 282 hours to build. Every single pixel is a 1x1 LEGO brick! We love it so much so that I'm thinking of making one myself and use it to decorate a wall in our apartment in the future.</p> <p>To begin this endeavour, I'll need a handy tool to convert pictures to LEGO mosaic so that I can have a preview and the data to assemble laterly. It turns out that there's already an open-source library named legofy for this job. So I borrowed it and wrote a small Flask app on my server to do the magic.</p> <p>I wrote the frontend using React and Ant Design, and picked up the React Hooks along the way. It was great fun. I named it using a combination of LEGO and my surname Gao, so, LeGao.</p> <p>Now, LeGao is served at mingze-gao.com/legao. A preview is as below:</p> <p></p> <p>Users can upload an image(&lt;5MB) and decide on which palette to use and how many 1x1 bricks the output image should have for its longest axis. This is useful when we need to make a LEGO mosaic in real world, as a 1x1 brick's dimension is about 8mm x 8mm.</p> <p></p> <p>The output image can be downloaded, no problem. All images will be deleted from my server after 5 minutes since upload/creation for privacy concern and the fact that my server doesn't have a big storage.</p> <p>LeGao also tells you about how many bricks you'll need to assemble the mosaic, if you really want to. Then you can easily order the bricks online or visit a store to purchase them all~</p>","title":"LeGao to Make Your Own LEGO Mosaics"},{"location":"posts/merge-compustat-and-crsp/","text":"<p>Using the CRSP/Compustat Merged Database (CCM) to extract data is one of the fundamental steps in most finance studies. Here I document several SAS programs for annual, quarterly and monthly data, inspired by and adapted from several examples from the WRDS.1</p>","title":"Merge Compustat and CRSP"},{"location":"posts/merge-compustat-and-crsp/#gvkey-permno-link-table","text":"<p>First, we need to create a <code>GVKEY-PERMNO</code> link table.</p> <pre><code>%let beg_yr = 2000;\n%let end_yr = 2003;\n\nproc sql;\ncreate table lnk as\nselect *\nfrom crsp.ccmxpf_lnkhist\nwhere\n    /* See below for a description of the link types */\n    linktype in (\"LU\", \"LC\") and\n    /* Extend the period to deal with fiscal year issues */\n    /* Note that the \".B\" and \".E\" missing value codes represent the   */\n    /* earliest possible beginning date and latest possible end date   */\n    /* of the Link Date range, respectively.                           */\n    (&amp;end_yr+1 &gt;= year(linkdt) or linkdt = .B) and \n    (&amp;beg_yr-1 &lt;= year(linkenddt) or linkenddt = .E)\n    /* primary link assigned by Compustat or CRSP */\n    and linkprim in (\"P\", \"C\") \norder by \n    gvkey, linkdt;\nquit;\n</code></pre>    Link Type Description     LC Link research complete (after extensive research by CRSP). Standard connection between databases.   LU Link is unresearched by CRSP. It is established by comparing the Compustat and historical CRSP CUSIPs. LU represents the most popular link type.   LS Link valid for this security only.2   LX Link to a security that trades on foreign exchange not included in CRSP data.   LD Duplicate link to a security. Two GVKEYs map to a single <code>PERMNO</code> (<code>PERMCO</code>) during the same period, and this link should not be used. Almost all of these cases happened before 1990.   LN Primary link exists but Compustat does not have prices.3   NR No link available; confirmed by research.   NU No link available; not yet confirmed.    <p>According to WRDS's support page: </p> <ul> <li>Primary link types (<code>LC</code>, <code>LU</code> and <code>LS</code>) account for 41% of the links in CCM.</li> <li>Secondary link types (<code>LX</code>, <code>LD</code> and <code>LN</code>) account for only 2%. </li> <li>Non-matching link types (<code>NR</code> and <code>NU</code>) account for the rest 57%, which is   expected because of the different coverage of the two databases.</li> </ul> <p>Generally, using <code>LC</code> and <code>LU</code> should be sufficient.</p>","title":"<code>GVKEY-PERMNO</code> link table"},{"location":"posts/merge-compustat-and-crsp/#compustat-annual-and-crsp","text":"<p>Example <code>ccmfunda.sas</code>.</p> <pre><code>proc sql;\ncreate table mydata as \nselect *\nfrom lnk, comp.funda (keep=gvkey fyear datadate indfmt datafmt popsrc consol sale) as cst\nwhere indfmt= 'INDL' \nand datafmt='STD' \nand popsrc='D' \nand consol='C' \nand lnk.gvkey = cst.gvkey\nand (&amp;beg_yr &lt;= fyear &lt;= &amp;end_yr) \nand (linkdt &lt;= cst.datadate or linkdt = .B) \nand (cst.datadate &lt;= linkenddt or linkenddt = .E);\nquit;\n</code></pre>","title":"Compustat Annual and CRSP"},{"location":"posts/merge-compustat-and-crsp/#compustat-quarterly-and-crsp","text":"<p>Example <code>ccmfundq.sas</code>.</p> <pre><code>proc sql;\ncreate table mydata as \nselect *\nfrom lnk, comp.fundq (keep=gvkey fyearq datadate indfmt datafmt popsrc consol saley saleq) as cst\nwhere indfmt= 'INDL' \nand datafmt='STD' \nand popsrc='D' \nand consol='C' \nand lnk.gvkey = cst.gvkey\nand (&amp;beg_yr &lt;= fyearq &lt;= &amp;end_yr) \nand (linkdt &lt;= cst.datadate or linkdt = .B) \nand (cst.datadate &lt;= linkenddt or linkenddt = .E);\nquit;\n</code></pre>","title":"Compustat Quarterly and CRSP"},{"location":"posts/merge-compustat-and-crsp/#compustat-monthly-and-crsp","text":"<p>To be done.</p>   <ol> <li> <p>WRDS Overview of CRSP/COMPUSTAT Merged:  https://wrds-www.wharton.upenn.edu/pages/support/manuals-and-overviews/crsp/crspcompustat-merged-ccm/wrds-overview-crspcompustat-merged-ccm/     Use CRSP-Compustat Merged Table to Add Permno to Compustat Data:  https://wrds-www.wharton.upenn.edu/pages/support/research-wrds/macros/wrds-macro-ccm/    Merging CRSP and Compustat Data:  https://wrds-www.wharton.upenn.edu/pages/support/applications/linking-databases/linking-crsp-and-compustat/ \u21a9</p> </li> <li> <p>Other CRSP <code>PERMNOs</code> with the same <code>PERMCO</code> will link to other <code>GVKEYs</code>. <code>LS</code> links mainly relate to ETFs where a single CRSP <code>PERMCO</code> links to multiple Compustat <code>GVKEYs</code>. In Compustat, even though they may belong to the same investment company (e.g. ISHARES), ETFs are presented with different <code>GVKEYs</code> and CRSP flags this situation.\u00a0\u21a9</p> </li> <li> <p>Prices are used to check the accuracy of the link. For linktype LN there is no price information available even on a quarterly or annual basis. The user will have to decide whether or not to include these links.\u00a0\u21a9</p> </li> </ol>","title":"Compustat Monthly and CRSP"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/","text":"<p>Thomson One Banker SDC Platinum database provides comprehensive M&amp;A transaction data from early 1980s, and is perhaps the most widely used M&amp;A database in the world. </p> <p>This post documents the steps of downloading M&amp;A deals from the SDC Platinum database. Specifically, I show how to download the complete M&amp;A data where:</p> <ul> <li>both the acquiror and the target are US firms,</li> <li>the acquiror is a public firm or a private firm,</li> <li>the target is a public firm, a private firm, or a subsidiary,</li> <li>the deal value is at least $1m, and</li> <li>the form of the deal is a acquisition, a merger or an acquisition of majority interest.</li> </ul>","title":"Download M&amp;A Deals from SDC Platinum"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/#interface-of-sdc-platinum","text":"<p>The screenshot below is the interface we'll see on launch of SDC Platinum. Click on <code>Login</code> and we'll be asked to enter our initials and a project name for billing purpose.</p> <p></p> <p>Click on <code>Login</code> and you'll be asked to enter your initials and a project name for billing purpose.</p>","title":"Interface of SDC Platinum"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/#database-selection","text":"<p>Since we're interested in M&amp;A deals, select the <code>Mergers &amp; Acquisitions</code> tab and check <code>US Targets</code>, so that we'll be searching in the domestic mergers database.</p> <p></p> <p>Then select the sample period, e.g. for the entire 2020 calendar year.</p> <p></p>","title":"Database Selection"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/#apply-filters-on-ma-deals","text":"<p>Now we can apply various filters on the M&amp;A deals we want to download.</p> <p></p> <p>We can quickly add some filters on the target's and acquiror's nation, and make sure we check the Action to be <code>Select</code> not <code>Exclude</code>. Under the <code>Deal</code> tab, we set the deal value to be at least $1m.</p> <p></p> <p>In case we couldn't find the desired filtering variable, we can head to <code>All Items</code> tab and search manually. We add restrictions on acquiror and target public status here.</p> <p></p> <p>Lastly, for the Form of the Deal, we restrict to <code>A</code> Acquisition (Stock), <code>M</code> Merger (Stock or Assets) and <code>AM</code> Acquisition of Majority Interest (Stock). We do not want to include deals that are acquisition of partial interest, recapitalization or repurchases in this case.</p> <p>Our search requests should now look like below. Strongly recommended saving this session for later reuse.</p> <p></p>","title":"Apply Filters on M&amp;A Deals"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/#specify-deal-variables-to-download","text":"<p>Our effort so far is only shortlisting the M&amp;A deals that we're interested in. We now need to specify the relevant deal variables to download by creating a new custom report.</p> <p></p> <p>As before, we can check those variables in the <code>Basics</code> tab or search under <code>All Items</code> tab. Once done, we format the report like below by arranging the order of the variables. This order is preserved when exported to spreadsheet. One note here is that each page has a maximum width of 160, so we need to insert page at proper places. It does not affect the layout of output spreadsheet. It also recommended to save the custom report for later reuse. </p> <p></p> <p>Finally, it's time to execute the requests and download the M&amp;A deal data.</p> <p></p>","title":"Specify Deal Variables to Download"},{"location":"posts/merger_acquisition_deals_from_sdc_platinum/#final-note","text":"<p>As a final remark, the downloaded spreadsheet can be imported into SAS and matched with CRSP/Compustat using CUSIP and Ticker (SDC doesn't have <code>permno</code> or <code>gvkey</code>). First, merge the SDC CUSIP with the first 6-digit CUSIP in CRSP or Compustat; if no match, then use SDC Primary Ticker Symbol to match with the ticker symbol in CRSP or Compustat.</p>","title":"Final Note"},{"location":"posts/minimum-variance-hedge-ratio/","text":"<p>This note briefly explains what's the minimum variance hedge ratio and how to derive it in a cross hedge, where the asset to be hedged is not the same as underlying asset.</p>","title":"Minimum Variance Hedge Ratio"},{"location":"posts/minimum-variance-hedge-ratio/#the-hedge-ratio-hh","text":"<p>The hedge ratio hh is the ratio of the size of the hedging position to the exposure of the asset to be hedged:</p> <ul> <li>N_AN_A: the units of asset held to be hedged (AA), i.e. the risk exposure.</li> <li>N_FN_F: the units of the underlying asset hedged with futures (A'A').<ul> <li>Note that the underlying asset A'A' may not be the same as the asset to be   hedged AA.</li> <li>In a cross hedge, the underlying of the futures is different from the   asset to be hedge.</li> </ul> </li> </ul>  h=\\frac{N_F}{N_A}=\\frac{\\text{size of hedging position}}{\\text{size of  exposure}} h=\\frac{N_F}{N_A}=\\frac{\\text{size of hedging position}}{\\text{size of  exposure}}  <p>Apparently, if we vary hh, the variance (risk) of the combined hedged position will also change.</p>","title":"The Hedge Ratio h"},{"location":"posts/minimum-variance-hedge-ratio/#the-optimal-minimum-variance-hedge-ratio-hh","text":"<p>Our objective in hedging is to manage the variance (risk) of our position, making it as low as possible by setting the hedge ratio hh to be the optimal hedge ratio h^*h^* that minimises the variance of the combined hedged position. </p>","title":"The (Optimal) Minimum-Variance Hedge Ratio h^*h^*"},{"location":"posts/minimum-variance-hedge-ratio/#hedge-where-aaaa","text":"<p>It's relatively easy when the underlying asset of the futures (A'A') is the same as the asset to be hedged (AA), as they have a perfect correlation and the same variance. Thus, as long as the hedge ratio h=1h=1, where the size of hedging position equals the exposure of the asset held, the perfect correlation and same variance ensure the value changes in the hedging position offset the changes in the value of asset to be hedged, so that the variance of the hedged position is minimum at zero (ignoring other basis risks). This means, the optimal minimum-variance hedge ratio h^*=1h^*=1.</p>","title":"Hedge where A'=AA'=A"},{"location":"posts/minimum-variance-hedge-ratio/#cross-hedge-where-a-neq-aa-neq-a","text":"<p>When the underlying asset of the futures (A'A') differ from asset to be hedged (AA), the optimal hedge ratio h^*h^* that minimises the portfolio variance is not necessarily 1 anymore. </p> <p>Let's now derive h^*h^*.</p> <ul> <li>S_tS_t: the spot price of the asset to be hedged at time t=1,2t=1,2.</li> <li>F_tF_t: the price of the futures at time t=1,2t=1,2.</li> <li>\\sigma_S\\sigma_S: the standard deviation of \\Delta S=S_2 - S_1\\Delta S=S_2 - S_1.</li> <li>\\sigma_F\\sigma_F: the standard deviation of \\Delta F=F_2 - F_1\\Delta F=F_2 - F_1.</li> <li>\\rho\\rho: the correlation coefficient between \\Delta S\\Delta S and \\Delta F\\Delta F.</li> </ul> <p>Let's consider a short hedge, where we long S_tS_t and short h\\times F_th\\times F_t, hence\uff1a</p> <ul> <li>Combined position C=S_t-h\\times F_tC=S_t-h\\times F_t</li> <li>\\Delta C=\\Delta S_t-h\\times \\Delta F_t\\Delta C=\\Delta S_t-h\\times \\Delta F_t</li> </ul> <p>The optimal hedge ratio h^*h^* is the hedge ratio that minimises the variance of \\Delta C\\Delta C.</p>  h^* =\\underset{h}{\\operatorname{argmin}} \\text{Var}(\\Delta C) =\\underset{h}{\\operatorname{argmin}} \\text{Var}(\\Delta S_t-h\\times \\Delta F_t) h^* =\\underset{h}{\\operatorname{argmin}} \\text{Var}(\\Delta C) =\\underset{h}{\\operatorname{argmin}} \\text{Var}(\\Delta S_t-h\\times \\Delta F_t)  <p>We also know that</p>  \\text{Var}(\\Delta S_t-h\\times \\Delta F_t) = \\sigma^2_S + h^2\\sigma^2_F - 2h(\\rho \\sigma_S \\sigma_F) \\text{Var}(\\Delta S_t-h\\times \\Delta F_t) = \\sigma^2_S + h^2\\sigma^2_F - 2h(\\rho \\sigma_S \\sigma_F)  <p>To minimise the variance, the first-order condition (FOC) is that</p>  \\frac{\\partial \\text{Var}(\\Delta C)}{\\partial h}=2h\\sigma^2_F-2(\\rho \\sigma_S \\sigma_F)=0 \\frac{\\partial \\text{Var}(\\Delta C)}{\\partial h}=2h\\sigma^2_F-2(\\rho \\sigma_S \\sigma_F)=0  <p>The optimal hedge ratio h^*h^* is the hh that solves the FOC above. Therefore,</p>  h^* = \\rho \\frac{\\sigma_S}{\\sigma_F} h^* = \\rho \\frac{\\sigma_S}{\\sigma_F}","title":"Cross Hedge where A' \\neq AA' \\neq A"},{"location":"posts/minimum-variance-hedge-ratio/#intuition","text":"<p>The optimal hedge ratio h^*h^* describes the optimal N_F/N_AN_F/N_A, so that the optimal size of the hedging position:</p>  N_F^* = h^* \\times N_A N_F^* = h^* \\times N_A  <ul> <li>If FF changes by 1%, SS is expected to change by h^*h^*%.</li> <li>If SS changes by 1%, FF is expected to change by (1/h^*)(1/h^*)%.</li> </ul> <p>If \\rho=1\\rho=1 and \\sigma_F=\\sigma_S\\sigma_F=\\sigma_S, then h^*=1h^*=1:</p> <ul> <li>FF and SS always change in the same way by the same size.</li> <li>Holding the same amount of FF as SS gives the perfect hedge.</li> </ul> <p>If \\rho=1\\rho=1 and \\sigma_F=2\\sigma_S\\sigma_F=2\\sigma_S, then h^*=0.5h^*=0.5:</p> <ul> <li>FF always changes twice as much as SS.</li> <li>Holding half as much of FF as SS gives the perfect hedge.</li> </ul> <p>If \\rho&lt;1\\rho&lt;1, then h^*h^* depends on \\rho\\rho and {\\sigma_S}/{\\sigma_F}{\\sigma_S}/{\\sigma_F}:</p> <ul> <li>FF is expected to change by (1/h^*)(1/h^*)% when SS changes by 1%.</li> <li>Holding h^*h^* as much of FF as SS gives an imperfect hedge where the value   of the hedging position is expected to offset the change in SS.</li> </ul>","title":"Intuition"},{"location":"posts/never-use-a-brain-wallet/","text":"<p>Among many reasons why people find it hard to use cryptocurrency there's a simple one -- memorising the private key is too hard. So, people invented brain wallet, which turns a string of words into a private key and thus wallet. </p> <p>It's genius in that now a user needs only to memorise whatever he or she used to create the wallet. You can turn your name, phone number, DoB, favourite quote, lover's home address, ..., literally anything into a cryptocurrency wallet. However, this also means that if someone else successfully guessed the passphrase you used, they can sweep all the coins you have! </p>","title":"Never Use a Brain Wallet"},{"location":"posts/never-use-a-brain-wallet/#python-brain-wallet-for-bitcoin","text":"<p>After a little bit of research, I've put together a simple brain wallet Python script that turns any input string to a legal Bitcoin private key and its address.</p> <pre><code>import codecs\nimport hashlib\nimport ecdsa\n\n\nclass BrainWallet:\n\n    @staticmethod\n    def generate_address_from_passphrase(passphrase):\n        private_key = str(hashlib.sha256(\n            passphrase.encode('utf-8')).hexdigest())\n        address =  BrainWallet.generate_address_from_private_key(private_key)\n        return private_key, address\n\n    @staticmethod\n    def generate_address_from_private_key(private_key):\n        public_key = BrainWallet.__private_to_public(private_key)\n        address = BrainWallet.__public_to_address(public_key)\n        return address\n\n    @staticmethod\n    def __private_to_public(private_key):\n        private_key_bytes = codecs.decode(private_key, 'hex')\n        # Get ECDSA public key\n        key = ecdsa.SigningKey.from_string(\n            private_key_bytes, curve=ecdsa.SECP256k1).verifying_key\n        key_bytes = key.to_string()\n        key_hex = codecs.encode(key_bytes, 'hex')\n        # Add bitcoin byte\n        bitcoin_byte = b'04'\n        public_key = bitcoin_byte + key_hex\n        return public_key\n\n    @staticmethod\n    def __public_to_address(public_key):\n        public_key_bytes = codecs.decode(public_key, 'hex')\n        # Run SHA256 for the public key\n        sha256_bpk = hashlib.sha256(public_key_bytes)\n        sha256_bpk_digest = sha256_bpk.digest()\n        # Run ripemd160 for the SHA256\n        ripemd160_bpk = hashlib.new('ripemd160')\n        ripemd160_bpk.update(sha256_bpk_digest)\n        ripemd160_bpk_digest = ripemd160_bpk.digest()\n        ripemd160_bpk_hex = codecs.encode(ripemd160_bpk_digest, 'hex')\n        # Add network byte\n        network_byte = b'00'\n        network_bitcoin_public_key = network_byte + ripemd160_bpk_hex\n        network_bitcoin_public_key_bytes = codecs.decode(\n            network_bitcoin_public_key, 'hex')\n        # Double SHA256 to get checksum\n        sha256_nbpk = hashlib.sha256(network_bitcoin_public_key_bytes)\n        sha256_nbpk_digest = sha256_nbpk.digest()\n        sha256_2_nbpk = hashlib.sha256(sha256_nbpk_digest)\n        sha256_2_nbpk_digest = sha256_2_nbpk.digest()\n        sha256_2_hex = codecs.encode(sha256_2_nbpk_digest, 'hex')\n        checksum = sha256_2_hex[:8]\n        # Concatenate public key and checksum to get the address\n        address_hex = (network_bitcoin_public_key + checksum).decode('utf-8')\n        wallet = BrainWallet.base58(address_hex)\n        return wallet\n\n    @staticmethod\n    def base58(address_hex):\n        alphabet = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'\n        b58_string = ''\n        # Get the number of leading zeros and convert hex to decimal\n        leading_zeros = len(address_hex) - len(address_hex.lstrip('0'))\n        # Convert hex to decimal\n        address_int = int(address_hex, 16)\n        # Append digits to the start of string\n        while address_int &gt; 0:\n            digit = address_int % 58\n            digit_char = alphabet[digit]\n            b58_string = digit_char + b58_string\n            address_int //= 58\n        # Add '1' for each 2 leading zeros\n        ones = leading_zeros // 2\n        for one in range(ones):\n            b58_string = '1' + b58_string\n        return b58_string\n</code></pre>","title":"Python brain wallet for Bitcoin"},{"location":"posts/never-use-a-brain-wallet/#easily-cracking-a-wallet","text":"<p>Let me show you some really easy-to-guess passphrases and their associated private keys and addresses. As an example, the code below uses \"password\" as the input passphrase and derives the private key and address from it.</p> <pre><code>passphrase = 'password'\nwallet = BrainWallet()\nprivate_key, address = wallet.generate_address_from_passphrase(passphrase)\nprint(f'passphrase: {passphrase}')\nprint(f'private key: {private_key}')\nprint(f'address: {address}')\n</code></pre> <p>The output is:</p> <pre><code>passphrase: password\nprivate key: 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8\naddress: 16ga2uqnF1NqpAuQeeg7sTCAdtDUwDyJav\n</code></pre> <p>As at May 22, 2019, this address has 45,014 transactions with a total of 0.3563 BTC (of course the balance is zero)! You can check its current balance at blockchain.com. Also, congratulations, you are now one of the many owners of this address/wallet. So next time you observe some coins transfered to it, you'll be able to use it as well (though I don't suggest you to do so)!</p>","title":"Easily \"cracking\" a wallet"},{"location":"posts/never-use-a-brain-wallet/#some-other-cracked-wallets","text":"<p>I explored a little bit more and it's surprising to find out how easy it is to crack a wallet this way. Below is a table of some passphrases and their associated keys and addresses.</p>    Passphrase Private Key Address Used     satoshi da2876b3eb31edb4436fa4650673fc6f01f90de2f1793c4ec332b2387b09726f 1ADJqstUMBB5zFquWg19UqZ7Zc6ePCpzLE True   bitcoin 6b88c087247aa2f07ee1c5956b8e1a9f4c7f892a70e324f1bb3d161e05ca107b 1E984zyYbNmeuumzEdqT8VSL8QGJi3byAD True   hello world b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9 1CS8g7nwaxPPprb4vqcTVdLCuCRirsbsMb True   testing cf80cd8aed482d5d1527d7dc72fceff84e6326592848447d2dc0b0e87dfc9a90 1JdDsbYYRSpsTnBVgenruULVeUjt5z6WnR True   god 5723360ef11043a879520412e9ad897e0ebcb99cc820ec363bfecc9d751a1a99 1KxmSmcMTmPvU1qSLYpJLrqnSzBoQ53NXN True   terminator aa802f654e3ae7aaa1b73f8724056a05e2691accea8dd90057916080f84d7e93 18kvt3D6K1CG8MxGP6ke7q6vLU5NGpLZdR True   abc ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad 1NEwmNSC7w9nZeASngHCd43Bc5eC2FmXpn True    <p>And a lot of swear words are used as well, but I'm just going to skip them.</p> <p>Apart from the single world and short phrases, some people do use famous quotes. As an example, see this one from A Tale of Two Cities:</p>  <p>it was the best of times it was the worst of times</p>  <p>Its corresponding private key is <code>af8da705bfd95621983e5cf4232ac1ca0c79b47122e3defd8a98fa9a4387d985</code> and its address is 17WenQJaYvqCNumebQU54TsixWtQ1GQ4ND. It has received 1 BTC in total but again the balance is zero, lol.</p>","title":"Some other \"cracked\" wallets"},{"location":"posts/never-use-a-brain-wallet/#concluding-remark","text":"<p>Never use a brain wallet. Because if you can think of it, someone else might also be able to come up with same passphrase. But, if you are comfortable or absolutely sure that your passphrase is secret, feel free to use the script above and make yourself a wallet. \ud83d\ude0f</p>","title":"Concluding remark"},{"location":"posts/python-shared-memory-in-multiprocessing/","text":"<p>Python 3.8 introduced a new module <code>multiprocessing.shared_memory</code> that provides shared memory for direct access across processes. My test shows that it significantly reduces the memory usage, which also speeds up the program by reducing the costs of copying and moving things around.1</p>","title":"Python Shared Memory in Multiprocessing"},{"location":"posts/python-shared-memory-in-multiprocessing/#test","text":"<p>In this test, I generated a 240MB <code>numpy.recarray</code> from a <code>pandas.DataFrame</code> with <code>datetime</code>, <code>int</code> and <code>str</code> typed columns. I used <code>numpy.recarray</code> because it can preserve the <code>dtype</code> of each column, so that later I can reconstruct the same array from the buffer of shared memory.</p> <p>I performed a simple <code>numpy.nansum</code> on the numeric column of the data using two methods. The first method uses <code>multiprocessing.shared_memory</code> where the 4 spawned processes directly access the data in the shared memory. The second method passes the data to the spawned processes, which effectively means each process will have a separate copy of the data.</p>","title":"Test"},{"location":"posts/python-shared-memory-in-multiprocessing/#test-result","text":"<p></p> <p>A quick run of the test code below shows that the first method based on <code>shared_memory</code> uses minimal memory (peak usage is 0.33MB) and is much faster (2.09s) than the second one where the entire data is copied and passed into each process (peak memory usage of 1.8G and takes 216s). More importantly, the memory usage under the second method is consistently high.</p>","title":"Test Result"},{"location":"posts/python-shared-memory-in-multiprocessing/#test-code","text":"<pre><code>from multiprocessing.shared_memory import SharedMemory\nfrom multiprocessing.managers import SharedMemoryManager\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom multiprocessing import current_process, cpu_count, Process\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport tracemalloc\nimport time\n\n\ndef work_with_shared_memory(shm_name, shape, dtype):\n    print(f'With SharedMemory: {current_process()=}')\n    # Locate the shared memory by its name\n    shm = SharedMemory(shm_name)\n    # Create the np.recarray from the buffer of the shared memory\n    np_array = np.recarray(shape=shape, dtype=dtype, buf=shm.buf)\n    return np.nansum(np_array.val)\n\n\ndef work_no_shared_memory(np_array: np.recarray):\n    print(f'No SharedMemory: {current_process()=}')\n    # Without shared memory, the np_array is copied into the child process\n    return np.nansum(np_array.val)\n\n\nif __name__ == \"__main__\":\n    # Make a large data frame with date, float and character columns\n    a = [\n        (datetime.today(), 1, 'string'),\n        (datetime.today(), np.nan, 'abc'),\n    ] * 5000000\n    df = pd.DataFrame(a, columns=['date', 'val', 'character_col'])\n    # Convert into numpy recarray to preserve the dtypes (1)\n    np_array = df.to_records(index=False, column_dtypes={'character_col': 'S6'})\n    del df\n    shape, dtype = np_array.shape, np_array.dtype\n    print(f\"np_array's size={np_array.nbytes/1e6}MB\")\n\n    # With shared memory\n    # Start tracking memory usage\n    tracemalloc.start()\n    start_time = time.time()\n    with SharedMemoryManager() as smm:\n        # Create a shared memory of size np_arry.nbytes\n        shm = smm.SharedMemory(np_array.nbytes)\n        # Create a np.recarray using the buffer of shm\n        shm_np_array = np.recarray(shape=shape, dtype=dtype, buf=shm.buf)\n        # Copy the data into the shared memory\n        np.copyto(shm_np_array, np_array)\n        # Spawn some processes to do some work\n        with ProcessPoolExecutor(cpu_count()) as exe:\n            fs = [exe.submit(work_with_shared_memory, shm.name, shape, dtype)\n                  for _ in range(cpu_count())]\n            for _ in as_completed(fs):\n                pass\n    # Check memory usage\n    current, peak = tracemalloc.get_traced_memory()\n    print(f\"Current memory usage {current/1e6}MB; Peak: {peak/1e6}MB\")\n    print(f'Time elapsed: {time.time()-start_time:.2f}s')\n    tracemalloc.stop()\n\n    # Without shared memory\n    tracemalloc.start()\n    start_time = time.time()\n    with ProcessPoolExecutor(cpu_count()) as exe:\n        fs = [exe.submit(work_no_shared_memory, np_array)\n              for _ in range(cpu_count())]\n        for _ in as_completed(fs):\n            pass\n    # Check memory usage\n    current, peak = tracemalloc.get_traced_memory()\n    print(f\"Current memory usage {current/1e6}MB; Peak: {peak/1e6}MB\")\n    print(f'Time elapsed: {time.time()-start_time:.2f}s')\n    tracemalloc.stop()\n</code></pre> <ol> <li>Check the note below for preventing segfault.</li> </ol>","title":"Test Code"},{"location":"posts/python-shared-memory-in-multiprocessing/#important-note","text":"<p>Warning</p> <p>A very important note about using <code>multiprocessing.shared_memory</code>, as at June 2020, is that the <code>numpy.ndarray</code> cannot have a <code>dtype=dtype('O')</code>. That is, the <code>dtype</code> cannot be <code>dtype(object)</code>. If it is, there will be a segmentation fault when child processes try to access the shared memory and dereference it. It happens when the column contains strings.</p>  <p>To solve this problem, you need to specify the <code>dtype</code> in <code>df.to_records()</code>. For example:</p> <pre><code>np_array = df.to_records(index=False\uff0ccolumn_dtypes={'character_col': 'S6'})\n</code></pre> <p>Here, we specify that <code>character_col</code> contains strings of length 6. If it contains Unicode, we can use <code>'U6'</code> instead. Longer strings will then be truncated at the specified length. As such, there won't be anymore segfault.</p>   <ol> <li> <p>This test is performed on a 2017 12-inch MacBook with 1.3 GHz Dual-Core Intel Core i5 and 8 GB 1867 MHz LPDDR3 RAM.\u00a0\u21a9</p> </li> </ol>","title":"Important Note"},{"location":"posts/reconciliation-of-black-scholes-variants/","text":"<p>This note is just to show that the different variants of Black-Scholes formula in textbook and tutorial solutions are in fact the same.</p> <ul> <li>S: Underlying share price</li> <li>tt: Time to maturity</li> <li>\\sigma\\sigma: Standard deviation of underlying share price</li> <li>KK: Exercise price</li> <li>r_fr_f: Risk-free rate</li> </ul>","title":"Reconciliation of Black-Scholes Variants"},{"location":"posts/reconciliation-of-black-scholes-variants/#variant-1","text":"<p>This is the one shown in our formula sheet, and is also the traditional presentation of Black-Scholes model.</p>   \\begin{equation} C=SN(d_1)-N(d_2)Ke^{-r_f t} \\end{equation}   \\begin{equation} C=SN(d_1)-N(d_2)Ke^{-r_f t} \\end{equation}     \\begin{equation} d_1=\\frac{ln(\\frac{S}{K})+(r_f+\\frac{\\sigma^2}{2})t}{\\sigma \\sqrt{t}} \\end{equation}   \\begin{equation} d_1=\\frac{ln(\\frac{S}{K})+(r_f+\\frac{\\sigma^2}{2})t}{\\sigma \\sqrt{t}} \\end{equation}     \\begin{equation} d_2=d_1 - \\sigma \\sqrt{t} \\end{equation}   \\begin{equation} d_2=d_1 - \\sigma \\sqrt{t} \\end{equation}","title":"Variant 1"},{"location":"posts/reconciliation-of-black-scholes-variants/#variant-2","text":"<p>This one comes from textbook, and looks slightly different in that PV(K)PV(K) replaces KK in the natural logarithm.</p>   \\begin{equation} C=SN(d_1)-N(d_2)PV(K) \\end{equation}  \\begin{equation} C=SN(d_1)-N(d_2)PV(K) \\end{equation}    \\begin{equation} d_1=\\frac{ln(\\frac{S}{PV(K)})}{\\sigma \\sqrt{t}}+\\frac{\\sigma \\sqrt{t}}{2} \\end{equation}   \\begin{equation} d_1=\\frac{ln(\\frac{S}{PV(K)})}{\\sigma \\sqrt{t}}+\\frac{\\sigma \\sqrt{t}}{2} \\end{equation}     \\begin{equation} d_2=d_1 - \\sigma \\sqrt{t} \\end{equation}   \\begin{equation} d_2=d_1 - \\sigma \\sqrt{t} \\end{equation}   <p>However, it's in fact easy to show that d_1d_1 in eq. (5) is the same as in eq. (2): Under continuous compounding, PV(K)=Ke^{-r_f t}PV(K)=Ke^{-r_f t}:</p>   \\begin{align} d_1 &amp;=\\frac{ln(\\frac{S}{PV(K)})}{\\sigma \\sqrt{t}}+\\frac{\\sigma \\sqrt{t}}{2}\\newline &amp;=\\frac{ln(\\frac{S}{Ke^{-r_f t}})}{\\sigma \\sqrt{t}} +\\frac{\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{Ke^{-r_f t}})+\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{K})+r_f t+\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{K})+(r_f+\\frac{\\sigma^2}{2})t}{\\sigma \\sqrt{t}}=eq. (2) \\end{align}   \\begin{align} d_1 &amp;=\\frac{ln(\\frac{S}{PV(K)})}{\\sigma \\sqrt{t}}+\\frac{\\sigma \\sqrt{t}}{2}\\newline &amp;=\\frac{ln(\\frac{S}{Ke^{-r_f t}})}{\\sigma \\sqrt{t}} +\\frac{\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{Ke^{-r_f t}})+\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{K})+r_f t+\\frac{\\sigma^2}{2}t}{\\sigma \\sqrt{t}}\\newline &amp;=\\frac{ln(\\frac{S}{K})+(r_f+\\frac{\\sigma^2}{2})t}{\\sigma \\sqrt{t}}=eq. (2) \\end{align}   <p>Therefore, the two variants are effectively the same under continuous compounding. \u00a0</p>","title":"Variant 2"},{"location":"posts/textual-analysis-on-sec-filings/","text":"<p>Nowadays top journals favour more granular studies. Sometimes it's useful to dig into the raw SEC filings and perform textual analysis. This note documents how I download all historical SEC filings via EDGAR and conduct some textual analyses.</p>  <p>Tip</p> <p>If you don't require a very customized textual analysis, you should try for example SeekEdgar.com.</p>","title":"Textual Analysis on SEC Filings"},{"location":"posts/textual-analysis-on-sec-filings/#1-build-a-master-index-of-sec-filings","text":"<p>I use the <code>python-edgar</code> to download quarterly zipped index files to <code>./edgar-idx</code>.</p> <pre><code>$ mkdir ~/edgar &amp;&amp; cd ~/edgar\n$ git clone https://github.com/edouardswiac/python-edgar.git\n$ python ./python-edgar/run.py -d ./edgar-idx\n</code></pre> <p>Then merge the downloaded tsv files into a master file using <code>cat</code>.</p> <pre><code>$ cat ./edgar-idx/*.tsv &gt; ./edgar-idx/master.tsv\n$ du -h ./edgar-idx/master.tsv\n</code></pre> <p>The resulting <code>master.tsv</code> is about 2.6G as at Feb 2020. I then use the following python script to build a SQLite database for more efficient query.</p> <pre><code># Load index files in `edgar-idx` to a sqlite database.\n\nimport sqlite3\n\nEDGAR_BASE = \"https://www.sec.gov/Archives/\"\n\ndef parse(line):\n    # each line: \"cik|firm_name|file_type|date|url_txt|url_html\"\n    # an example:\n    # \"99780|TRINITY INDUSTRIES INC|8-K|2020-01-15|edgar/data/99780/0000099780-\\\n    # 20-000008.txt|edgar/data/99780/0000099780-20-000008-index.html\"\n    line = tuple(line.split('|')[:5])\n    l = list(line)\n    l[-1] = EDGAR_BASE + l[-1]\n    return tuple(l)\n\nif __name__ == '__main__':\n    conn = sqlite3.connect(r\"edgar-idx.sqlite3\")\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS edgar_idx \n        (cik TEXT, firm_name TEXT, file_type TEXT, date DATE, url TEXT,\n        PRIMARY KEY(cik, file_type, date));''')\n\n    filename = './edgar-idx/master.tsv'\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n\n    data = [parse(line) for line in lines]\n    c.executemany('INSERT OR IGNORE INTO edgar_idx \\\n        (cik, firm_name, file_type, date, url) VALUES (?,?,?,?,?)', data)\n\n    conn.commit()\n    conn.close()\n</code></pre>","title":"1. Build a master index of SEC filings"},{"location":"posts/textual-analysis-on-sec-filings/#2-download-filings-from-edgar","text":"<p>I write the following script to download filings from EDGAR. Note that this script is only a skeleton. The full implementation has proper logging, speed control and detailed error handling. For example, you'll need to keep track of failures and re-download them later. </p>  <p>Warning</p> <p>As per SEC's policy, you should limit concurrent requests to below 10 per second. Hence, there is no need to use a proxy pool, such as <code>Scylla</code>. </p> <p>This example script download all 8-K files to <code>./data/{cik}/{file_type}/{date}.txt.gz</code>.</p> <p>Compression is highly recommended unless you've TBs of free disk space!</p>  <pre><code># Download all 8-K filings.\n\nimport os\nimport sqlite3\nimport requests\nimport concurrent.futures\nimport gzip\nimport tqdm\n\ndef download(job):\n    cik, _, file_type, date, url = job\n    try:\n        res = requests.get(url)\n        filename = f'./data/{cik}/{file_type}/{date}.txt.gz'\n        if res.status_code == 200:\n            with gzip.open(filename, 'wb') as f:\n                f.write(res.content)\n    except Exception:\n        pass\n\nif __name__ == \"__main__\":\n    # select what to download\n    conn = sqlite3.connect(r\"edgar-idx.sqlite3\")\n    c = conn.cursor()\n    c.execute('SELECT * FROM edgar_idx WHERE file_type=\"8-K\";')\n    jobs = c.fetchall()\n    conn.close()\n    # start downloading\n    progress = tqdm.tqdm(total=len(jobs))\n    futures = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as exe:\n        for job in jobs:\n            cik, _, file_type, date, url = job\n            filename = f'./data/{cik}/{file_type}/{date}.txt.gz'\n            os.makedirs(os.path.dirname(filename), exist_ok=True)\n            if os.path.exists(filename):\n                progress.update()\n            else:\n                f = exe.submit(download, job)\n                f.add_done_callback(progress.update)\n                futures.append(f)\n    for f in concurrent.futures.as_completed(futures):\n        pass\n</code></pre>","title":"2. Download filings from EDGAR"},{"location":"posts/textual-analysis-on-sec-filings/#3-example-textual-analyses","text":"<p>The downloaded txt files are the text version of filings htmls, which generally are well structured. Specifically, each filing is structured as:</p> <pre><code>&lt;SEC-DOCUMENT&gt;\n  &lt;SEC-HEADER&gt;&lt;/SEC-HEADER&gt;\n  &lt;DOCUMENT&gt;\n    &lt;TYPE&gt;\n      &lt;SEQUENCE&gt;\n        &lt;FILENAME&gt;\n          &lt;DESCRIPTION&gt;\n            &lt;TEXT&gt;\n            &lt;/TEXT&gt;\n          &lt;/DESCRIPTION&gt;\n        &lt;/FILENAME&gt;\n      &lt;/SEQUENCE&gt;\n    &lt;/TYPE&gt;\n  &lt;/DOCUMENT&gt;\n  &lt;DOCUMENT&gt;&lt;/DOCUMENT&gt;\n  &lt;DOCUMENT&gt;&lt;/DOCUMENT&gt;\n  ...\n&lt;/SEC-DOCUMENT&gt;\n</code></pre>  Example <pre><code>&lt;SEC-DOCUMENT&gt;\n    &lt;SEC-HEADER&gt;&lt;/SEC-HEADER&gt;\n    &lt;DOCUMENT&gt;\n        &lt;TYPE&gt;8-K\n            &lt;SEQUENCE&gt;1\n                &lt;FILENAME&gt;f13478e8vk.htm\n                    &lt;DESCRIPTION&gt;FORM 8-K\n                        &lt;TEXT&gt;\n                        ...\n                        &lt;/TEXT&gt;\n                    &lt;/DESCRIPTION&gt;\n                &lt;/FILENAME&gt;\n            &lt;/SEQUENCE&gt;\n        &lt;/TYPE&gt;\n    &lt;/DOCUMENT&gt;\n    &lt;DOCUMENT&gt;\n        &lt;TYPE&gt;EX-99.1\n            &lt;SEQUENCE&gt;2\n                &lt;FILENAME&gt;f13478exv99w1.htm\n                    &lt;DESCRIPTION&gt;EXHIBIT 99.1\n                        &lt;TEXT&gt;\n                        ...\n                        &lt;/TEXT&gt;\n                    &lt;/DESCRIPTION&gt;\n                &lt;/FILENAME&gt;\n            &lt;/SEQUENCE&gt;\n        &lt;/TYPE&gt;\n    &lt;/DOCUMENT&gt;\n    &lt;DOCUMENT&gt;&lt;/DOCUMENT&gt;\n    ...\n&lt;/SEC-DOCUMENT&gt;\n</code></pre>","title":"3. Example textual analyses"},{"location":"posts/textual-analysis-on-sec-filings/#31-extract-all-items-reported-in-8-k-filings-since-2004","text":"<p>Since 2004, SEC requires companies to file 8-K within 4 business days of many types of events. For a short description, see SEC's fast answer to Form 8-K. The detailed instruction (PDF) is available at here.</p> <p>To extract all items reported in each filing since 2004, there are several ways. First, I can use a regular expression to extract all <code>\"Item X.XX\"</code> from the 8-K <code>&lt;DOCUMENT&gt;</code>. Or, I can take advantage of the information in <code>&lt;SEC-HEADER&gt;</code>. Below is an example <code>&lt;SEC-HEADER&gt;</code>1, of which the lines of <code>ITEM INFORMATION</code> actually describe the items reported in the filing. </p> <pre><code>&lt;SEC-HEADER&gt;0000079732-02-000036.hdr.sgml : 20020802\n&lt;ACCEPTANCE-DATETIME&gt;20020802082752\nACCESSION NUMBER:       0000079732-02-000036\nCONFORMED SUBMISSION TYPE:  8-K\nPUBLIC DOCUMENT COUNT:      4\nCONFORMED PERIOD OF REPORT: 20020801\nITEM INFORMATION:       Changes in control of registrant\nITEM INFORMATION:       Financial statements and exhibits\nFILED AS OF DATE:       20020802\n\nFILER:\n\n    COMPANY DATA:   \n        COMPANY CONFORMED NAME:         ATLANTIC CITY ELECTRIC CO\n        CENTRAL INDEX KEY:          0000008192\n        STANDARD INDUSTRIAL CLASSIFICATION: ELECTRIC SERVICES [4911]\n        IRS NUMBER:             210398280\n        STATE OF INCORPORATION:         NJ\n        FISCAL YEAR END:            1231\n\n    FILING VALUES:\n        FORM TYPE:      8-K\n        SEC ACT:        1934 Act\n        SEC FILE NUMBER:    001-03559\n        FILM NUMBER:        02717802\n\n    BUSINESS ADDRESS:   \n        STREET 1:       800 KING STREET\n        STREET 2:       PO BOX 231\n        CITY:           WILMINGTON\n        STATE:          DE\n        ZIP:            19899\n        BUSINESS PHONE:     6096454100\n\n    MAIL ADDRESS:   \n        STREET 1:       800 KING STREET\n        STREET 2:       PO BOX 231\n        CITY:           WILMINGTON\n        STATE:          DE\n        ZIP:            19899\n&lt;/SEC-HEADER&gt;\n</code></pre> <p>Following this strategy, I write the code below to extract all items reported in 8-K filings since 2004. I didn't use regex for this task because the text portion of the filing is actually dirty. For instance, you'll need to remove all html tags, and be careful about the \"non-breaking space\", <code>&amp;nbsp;</code>, etc. My experience is that using <code>&lt;SEC-HEADER&gt;</code> for this task is the best.</p> <pre><code># Extract all items reported in 8-K filings since 2004.\nimport os\nimport gzip\nimport tqdm\nimport sqlite3\nimport concurrent.futures\n\n\nBASE_DIR = './data'\nFILE_TYPE = '8-K'\nDB = \"result.sqlite3\"\n\n\ndef walk_dirpath(cik, file_type):\n    \"\"\" Yield paths of all files for a given cik and file type \"\"\"\n    for root, _, files in os.walk(os.path.join(BASE_DIR, cik, file_type)):\n        for filename in files:\n            yield os.path.join(root, filename)\n\n\ndef regsearch(cik):\n    matches = []\n    for filepath in walk_dirpath(cik, FILE_TYPE):\n        date = os.path.split(filepath)[1].strip('.txt.gz')\n        if int(date.split('-')[0]) &lt; 2004:\n            continue\n        with gzip.open(filepath, 'rb') as f:\n            data = f.readlines()\n        ls = [l for l in data if l.startswith(b'ITEM INFORMATION')]\n        for l in ls:\n            item = l.decode().replace('\\t','').replace('ITEM INFORMATION:', '')\n            if len(item.strip()):\n                matches.append((cik, FILE_TYPE, date, item.strip()))\n    return matches\n\n\nif __name__ == \"__main__\":\n    conn = sqlite3.connect(DB)\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS files_all_items\n        (cik TEXT, file_type TEXT, date DATE, item TEXT,\n        PRIMARY KEY(cik, file_type, date, item));''')\n    conn.commit()\n\n    _, ciks, _ = next(os.walk(BASE_DIR))\n    progress = tqdm.tqdm(total=len(ciks))\n    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as exe:\n        futures = [exe.submit(regsearch, cik) for cik in ciks]\n        for f in concurrent.futures.as_completed(futures):\n            res = f.result()\n            c.executemany(\n                \"INSERT OR IGNORE INTO files_all_items \\\n                    (cik, file_type, date, item) VALUES (?,?,?,?)\", res)\n            conn.commit()\n            progress.update()\n\n    conn.close()\n</code></pre>","title":"3.1 Extract all items reported in 8-K filings since 2004"},{"location":"posts/textual-analysis-on-sec-filings/#32-find-all-8-k-filings-with-item-101-andor-item-203","text":"<p>To get those filings that have either:</p> <ul> <li>Item 1.01 Entry into a Material Definitive Agreement, or</li> <li>Item 2.03 Creation of a Direct Financial Obligation or an Obligation under an   Off-Balance Sheet Arrangement of a Registrant</li> </ul> <p>I run the following SQL query:</p> <pre><code>-- SQLite\nCREATE TABLE `files_with_items_101_or_203` AS\nSELECT DISTINCT cik, file_type, date\nFROM `files_all_items`\nWHERE\n    instr(lower(item), \"creation of a direct financial obligation\") &gt; 0 OR\n    instr(lower(item), \"entry into a material definitive agreement\") &gt; 0\nORDER BY cik, file_type, date;\n</code></pre> <p>To get those with both items, use the following query:</p> <pre><code>-- SQLite\nCREATE TABLE `files_with_items_101_and_203` AS\nSELECT cik, file_type, date\nFROM `files_all_items`\nWHERE\n    instr(lower(item), \"creation of a direct financial obligation\") &gt; 0 OR\n    instr(lower(item), \"entry into a material definitive agreement\") &gt; 0\nGROUP BY cik, file_type, date\nHAVING count(*) &gt; 1\nORDER BY cik, file_type, date;\n</code></pre>","title":"3.2 Find all 8-K filings with Item 1.01 and/or Item 2.03"},{"location":"posts/textual-analysis-on-sec-filings/#33-nini-smith-and-sufi-2009","text":"<p>This example code finds the appearance of any of the 10 search words used in \"Creditor control rights and firm investment policy\" by Nini, Smith and Sufi (JFE 2009), which is used to identify the loan contracts as attached in the SEC filing.</p> <pre><code>import re\nimport os\nimport sys\nimport gzip\nimport tqdm\nimport sqlite3\nimport logging\nimport concurrent.futures\n\nlogging.basicConfig(stream=sys.stdout, level=logging.WARN)\n\nBASE_DIR = './data'\nFILE_TYPE = '10-Q'\nDB = \"result.sqlite3\"\n\n# Regex pattern used to remove html tags\ncleanr = re.compile(b'&lt;.*?&gt;|&amp;([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n\n# Regex pattern used to find the appearance of any of the 10 search words used\n# in \"Creditor control rights and firm investment policy\"\n# by Nini, Smith and Sufi (JFE 2009)\n# pat_10_words = r\"CREDIT FACILITY|REVOLVING CREDIT|(CREDIT|LOAN|(LOAN (AND|&amp;) \\\n#    SECURITY)|(FINANCING (AND|&amp;) SECURITY)|CREDIT (AND|&amp;) GUARANTEE) AGREEMENT\"\nNSS_10_words = ['credit facility',\n                'revolving credit',\n                'credit agreement',\n                'loan agreement',\n                'loan and security agreement',\n                'loan &amp; security agreement',\n                'credit and guarantee agreement',\n                'credit &amp; guarantee agreement',\n                'financing and security agreement',\n                'financing &amp; security agreement']\nNSS_10_words_str = '|'.join([word.upper() for word in NSS_10_words])\npat_10_words = re.compile(NSS_10_words_str.encode())\n\n# Regex pattern used in this search\npattern = pat_10_words\n\n\ndef walk_dirpath(cik, file_type):\n    \"\"\" Yield paths of all files for a given cik and file type \"\"\"\n    for root, _, files in os.walk(os.path.join(BASE_DIR, cik, file_type)):\n        for filename in files:\n            yield os.path.join(root, filename)\n\n\ndef regsearch(cik):\n    matches = []\n    for filepath in walk_dirpath(cik, FILE_TYPE):\n        date = os.path.split(filepath)[1].strip('.txt.gz')\n        try:\n            with gzip.open(filepath, 'rb') as f:\n                data = b' '.join(f.read().splitlines())\n                data = re.sub(cleanr, b'', data)\n            match = pattern.search(data)\n            if match:\n                matches.append((cik, FILE_TYPE, date))\n                logging.info(f'{filepath}, {match.group()}')\n        except Exception as e:\n            logging.error(f'failed at {filepath}, {e}')\n    return matches\n\n\nif __name__ == \"__main__\":\n\n    conn = sqlite3.connect(DB)\n    c = conn.cursor()\n    # create a table to store the indices\n    c.execute('''CREATE TABLE IF NOT EXISTS files_with_10_words\n        (cik TEXT, file_type TEXT, date DATE,\n        PRIMARY KEY(cik, file_type, date));''')\n    conn.commit()\n    _, ciks, _ = next(os.walk(BASE_DIR))\n    progress = tqdm.tqdm(total=len(ciks))\n    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as exe:\n        futures = [exe.submit(regsearch, cik) for cik in ciks]\n        for f in concurrent.futures.as_completed(futures):\n            matches = f.result()\n            c.executemany(\n                \"INSERT OR IGNORE INTO files_with_10_words \\\n                    (cik, file_type, date) VALUES (?,?,?)\", matches)\n            conn.commit()\n            progress.update()\n    conn.close()\n    logging.info('complete')\n</code></pre>   <ol> <li> <p>The original file is at https://www.sec.gov/Archives/edgar/data/0000008192/0000079732-02-000036.txt \u21a9</p> </li> </ol>","title":"3.3 Nini, Smith and Sufi (2009)"},{"location":"posts/use-sas-macros-on-wrds/","text":"<p>The Wharton Research Data Services (WRDS) provides quite a handful of SAS macros that can be used directly. This article explains how to use those handy macros on WRDS when you use remote submission to run your code on the WRDS cloud. Lastly, it explains how to load and use third-party SAS macros from a URL.</p>","title":"Use SAS Macros on WRDS"},{"location":"posts/use-sas-macros-on-wrds/#prerequisite","text":"<p>Before everything, just make sure that this <code>autoexec.sas</code> is located in the home folder on your WRDS cloud.</p> <pre><code>*  The library name definitions below are used by SAS;\n*  Assign default libref for WRDS (Wharton Research Data Services);\n%include '/wrds/lib/utility/wrdslib.sas';\noptions sasautos=('/wrds/wrdsmacros/', SASAUTOS) MAUTOSOURCE;\n</code></pre> <p>This code runs automatically when you've connected to the WRDS cloud. The first line assigns the default library references for you to use, e.g. <code>comp</code> for Compustat. The second line makes available the macros. A list of these handy macros is available at the WRDS documentation.</p> <p>If you don't have this SAS code in the home folder, simply create one there or you can choose to include these two lines of code in your remotely submitted code.</p>","title":"Prerequisite"},{"location":"posts/use-sas-macros-on-wrds/#simple-usage","text":"<p>Let's say we want to winsorize a dataset by using the macro provided by WRDS (full code). Below is an example of winsorizing Total Assets <code>AT</code> of Compustat sample by fiscal year from 1980 to 2018.</p> <pre><code>%let wrds=wrds-cloud.wharton.upenn.edu 4016;\noptions comamid=TCP remote=WRDS;\nsignon username=_prompt_;\n\nrsubmit;\n\n/* Create a dataset in the work directory */\ndata work.funda(keep=gvkey fyear at);\n    set comp.funda;\n    if 1980 &lt;= fyear &lt;= 2018;\n    /* Generic filter */\n    if indfmt='INDL' and datafmt='STD' and popsrc='D' and consol='C';\nrun;\n\n/* Invoke the macro */\n/* The documentation is available at:\n   https://wrds-www.wharton.upenn.edu/pages/support/research-wrds/macros/wrds-macros-winsorize/ */\n%WINSORIZE(INSET=funda,OUTSET=funda_w,SORTVAR=fyear,VARS=at,PERC1=1,TRIM=0);\n\n/* Before the winsorization */\nproc means data=work.funda; by fyear; var at; \noutput out=funda_before_win min= mean= max= / autoname; run;\n/* After the winsorization */\nproc means data=work.funda_w; by fyear; var at;\noutput out=funda_after_win min= mean= max= / autoname; run;\n\nproc print data=funda_before_win;\nproc print data=funda_after_win; run;\n\nendrsubmit;\nsignoff;\n</code></pre> <p>Invoking the macro is as simple as a single line:</p> <pre><code>%WINSORIZE(INSET=funda,OUTSET=funda_w,SORTVAR=fyear,VARS=at,PERC1=1,TRIM=0);\n</code></pre> <p>However, one thing to note about this particular winsorization macro by WRDS is that a variable named <code>a</code> is used in line 57 and 59. So if the <code>INSET</code> has a variable named <code>a</code> as well, there\u2019ll be possible data integrity issue. Hence, I prefer to use another version described in my other post Winsorization in SAS.</p>","title":"Simple usage"},{"location":"posts/use-sas-macros-on-wrds/#load-sas-macros-from-url","text":"<p>I tend to collect and store all useful macros on my personal server, hence I don't need to worry about a loss of or changes to the macros. To use these macros, simply include them before invoking.</p> <pre><code>filename winsor url \"https://mingze-gao.com/utils/winsor.sas\";\n%include winsor;\n</code></pre> <p>Then, I can simply call <code>winsor</code> as below.</p> <pre><code>%let winsVars = tac inv_at_l drev drevadj ppe roa;\n%winsor(dsetin=work.funda, dsetout=work.funda_wins, byvar=fyear, vars=&amp;winsVars, type=winsor, pctl=1 99);\n</code></pre>","title":"Load SAS macros from URL"},{"location":"posts/what-it-takes-to-be-a-ceo/","text":"<p>Taking up the position of CEO means more than pressure from the board and investors. You\u2019ll also face heavy scrutiny from academia. Whether or not a firm\u2019s hiring and compensation committees use them as a reference, here are some of the findings that you may want to be aware of. </p>","title":"What it takes to be a CEO? A fun survey of literature."},{"location":"posts/what-it-takes-to-be-a-ceo/#upon-birth","text":"<p>There are many things determined when you\u2019re born. It\u2019ll be naive to think that they matter less than anything else. A starter example is the Journal of Financial Economics paper \"Are CEOs born leaders? Lessons from traits of a million individuals\" by Adams, Keloharju and Knupfer (2018).</p>","title":"Upon birth"},{"location":"posts/what-it-takes-to-be-a-ceo/#1-birthday-month","text":"<p>Birth month affects school entry, which affects whether you are relatively older in the class. If you are born after the cutoff month, you'll have to wait for another year for entry. But this extra year buys you some more time to develop, which makes you more confident than the younger peers. This increased confidence is linked to adult labor market outcomes. Bai, Ma, Mullally and Solomon (2019 JFE) find that in mutual fund industry, it's associated with better stock selection and fund performance. These relatively older fund managers also appear more confident in photographs and display more confident behaviours: making larger bets, window dressing their holdings less, and so on.</p>","title":"1. Birthday (month)"},{"location":"posts/what-it-takes-to-be-a-ceo/#2-birth-order","text":"<p>The birth order also matters: negative associations between birth order and intelligence level have been found in numerous studies. More frankly, first born kids tend to have higher IQ scores. Kristensen and Bjerkedal (2007 Science) show that this is more dependent on social rank in the family where they receive more-favorable family interaction and stimulation. However, birth order is still the most prominent observable factor. Custodio and Siegel (2018) published a working paper where they find CEOs are more likely to be the first-born child of their family, and the results hold for both family and non-family firms, though thankfully the advantage of being first-born seems to decay over time.</p>","title":"2. Birth order"},{"location":"posts/what-it-takes-to-be-a-ceo/#3-gender","text":"<p>Studies on CEO gender difference and its relation with firm risk-taking, capital allocation, accounting conservatism, corporate social responsibility, and so on are plenty. Genearlly it is shown that male executives are overconfident relative to female executives (Huang and Kisgen, 2013 JFE), and we know that overconfidence is not necessarily a good thing. Firms run by female CEOs use lower leverage and have less volative earnings, (Faccio, Marchica and Mura 2016 JCF), and there are a lot more differences in terms of firm operational, financial, and M&amp;A performances. Tate and Yang (2015 JFE) show that female leaders cultivate more female-friendly cultures inside their firms.  Moreover, (having) a female director may bring a firm more access to external finance. Goldman Sachs announced on 23 January 2020 that they won't take companies public anymore unless they have at least one \"diverse\" board member.</p>","title":"3. Gender"},{"location":"posts/what-it-takes-to-be-a-ceo/#4-hometown","text":"<p>Everyone has some sort of hometown biases as well as hometown advantages. For example, Jiang, Qian and Yonker (2019 JFQA) find that CEOs are over twice as likely to acquire targets located in the states of their childhood homes than similar targets elsewhere. Smaller such deals are on average destorying shareholder value but bigger ones tend to be value enhancing. They conclude that CEOs may seek private benefits when acquiring small targets in their hometown but can also avoid poor deals due to hometown advantages. In a Chinese study, Kong, Pan, Tian and Zhang (2020 JCF) show that CEO's hometown connections increase access to trade credit and such effect is more pronounced for non-SOEs and firms in poor regions. In another Chinese study on commercial banks, Bian, Ji and Zhang (2019 JBF) find that a higher degree of dialect similarity between chairman and the CEO is associated with a higher ROA, ROE and a lower cost-to-income ratio, but is not with bank risks, CEO pay or lower pay-performance sensitivity. They conclude that speaking a similar dialect with the chairman doesn't undermine monitoring and reduces agency costs.</p>","title":"4. Hometown"},{"location":"posts/what-it-takes-to-be-a-ceo/#5-cultural-heritage","text":"<p>The place where you're born has even more profound implications through cultural heritage. Nguyen, Hagendorff and Eshraghi (2018 RFS) show that following shocks to industry competition, firms led by CEOs who are second- or third-generation immigrants are associated with a 6.2% higher profitability compared with the average firm. Their analysis attributes this effect to various cultural values that prevail in a CEO\u2019s ancestral country of origin. Through an epidemiological approach, Liu (2016 JFE) show that a corruption culture of corporate insiders' country of ancestry is associated with higher likelihood of earnings management, accounting fraud, option backdating and opportunistic insider trading.</p>","title":"5. Cultural heritage"},{"location":"posts/what-it-takes-to-be-a-ceo/#early-in-life","text":"<p>Many early life experiences are closely linked to natural and family endowment, yet others may be random and exogenous. Either way, early life experience is something that will have an impact on CEO behaviours later on.</p>","title":"Early in life"},{"location":"posts/what-it-takes-to-be-a-ceo/#1-education","text":"<p>No doubt education matters for everyone including CEO. Custodio and Metzger (2014 JFE) find that financial expert CEOs tend to be hired by more mature firms. Firms with financial expert CEOs hold less cash, more debt and engage in more share repurchases. They are able to raise external funds even when credit conditions are tight and their investments are less sensitive to cash flows. On the other hand, CEOs with an engineering (or scientific) education display higher investment-cash flow sensitivity (Malmendier and Tate (2005 JFE)). Similar findings appear in banking sector as shown by King, Srivastav and Williams (2016 JCF) focusing on CEO's MBA quality. Moreover, education offers more than just knowledge and skills. Although Khanna, Kim and Lu (2015 JF) do not find evidence that connections and network ties developed during education are associated with corporate fraud, such CEO connectedness certainly affect information sharing, investments and so on. Wang and Yin (2018 JCF) find that CEOs tend to initiate more, larger and better M&amp;A deals where target firms are headquarted in those states where they received their undergraduate and graduate degrees.</p>","title":"1. Education"},{"location":"posts/what-it-takes-to-be-a-ceo/#2-disaster-experience","text":"<p>People are shaped by their experiences and disasters are a major one. Several Chinese studies have shown that CEOs who have experienced famine are more risk-averse and hold more cash. They conduct less takeovers, but the M&amp;A deals tend to perform better when they do according to Zhang (2017 PBFJ). Such risk aversion can sometimes be good as Hu, Li and Luo (2019 PBFJ) find that firms governed by CEOs experienced great famine have higher market value during crisis. But generally speaking this effect is mitigated by higher education background and is weaker in SOEs, as well as for CEOs who also experienced economic reform, which is shown to increase CEO's risk tolerance by Hao, Wang, Chou and Ko (2018 IRF). American CEOs, for sure, are no exception. A famous Journal of Finance paper \"what doesn't kill you will only make you more risk-loving\" by Bernile, Bhagwat and Rau (2016) concludes like its title. But more importantly, CEOs who experienced fatal disasters without extremely negative consequences lead firms more aggressively, whereas CEOs who witness the extreme downside of disasters behave more conservatively.</p>","title":"2. Disaster experience"},{"location":"posts/what-it-takes-to-be-a-ceo/#3-academic-military-and-other-experiences","text":"<p>Apart from previous industry experience, researchers also examined the role of many other executive experiences. Shen, Lan, Xiong, Lv and Jian (2019 Economic Modelling) find that top management team's academic experience promotes corporate innovations and attribute the effect to improved internal control level and reduced information asymmetry. Benmelech and Frydman (2015 JFE) find that military service could make CEOs pursue lower coporate investment, and military CEOs are less likely to be involved in corporate fraudulent activity, performing better during industry downturns.</p>","title":"3. Academic, military and other experiences"},{"location":"posts/what-it-takes-to-be-a-ceo/#personality-traits","text":"","title":"Personality traits"},{"location":"posts/what-it-takes-to-be-a-ceo/#1-masculinity","text":"<p>Masculinity is a long-studied factor in many fields of research and there're also many interesting papers specifically on male CEOs. Since CEO testosterone levels cannot be tested directly, a common proxy in the literature is the facial width-to-height ratio (fWHR). Jia, Van Lent and Zeng (2014 JAR) find that a higher fWHR of a male CEO, representing more masculine faces, is associated with more misreporting, predicts his firm's likelihood of being subject to SEC enforcement action and incidence of insider trading and option backdating. They also find that executive's facial masculinity is associated the likelihood of being named as a perpetrator by SEC. In a forthcoming European Financial Management paper by Kamiya, Kim and Park (2018), male CEOs' facial masculinity is found to be related to higher stock return volatility, higher financial leverage and more M&amp;A activities. A paper at the 2018 Academy of Managment Annual Meeting by Joshi, Misangyi, Rizzi and Neely (2018), however, find that masculinity does not have a direct effect on the firm's operational performance. The researchers also find that masculinity worked to the detriment of CEOs in female-dominated industries; less masculine CEOs also performed poorly in highly male-dominated environments.</p>","title":"1. Masculinity"},{"location":"posts/what-it-takes-to-be-a-ceo/#2-sensation-seeking-corruption-and-frugality","text":"<p>In \"desperate\" search for proxies and signals of CEO/manager quality and traits, studies have turned to some really interesting areas such as the cars they drive and whether they can fly airplanes. Brown, Lu, Ray and Teo (2018 JF) show that sensation-seeking hedge fund managers who own powerful sports cars take on more investment risks but do not deliver higher returns. \"Red Ferrari syndrome\", as described by Business Insider, February 2016. Unfortunately, some investors themselves are susceptible to sensation seeking and hence fuel the demand for such managers. Mironov (2015 JFE) has an interesting study and finds that if you can get away from a traffic violation through bribe, as a manager, you may deliver some outperformance through, for instance, tax evasion, because corruption sometimes promotes efficiency.</p> <p>Sunder, Sunder and Zhang (2017 JFE) look at pilot CEOs who fly airplanes as a hobby and find that they are significantly associated with better corporate innovation outcomes. They conclude that sensation seeking combines risk taking with a desire to pursue novel experiencecs and has been associated with creativity. Davidson, Dey and Smith (2015 JFE) even hired private investigators to collect data on executives' legal infractions and ownership of real estate, boats, luxury vehicles and motocycles. They find no direct evidence of a relation between executives' frugality and the propensity to perpetrate fraud. But there will be a relatively loose control environment characterized by relatively high and increasing probabilities of other insiders perpetrating fraud and unintentional material reporting errors during unfrugal CEOs' reigns.</p>","title":"2. Sensation-seeking, corruption and frugality"},{"location":"posts/what-it-takes-to-be-a-ceo/#3-creativity-and-innovation","text":"<p>One in five U.S. high-technology firms are led by CEOs with hands-on innovation experience as inventors. Islam and Zein (2020 JFE) show that firms led by \u201cInventor CEOs\u201d are associated with higher quality innovation, especially when the CEO is a high-impact inventor. During an inventor CEO's tenure, firms file a greater number of patents and more valuable patents in technology classes where the CEO's hands-on experience lies. It is possible that such inventor CEOs are more capable of evaluating, selecting and executing innovative investment projects.</p>","title":"3. Creativity and innovation"},{"location":"posts/what-it-takes-to-be-a-ceo/#family-marriage-and-fidelity","text":"","title":"Family, marriage and fidelity"},{"location":"posts/what-it-takes-to-be-a-ceo/#1-newborns-and-loss-of-family-members","text":"<p>\"Corporate executives managing some of the largest public companies in the U.S. are shaped by their daughters\". Cronqvist and Yu (2017 JFE) find that when a firm\u2019s CEO has a daughter, the corporate social responsibility rating (CSR) is about 9.1% higher, compared to a median firm. This finding perhaps reveals another plausibly exogenous determinant of CEO's styles. On the other hand, a loss of important family member poses a significant negative shock. In the 2020 AFA Annual Meeting, I encountered a paper by Liu, Shu, Sulaeman and Yeung (2019) who find that after deaths in the family, bereaved managers take significantly less risk. Firms managed by bereaved CEOs exhibit lower capital expenditures, fewer acquisitions, lower debt issuance and lower CEO ownership after the bereavement events.</p>","title":"1. Newborns and loss of family members"},{"location":"posts/what-it-takes-to-be-a-ceo/#2-marriage-divorce-and-infidelity","text":"<p>While previous studies focused on the cultural background of the CEOs themselves, another paper I encountered in AFA Annual Meeting by Antoniou, Cuculiza, Kumar and Yang (2019) incorporates CEO spouses into the research. They show that the high uncertainty avoidance of CEO spouses will influence CEOs\u2019 personal uncertainty avoidance, and then lead to less corporate risk-taking. Larcker, McCall and Tayan (2013) show that CEO's divorce is impactful because it causes loss of control due to sale of stocks for divorce settlement, affects productivity, and attitude towards defaults.</p> <p>One final interesting paper I want to mention to conclude this post is \"the geography of financial miscoundct\" by Parsons, Sulaeman and Titman (2018 JF). In 2015, the website Ashley Madison, whose target clients are married people seeking an extramarital affair, was hacked and there was a leak of 40 million user account data of name, address and billing information. The researchers use the data to measure the intensity of spousal infidelity of a local area and find that financial misconducts are strongly related to unfaithfulness in the city.</p>","title":"2. Marriage, divorce and (in)fidelity"},{"location":"posts/what-it-takes-to-be-a-ceo/#final-note","text":"<p>This short survey of CEO literature is not meant to be comprehensive, but to list a few very interesting papers that I find fun to read. I guess the message is that being a CEO means a lot more than managing the firm and stakeholders, and shareholders also need to open their minds and eyes.</p>  <p>A funny example. Next time hiring a CEO, other things equal, maybe you'll want a female immigrant who is the first-born kid and born in August, attended certain schools in certain areas, experienced natural diasters, served in military before, has a daughter and no sports cars, knows how to fly airplanes, loyal to her spouse from certain countries, and whose all family members are live and well...</p>","title":"Final note"},{"location":"posts/winsorization-in-sas/","text":"<p>These are two versions of winsorization in SAS, of which I recommend the first one.</p>","title":"Winsorization in SAS"},{"location":"posts/winsorization-in-sas/#version-1-unknown-author","text":"<pre><code>/*****************************************\nAuthor unknown - that is a pity because this macro is the best since sliced bread! \nTrim or winsorize macro\n* byvar = none for no byvar;\n* type  = delete/winsor (delete will trim, winsor will winsorize;\n*dsetin = dataset to winsorize/trim;\n*dsetout = dataset to output with winsorized/trimmed values;\n*byvar = subsetting variables to winsorize/trim on;\nSample usage:\n%winsor(dsetin=work.myDsetIn, byvar=fyear, \n        dsetout=work.myDsOut, vars=btm roa roe, type=winsor, pctl=1 99);\n****************************************/\n\n%macro winsor(dsetin=, dsetout=, byvar=none, vars=, type=winsor, pctl=1 99);\n\n%if &amp;dsetout = %then %let dsetout = &amp;dsetin;\n\n%let varL=;\n%let varH=;\n%let xn=1;\n\n%do %until ( %scan(&amp;vars,&amp;xn)= );\n    %let token = %scan(&amp;vars,&amp;xn);\n    %let varL = &amp;varL &amp;token.L;\n    %let varH = &amp;varH &amp;token.H;\n    %let xn=%EVAL(&amp;xn + 1);\n%end;\n\n%let xn=%eval(&amp;xn-1);\n\ndata xtemp;\n    set &amp;dsetin;\n    run;\n\n%if &amp;byvar = none %then %do;\n\n    data xtemp;\n        set xtemp;\n        xbyvar = 1;\n        run;\n\n    %let byvar = xbyvar;\n\n%end;\n\nproc sort data = xtemp;\n    by &amp;byvar;\n    run;\n\nproc univariate data = xtemp noprint;\n    by &amp;byvar;\n    var &amp;vars;\n    output out = xtemp_pctl PCTLPTS = &amp;pctl PCTLPRE = &amp;vars PCTLNAME = L H;\n    run;\n\ndata &amp;dsetout;\n    merge xtemp xtemp_pctl;\n    by &amp;byvar;\n    array trimvars{&amp;xn} &amp;vars;\n    array trimvarl{&amp;xn} &amp;varL;\n    array trimvarh{&amp;xn} &amp;varH;\n\n    do xi = 1 to dim(trimvars);\n\n        %if &amp;type = winsor %then %do;\n            if not missing(trimvars{xi}) then do;\n              if (trimvars{xi} &lt; trimvarl{xi}) then trimvars{xi} = trimvarl{xi};\n              if (trimvars{xi} &gt; trimvarh{xi}) then trimvars{xi} = trimvarh{xi};\n            end;\n        %end;\n\n        %else %do;\n            if not missing(trimvars{xi}) then do;\n              if (trimvars{xi} &lt; trimvarl{xi}) then delete;\n              if (trimvars{xi} &gt; trimvarh{xi}) then delete;\n            end;\n        %end;\n\n    end;\n    drop &amp;varL &amp;varH xbyvar xi;\n    run;\n\n%mend winsor;\n</code></pre>","title":"Version 1 (Unknown Author)"},{"location":"posts/winsorization-in-sas/#version-2-wrds","text":"<p>A potential problem with this WRDS macro is that a variable named <code>a</code> is used in line 57 and 59 (highlighted below). So if the <code>INSET</code> has a variable named <code>a</code> as well, there\u2019ll be possible data integrity issue.</p>  <code>WINSORIZE</code> macro <pre><code>/* ********************************************************************************* */\n/* ******************** W R D S   R E S E A R C H   M A C R O S ******************** */\n/* ********************************************************************************* */\n/* WRDS Macro: WINSORIZE                                                             */\n/* Summary   : Winsorizes or Trims Outliers                                          */\n/* Date      : April 14, 2009                                                        */\n/* Author    : Rabih Moussawi, WRDS                                                  */\n/* Variables : - INSET and OUTSET are input and output datasets                      */\n/*             - SORTVAR: sort variable used in ranking                              */\n/*             - VARS: variables to trim and winsorize                               */\n/*             - PERC1: trimming and winsorization percent, each tail (default=1%)   */\n/*             - TRIM: trimming=1/winsorization=0, default=0                         */\n/* ********************************************************************************* */\n\n%MACRO WINSORIZE (INSET=,OUTSET=,SORTVAR=,VARS=,PERC1=1,TRIM=0);\n\n/* List of all variables */\n%let vars = %sysfunc(compbl(&amp;vars));\n%let nvars = %nwords(&amp;vars);\n\n/* Display Output */\n%put ### START.;\n\n/* Trimming / Winsorization Options */\n%if &amp;trim=0 %then %put ### Winsorization; %else %put ### Trimming;\n%put ### Number of Variables:  &amp;nvars;\n%put ### List   of Variables:  &amp;vars;\noptions nonotes;\n\n/* Ranking within &amp;sortvar levels */\n%put ### Sorting... ;\nproc sort data=&amp;inset; by &amp;sortvar; run;\n\n/* 2-tail winsorization/trimming */\n%let perc2 = %eval(100-&amp;perc1);\n\n%let var2 = %sysfunc(tranwrd(&amp;vars,%str( ),%str(__ )))__;\n%let var_p1 = %sysfunc(tranwrd(&amp;vars,%str( ),%str(__&amp;perc1 )))__&amp;perc1 ;\n%let var_p2 = %sysfunc(tranwrd(&amp;vars,%str( ),%str(__&amp;perc2 )))__&amp;perc2 ;\n\n/* Calculate upper and lower percentiles */\nproc univariate data=&amp;inset noprint;\nby &amp;sortvar;\nvar &amp;vars;\noutput out=_perc pctlpts=&amp;perc1 &amp;perc2 pctlpre=&amp;var2;\nrun;\n\n%if &amp;trim=1 %then\n%let condition = %str(if myvars(i)&gt;=perct2(i) or myvars(i)&lt;=perct1(i) then myvars(i)=. );\n%else %let condition = %str(myvars(i)=min(perct2(i),max(perct1(i),myvars(i))) );\n\n%if &amp;trim=0 %then %put ### Winsorizing at &amp;perc1.%... ;\n%else %put ### Trimming at &amp;perc1.%... ;\n\n/* Save output with trimmed/winsorized variables */\ndata &amp;outset;\nmerge &amp;inset (in=a) _perc;\nby &amp;sortvar;\nif a;\narray myvars {&amp;nvars} &amp;vars;\narray perct1 {&amp;nvars} &amp;var_p1;\narray perct2 {&amp;nvars} &amp;var_p2;\ndo i = 1 to &amp;nvars;\n    if not missing(myvars(i)) then\n    do;\n    &amp;condition;\n    end;\nend;\ndrop i &amp;var_p1 &amp;var_p2;\nrun;\n\n/* House Cleaning */\nproc sql; drop table _perc; quit;\noptions notes;\n\n%put ### DONE . ; %put ;\n\n%MEND WINSORIZE;\n\n/* ********************************************************************************* */\n/* *************  Material Copyright Wharton Research Data Services  *************** */\n/* ****************************** All Rights Reserved ****************************** */\n/* ********************************************************************************* */\n</code></pre>","title":"Version 2 (WRDS)"},{"location":"posts/working-remotely-on-a-windows-machine-wsl-from-vscode-on-a-mac/","text":"<p>Now I only need a MacBook (1.3 GHz dual-core i5) to do all my work anywhere, thanks to a powerful workstation provided by the university. Yet the workstation is based on Windows 10 and sitting behind the university VPN. I don't want to use Remote Desktop everytime I need to do some coding, so I decided to make it so I can code remotely on the workstation but from the lovely VSCode on my little Mac.</p>","title":"Working Remotely on a Windows Machine from VSCode on a Mac"},{"location":"posts/working-remotely-on-a-windows-machine-wsl-from-vscode-on-a-mac/#1-set-up-the-windows-10-host-machine","text":"<p>The first step is to enable remote SSH login on the Windows machine. It is now super easy to do with the Windows Subsystem for Linux (WSL). I use the Ubuntu 18.04 LTS distro but other Linux distros should work just fine. This will be the remote environment that I work in. Then I follow the instruction in SSH on Windows Subsystem for Linux (WSL). The post is in great detail with step-by-step guidance. So I won't repeat it again.</p>","title":"1. Set up the Windows 10 host machine"},{"location":"posts/working-remotely-on-a-windows-machine-wsl-from-vscode-on-a-mac/#2-set-up-the-vscode-on-mac","text":"<p>The second step is to install the Remote-SSH extension on VSCode. Then simply ssh into the Ubuntu environment on Windows 10 host machine using the username and password created for the Ubuntu distro. In my case is <code>ssh</code><code>myusername</code><code>@asgard.econ.usyd.edu.au</code>. A password prompt will of course kindly show up.</p>","title":"2. Set up the VSCode on Mac"},{"location":"posts/working-remotely-on-a-windows-machine-wsl-from-vscode-on-a-mac/#3-use-ssh-key-to-avoid-using-password-login","text":"<p>The annoying thing is that each time the window reloads and when I start VSCode, I need to manually type in my lengthy password. The better way must be using a SSH key instead.</p> <p>To do so, open up the Terminal on the Mac and run:</p> <pre><code>ssh-keygen\n</code></pre> <p>A public-private key pair will be generated as <code>~/.ssh/id_rsa.pub</code> and <code>~/.ssh/id_rsa</code>. Then we need to tell the host machine that this key can be used to identify myself so i can skip entering password next time:</p> <pre><code>ssh-copy-id myusername@asgard.econ.usyd.edu.au\n</code></pre> <p>It will ask for the password on the host machine to confirm I am who I am. But after this, starting VSCode will never ask my password again. What a relief!</p>","title":"3. Use SSH key to avoid using password login"},{"location":"posts/working-remotely-on-a-windows-machine-wsl-from-vscode-on-a-mac/#lastly","text":"<p>Because the host machine is inside the university network, I need to first connect to the university VPN, otherwise the host address <code>asgard.econ.usyd.edu.au</code> will not resolve. Still, it's really great that I can code and run my programs remotely on the powerful 8-core 16-thread machine without feeling the hotness and noise, which turns out to be really important in the summer of Australia......</p>","title":"Lastly..."},{"location":"specurve/","text":"","title":"Specification Curve Analysis"},{"location":"specurve/#motivation","text":"<p>More often than not, empirical researchers need to argue that their chosen model specification reigns. If not, they need to run a battery of tests on alternative specifications and report them. The problem is, researchers can fit a few tables each with a few models in the paper at best, and it's extremely hard for readers to know whether the reported results are being cherry-picked.</p> <p>So, why not run all possible model specifications and find a concise way to report them all?</p>","title":"Motivation"},{"location":"specurve/#the-specification-curve","text":"<p>The idea of specification curve is a direct answer to the question provided by Simonsohn, Simmons and Nelson (2020).1 2</p> <p>To intuitively explain this concept, below is the Figure 2 from my recent paper Organization Capital and Executive Performance Incentives on the Journal of Banking &amp; Finance,3 which is used to show the robustness of an substitution effect of organization capital on executive pay-for-performance sensitivity. Therefore, the estimated coefficients for the variable of interest OC are expected to be negative across different model specifications.</p> <p></p> <p>The plot is made up of two parts. The upper panel plots the coefficient estimates of OC in various model specifications, in descending order, and the associated 95% confidence intervals. Sample sizes of each model are plotted as bars at the bottom of the upper panel. For simplicity, we annotate only the maximum and minimum coefficient estimates, as well as the threshold of zero. The lower panel reports the exact specification for each model, where colored dots indicate the choices from various specification alternatives. Both panels share the same x-axis of model number.</p> <p>To interpret this specification curve, for example, OC has an estimated coefficient of \u22120.11 in the first model, which uses the natural logarithm of DELTA_MGMT (a measure of executive pay-for-performance sensitivity) as the dependent variable, and control variables as in the baseline model, including industry fixed effects and year fixed effects, clustering standard errors at the firm level, and is estimated on the full sample.</p> <p>Further, the ordered nature of the curve implies that this is the minimum estimated impact of OC on ln(DELTA_MGMT), whereas the maximum estimated coefficient is doubled at \u22120.22 when the industry fixed effects are replaced with the more conservative firm fixed effects and estimated on the sample excluding global financial crisis period. More importantly, in all specifications, we find the coefficient estimates of OC to be statistically significant. Using an alternative measure of executive pay-for-performance sensitivity as the dependent variable, again, has minimal impact on the documented substitution effect of OC.</p> <p>This specification curve reports a total of 2\\times2\\times4\\times1\\times2=32 specifications:</p> <ul> <li>2 choices of dependent variables (as alternative measures of executive pay-for-performance sensitivity)</li> <li>2 choices of controls variables (controlling for managerial ability at the cost of reduced sample size)</li> <li>4 choices of fixed effects </li> <li>1 choice of standard error clustering</li> <li>2 choices of sample periods </li> </ul> <p>Beyond reporting all estimates from hundreds and thousands of models, the more appealing point of specification curve is that we can identify the most impactful factors in specifying the model. As the models are sorted by the coefficient estimates, the distribution of dots in the lower panel can reveal whether certain specification choices drive the results. </p> <ul> <li>Alternative measures of executive pay-for-performance sensitivity do not affect the main findings.</li> <li>The inclusion of additional control variable of managerial ability does not affect the main findings.</li> <li>The industry and year fixed effects seem to lead to weaker coefficient estimates for the variable of interest, albeit the more conservative firm and year fixed effects lead to stronger ones. This is very important.</li> <li>The main findings hold with and without the global financial crisis (GFC) period.</li> </ul> <p>Of course, even 32 models cannot exhaust all possible specifications. Nevertheless, by addressing the most critical ones, we are able to use one specification curve plot to convince readers that our findings are robust.</p>","title":"The Specification Curve"},{"location":"specurve/#specurve-stata-command-for-specification-curve-analysis","text":"<p>Since there was no available software or package to conduct specification curve analysis. I wrote myself a Stata command <code>specurve</code>, open-sourced at github.com/mgao6767/specurve.</p>","title":"<code>specurve</code> - Stata command for specification curve analysis"},{"location":"specurve/#dependencies","text":"<p><code>specurve</code> depends on Stata 16's Python integration and requires a Python version of 3.6 or above.</p> <p>Python modules required:</p> <ul> <li><code>pandas</code>: for basic dataset manipulation.</li> <li><code>pyyaml</code>: for reading and parsing the YAML-formatted configuration file.</li> <li><code>plotly</code>: for generating the specification curve plot.</li> <li><code>kaleido</code>: for static image export with <code>plotly</code>.</li> </ul> <p>To install the required modules, try:</p> <pre><code>pip install pandas pyyaml plotly kaleido\n</code></pre>","title":"Dependencies"},{"location":"specurve/#installation","text":"<p>Download <code>specurve.ado</code> and <code>specurve.hlp</code> and put them in your personal ado folder. To find the path to your personal ado folder, type <code>adopath</code> in Stata.</p>","title":"Installation"},{"location":"specurve/#example-usage","text":"<p>The associated help file contains a step-by-step guide on using <code>specurve</code>. To open the help file, type <code>help specurve</code> in Stata after installation.</p>","title":"Example usage"},{"location":"specurve/#example-output","text":"<p></p> <p></p>   <ol> <li> <p>Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D., 2020, Specification Curve Analysis, Nature Human Behaviour.\u00a0\u21a9</p> </li> <li> <p>Special thanks to Rawley Heimer from Boston College who visited our discipline in 2019 and introduced the Specification Curve Analysis to us in the seminar on research methods.\u00a0\u21a9</p> </li> <li> <p>Gao, M. Leung, H. and Qiu, B. (2021). Organization Capital and Executive Performance Incentives, Journal of Banking &amp; Finance, 123, 106017.\u00a0\u21a9</p> </li> </ol>","title":"Example output"}]}